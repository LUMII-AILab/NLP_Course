{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK un citas Python rīkkopas teksta apstrādei un normalizācijai\n",
        "\n",
        "Izmantoti piemēri no [NLTK grāmatas](https://www.nltk.org/book/ch02.html) un no [Annas Rodžersas nodarbībām](https://colab.research.google.com/drive/18raZBpdx5tDg3TZ015p99j2rF06qABbW?usp=drive_open#scrollTo=iAbsVVK2v-4w) ESSLLI 2019 vasaras skolā."
      ],
      "metadata": {
        "id": "krvncS3Glj-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Package setup\n",
        "!pip install nltk spacy bpe\n",
        "\n",
        "# Import NLTK\n",
        "import nltk\n",
        "\n",
        "# Download the Brown corpus\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "YXA9WEdHmvUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Darbs ar NLTK korpusiem\n",
        "\n",
        "NLTK includes a diverse set of corpora which can be downloaded and processed using the nltk.corpus package."
      ],
      "metadata": {
        "id": "cU8ze2lsTuOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "# List the documents in the Brown corpus\n",
        "brown.fileids()"
      ],
      "metadata": {
        "id": "peaPz53jKAyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the size of the corpus / document\n",
        "print(len(brown.words()))\n",
        "print(len(brown.words('cr09')))"
      ],
      "metadata": {
        "id": "xa0iGN2sLBuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK text corpora support methods to read the corpus as raw/annotated text, a list of words, a list of sentences, or a list of paragraphs.\n",
        "\n",
        "print(\"Raw text:\")\n",
        "print(brown.raw('cr09'))\n",
        "\n",
        "print(\"Words:\")\n",
        "print(brown.words('cr09'))\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(brown.sents('cr09'))\n",
        "\n",
        "print(\"Paragraphs:\")\n",
        "print(brown.paras('cr09'))"
      ],
      "metadata": {
        "id": "FxUBwMYPlpQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regulāro izteiksmju lietojums"
      ],
      "metadata": {
        "id": "4q4zPBBZThUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To work with non-NLTK corpora:\n",
        "\n",
        "!wget -O \"Rainis.txt\" https://repository.clarin.lv/repository/xmlui/bitstream/handle/20.500.12574/41/rainis_v20180716.txt\n",
        "!wget -O \"Romeo.txt\" https://www.gutenberg.org/cache/epub/1112/pg1112.txt\n",
        "\n",
        "# Alternatively, upload files from your local file system\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "file_en = open('Romeo.txt', mode='r', encoding='utf-8')\n",
        "file_lv = open('Rainis.txt', mode='r', encoding='utf-8')\n",
        "\n",
        "text_en = file_en.read()\n",
        "text_lv = file_lv.read()"
      ],
      "metadata": {
        "id": "F6k6O5A8oDRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vārdu atrašana\n",
        "# See also: https://docs.python.org/3/library/re.html\n",
        "import re\n",
        "\n",
        "words_en = re.findall('[A-z]+', text_en)\n",
        "print(words_en)\n",
        "\n",
        "words_lv = re.findall('[A-zĀāČčĒēĢģĪīĶķĻļŅņŠšŪūŽž]+', text_lv)\n",
        "#print(words_lv)"
      ],
      "metadata": {
        "id": "zm2fE8RXo-on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vienkāršota dalīšana teikumos un tekstvienībās\n",
        "words_en = re.split(r'\\W+', text_en)\n",
        "print(words_en)\n",
        "\n",
        "words_lv = re.split(r'\\W+', text_lv)\n",
        "#print(words_lv)\n",
        "\n",
        "# Punkts kā teikuma beigu indikators (naivs pieņēmums)\n",
        "sentences = re.split(r'\\.', text_en)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "q-1X48tXphxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitution\n",
        "one_liner = re.sub(r'[ ]+', ' ', re.sub(r'\\n+', ' ', text_en))  # FIXME: \\s\n",
        "print(one_liner)"
      ],
      "metadata": {
        "id": "TtBLeeAtpdhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK rīki un bibliotēkas"
      ],
      "metadata": {
        "id": "thuCr0KGUD9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic RegEx-based tokenization with NLTK\n",
        "nltk.regexp_tokenize(text_en, '\\w+')"
      ],
      "metadata": {
        "id": "aHyEjtRDprMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a pre-trained tokenizer\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Punkt Sentence Tokenizer\n",
        "# This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build\n",
        "# a model for abbreviation words, collocations, and words that start sentences.\n",
        "# It must be trained on a large collection of plaintext in the target language before it can be used.\n",
        "# See: https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk_words_en = word_tokenize(text_en)\n",
        "print(nltk_words_en)\n",
        "\n",
        "nltk_words_lv = word_tokenize(text_lv)\n",
        "#print(nltk_words_lv)"
      ],
      "metadata": {
        "id": "yIAXhgHJpw-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy bibliotēka dalīšanai tekstvienībās"
      ],
      "metadata": {
        "id": "YYR3KFe2g5CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To perform natural language processing tasks for a given language,\n",
        "# we must load a language model that has been trained to perform these tasks for the language in question.\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Tokenization with Spacy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Passing the variable text to the Language object nlp returns a spaCy Doc object\n",
        "doc = nlp(text_en)\n",
        "\n",
        "# For each token..\n",
        "n = 0\n",
        "for token in doc:\n",
        "    print(token)\n",
        "    n += 1\n",
        "    if n == 500: break"
      ],
      "metadata": {
        "id": "UkMaL10Ig3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE dalītājs tekstvienībās"
      ],
      "metadata": {
        "id": "xa1-4lOGhiwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE tokenizer from https://github.com/soaxelbrooke/python-bpe\n",
        "import bpe\n",
        "from bpe import Encoder\n",
        "\n",
        "#file=open('raimondspauls.txt', mode='r', encoding='utf-8')\n",
        "#text=file.read()\n",
        "encoder = Encoder(200, pct_bpe=0.88)\n",
        "encoder.fit(text_lv.split('\\n'))\n",
        "example=\"Ziema nemaz tik drīz nebeigsies .\"\n",
        "\n",
        "print(encoder.tokenize(example))\n",
        "print(next(encoder.inverse_transform(encoder.transform([example]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCISYVGOhr9y",
        "outputId": "fc987b9a-a589-4ac9-cde1-bb362b583b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__sow', 'zi', 'e', 'ma', '__eow', '__sow', 'n', 'e', 'ma', 'z', '__eow', '__sow', 'ti', 'k', '__eow', '__sow', 'd', 'rī', 'z', '__eow', '__sow', 'n', 'e', 'be', 'i', 'g', 's', 'ie', 's', '__eow', '.']\n",
            "ziema nemaz tik drīz nebeigsies .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dažādi rīki dalīšanai tekstvienībās:  https://github.com/huggingface/tokenizers/\n",
        "\n",
        "Moses tokenizer: https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl\n",
        "\n",
        "More tools in Python: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ],
      "metadata": {
        "id": "Qqkvt_tXigws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Celmošana (Stemming)"
      ],
      "metadata": {
        "id": "6nFhzDGSkErb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps =PorterStemmer()\n",
        "#for w in m:\n",
        "#\trootWord=ps.stem(w)\n",
        "#    print(rootWord)\n",
        "#ps.stem(m[6])\n",
        "stems_output = ' '.join([ps.stem(w) for w in word_list])"
      ],
      "metadata": {
        "id": "b4gn2HRBkZUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK's SnowballStemmer supports 13 languages\n",
        "from nltk.stem import SnowballStemmer\n",
        "print(SnowballStemmer.languages)"
      ],
      "metadata": {
        "id": "9cEB8PxXmdlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_en = SnowballStemmer('english')\n",
        "print(stemmer_en.stem(text_en))"
      ],
      "metadata": {
        "id": "3XQtgatgmiA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatizācijas rīki"
      ],
      "metadata": {
        "id": "mDpnXZkWkjAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNet lemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word_list = nltk.word_tokenize(text_en)\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "\n",
        "print(lemmatized_output)"
      ],
      "metadata": {
        "id": "Dzi08jDPksk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization with Spacy\n",
        "lemmas_spacy = \" \".join([token.lemma_ for token in doc])\n",
        "print(lemmas_spacy)"
      ],
      "metadata": {
        "id": "sbs_1Dvsk6kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}