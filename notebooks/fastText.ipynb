{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/fastText.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "fkWv6p4KNGlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fastText-based classifier"
      ],
      "metadata": {
        "id": "htrlt1a3W_tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment"
      ],
      "metadata": {
        "id": "skNUN8iH_0rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "DszSNZBeXMnk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "\n",
        "!gunzip cc.en.300.bin.gz"
      ],
      "metadata": {
        "id": "o8yJqCbaba5k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import numpy\n",
        "import re\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "cIqoRNUJYsi6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ilT-aHWYA7fy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained fastText model\n",
        "ft_model = fasttext.load_model('cc.en.300.bin')"
      ],
      "metadata": {
        "id": "B5pdOfpsYuzy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/news20/20_newsgroup.tsv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/news20/20_newsgroup-freq.tsv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/news20/stoplist.txt"
      ],
      "metadata": {
        "id": "7c6czL8-E2ij",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text preprocessing"
      ],
      "metadata": {
        "id": "l75Xehf2AGgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common with the NB classifier"
      ],
      "metadata": {
        "id": "I_HQpVKvC4pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialise(stop_txt, freq_tsv):\n",
        "\tglobal STOPLIST\n",
        "\tSTOPLIST = set()\n",
        "\n",
        "\twith open(stop_txt) as txt:\n",
        "\t\tfor word in txt:\n",
        "\t\t\tSTOPLIST.add(normalize_text(word.strip()))\n",
        "\n",
        "\tglobal WHITELIST\n",
        "\tWHITELIST = set()\n",
        "\n",
        "\twith open(freq_tsv) as tsv:\n",
        "\t\tfor entry in tsv:\n",
        "\t\t\tfreq, word = entry.strip().split(\"\\t\")\n",
        "\n",
        "\t\t\tif int(freq) < 3: # TODO: experiment with the threshold\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tWHITELIST.add(normalize_text(word))"
      ],
      "metadata": {
        "id": "9ggSEczRBS1e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "\ttext = text.lower()\n",
        "\ttext = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text) # e-mail addresses\n",
        "\ttext = re.sub(r'https?://[A-Za-z0-9./-]+|www\\.[A-Za-z0-9./-]+', '', text)\t\t\t\t# URLs\n",
        "\ttext = re.sub(r'\\d+', \"100\", text)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    # numbers\n",
        "\n",
        "\treturn text.strip()\n",
        "\n",
        "\n",
        "def filter_tokens(tokens): # cf. normalize_vector() in the NB implementation\n",
        "    tokens_prim = []\n",
        "\n",
        "    for t in tokens:\n",
        "        if t in STOPLIST or len(t) == 1 or t not in WHITELIST:\n",
        "            continue\n",
        "        else:\n",
        "            tokens_prim.append(t)\n",
        "\n",
        "    return tokens_prim"
      ],
      "metadata": {
        "id": "AZwo96NhB8_f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fastText-specific vectorization"
      ],
      "metadata": {
        "id": "PgvNxh0dDFrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vector(sentence):\n",
        "    # Normalization, tokenization and filtering - as for the NB classifier\n",
        "    tokens = filter_tokens(nltk.word_tokenize(normalize_text(sentence)))\n",
        "\n",
        "    if len(tokens) == 0: return []\n",
        "\n",
        "    # For each token, get its fastText vector representation\n",
        "    embeddings = [ft_model.get_word_vector(t) for t in tokens]\n",
        "\n",
        "    # Average the token vectors to create a single sentence/text vector\n",
        "    sentence_vector = numpy.sum(embeddings, axis=0)\n",
        "    # TODO: experiment with mean() vs. sum() vs. amax()\n",
        "\n",
        "    return sentence_vector"
      ],
      "metadata": {
        "id": "rHtkyXkkDQZ2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filename):\n",
        "    sentences, labels = [], []\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            cols = line.strip().split('\\t')\n",
        "            if len(cols) == 2:\n",
        "\n",
        "                label, sent = cols\n",
        "                sent_vec = get_sentence_vector(sent)\n",
        "\n",
        "                if len(sent_vec) > 0:\n",
        "                    sentences.append(sent_vec)\n",
        "                    labels.append(label)\n",
        "\n",
        "    print(\"[I] Samples loaded and vectorized:\", len(sentences))\n",
        "\n",
        "    return numpy.array(sentences), numpy.array(labels)"
      ],
      "metadata": {
        "id": "A6gio_gZck-K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentation & evaluation"
      ],
      "metadata": {
        "id": "jAODCoyeAN03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the stopword and word frequency lists\n",
        "initialise('stoplist.txt', '20_newsgroup-freq.tsv')\n",
        "\n",
        "# Load and vectorize the dataset\n",
        "X, y = load_dataset(\"20_newsgroup.tsv\")"
      ],
      "metadata": {
        "id": "eUZksKFcsC8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = lr_model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "8O9RP_EOXTCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model for later use\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(\"ft_classifier.pickle\", \"wb\") as dmp:\n",
        "\t\tpickle.dump(lr_model, dmp)\n",
        "\t\tprint(\"[I] FT classifier stored in a file\")"
      ],
      "metadata": {
        "id": "sZqUMcZyt5TW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}