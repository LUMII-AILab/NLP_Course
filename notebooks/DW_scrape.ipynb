{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/DW_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owkJNLbknhFr"
      },
      "source": [
        "# Building a multi-lingual text corpus\n",
        "\n",
        "Within this notebook, we will extract textual information from the news portal [Deutche Welle](https://www.dw.com/en/). DW produces content in 30+ languages.\n",
        "\n",
        "Our main goal is to use some articles available from the DW website to create a small corpus in the VERT format.\n",
        "\n",
        "We will:\n",
        "* Gather URLs to individual articles\n",
        "* Extract useful information from the dynamically loaded HTML pages\n",
        "* Use the Stanza toolkit to tokenize the texts and normalize the tokens\n",
        "* Store the corpus data in a VERT format file\n",
        "\n",
        "!NB: This notebook is only for educational purposes. Please respect the IPR of DW."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zw1eZ8x_pyz"
      },
      "source": [
        "## Corpus structure\n",
        "\n",
        "The final corpus [VERT file](https://www.sketchengine.eu/glossary/vertical-file/) should store all the tokenized texts and the metadata information of each document.\n",
        "\n",
        "Within this corpus, we will use two structures - documents (`<doc>`) and sentences (`<s>`) - where the division into sentences is handled by the `Stanza` toolkit.\n",
        "\n",
        "Each document tag (`<doc>`) shall contain the following attributes:\n",
        "* language\n",
        "* portal (always DW in this case)\n",
        "* article path\n",
        "* title\n",
        "* author\n",
        "* time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PUjDdPItZaQ"
      },
      "source": [
        "## Initial set-up\n",
        "\n",
        "In real world situations, the information on websites is often **loaded dynamically**, which means that our previous approach (cf. `TextExtraction.ipynb`) of simply reading the retrieved HTML file does not work.\n",
        "\n",
        "To address this issue, we will use a couple of python `WebDriver` libraries to load the pages, so that the article contents are loaded when we scrape the HTML file:\n",
        "* [Selenium](https://selenium-python.readthedocs.io/) with Python\n",
        "* [Geckodriver](https://github.com/mozilla/geckodriver) â€“ required driver for calling the Firefox browser (see Selenium with Python documentation Section 1.5 for other browser drivers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OqUS5cS0kdM9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install selenium\n",
        "!pip install pandas\n",
        "!pip install stanza\n",
        "\n",
        "!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n",
        "!tar -xvzf geckodriver*\n",
        "!chmod +x geckodriver\n",
        "!sudo mv geckodriver /usr/local/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuiaLMx3knBr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import time\n",
        "import stanza\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver import FirefoxOptions\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTytlIaCymz9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## HTML parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcf13CGskp8M"
      },
      "outputs": [],
      "source": [
        "MAX_COUNT = 10\n",
        "DW_URL = \"http://www.dw.com\"\n",
        "CORPUS_FILE = \"stanza_output.vert\"\n",
        "LANGUAGES = {\"de\", \"en\", \"es\", \"fr\", \"uk\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eycEbCEkk1T8"
      },
      "outputs": [],
      "source": [
        "info = []\n",
        "\n",
        "def scrape_article(soup, url):\n",
        "    a = {}\n",
        "\n",
        "    language = url[len(DW_URL):].split(\"/\")[1]\n",
        "    filename = url.split(\"/\")[-1]\n",
        "\n",
        "    a[\"language\"] = language\n",
        "    a[\"portal\"] = \"Deutsche Welle\"\n",
        "    a[\"link\"] = url\n",
        "\n",
        "    with open(os.getcwd()+\"/\"+language+\"/\"+filename+\".txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "        # Read Title\n",
        "        header = soup.find(\"header\")\n",
        "        try:\n",
        "            title = header.find(\"h1\").text\n",
        "            a[\"title\"] = title\n",
        "            file.write(title+\"\\n\")\n",
        "        except:\n",
        "            a[\"title\"] = \"\"\n",
        "\n",
        "        # Read article author\n",
        "        try:\n",
        "            author = header.find(\"div\", {\"class\": re.compile(\".*author-details.*\")}).text\n",
        "            a[\"author\"] = author\n",
        "            file.write(author+\"\\n\")\n",
        "        except:\n",
        "            a[\"author\"] = \"\"\n",
        "\n",
        "        # Read article publishing date\n",
        "        try:\n",
        "            time = header.find(\"time\").text\n",
        "            a[\"time\"] = time\n",
        "            file.write(time+\"\\n\")\n",
        "        except:\n",
        "            a[\"time\"] = \"\"\n",
        "\n",
        "        file.write(\"\\n\")\n",
        "        # Read main article text\n",
        "        try:\n",
        "            main_div = soup.find(\"div\", {\"class\": re.compile(\".*content-area.*\")})\n",
        "            main_text = main_div.find(\"div\", {\"class\": re.compile(\".*rich-text.*\")})\n",
        "            a[\"text\"] = \"\"\n",
        "            for p in main_text.find_all(\"p\"):\n",
        "                a[\"text\"] += p.text+\"\\n\"\n",
        "                file.write(p.text+\"\\n\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(a)\n",
        "    info.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1cWSkzwPVkO"
      },
      "outputs": [],
      "source": [
        "def scraper(urls):\n",
        "    opts = FirefoxOptions()\n",
        "    opts.add_argument(\"--headless\")\n",
        "    browser = webdriver.Firefox(options=opts)\n",
        "\n",
        "    for url in urls:\n",
        "        url = DW_URL + url\n",
        "        browser.get(url)\n",
        "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
        "        scrape_article(soup, url)\n",
        "\n",
        "    browser.quit();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXnqbfwpmw7n"
      },
      "outputs": [],
      "source": [
        "# Let's test that the scraper works\n",
        "if not os.path.exists(os.getcwd()+\"/fr\"):\n",
        "    os.mkdir(os.getcwd()+\"/fr\")\n",
        "\n",
        "TEST_URL = \"/fr/lubero-beni-butembo-meutres-serie-tutsis-m23-appels-au-calme-reportage/a-69649209\"\n",
        "\n",
        "scraper([TEST_URL])\n",
        "info.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP-3gILfKjlR"
      },
      "source": [
        "## Article URLs\n",
        "\n",
        "In many cases, RSS feeds are available for easily accessing new articles that have been published. DW also has a couple of [links](https://corporate.dw.com/en/rss-feeds/a-68693346) that allow for access to the news feed. However, these available feeds are not suited for the purposes of creating a multi-lingual text corpus.\n",
        "\n",
        "We will instead iterate recursively through the article links provided in each indiviadual article. Additionally, we will only examine those pages whose ID starts with \"a-\" as these are text based articles as opposed to video or image based news stories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U0def1dk6HO"
      },
      "outputs": [],
      "source": [
        "seen = []\n",
        "articles = []\n",
        "\n",
        "def fetch_articles(url):\n",
        "    if len(articles) >= MAX_COUNT:\n",
        "        return\n",
        "\n",
        "    seen.append(url)\n",
        "    r = requests.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    # print(soup.prettify())\n",
        "    teasers = soup.find_all(\"div\", {\"class\": \"teaser-data\"})\n",
        "    for t in teasers:\n",
        "        if len(articles) >= MAX_COUNT:\n",
        "            return\n",
        "        article = (t.find(\"a\").get(\"href\"))\n",
        "        artinfo = article.split(\"/\")[-1]\n",
        "        if artinfo[:2] == \"a-\" and article not in articles:\n",
        "            articles.append(article)\n",
        "        if DW_URL + article not in seen and artinfo[:2] == \"a-\":\n",
        "            fetch_articles(DW_URL + article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m45YazLhOcug"
      },
      "outputs": [],
      "source": [
        "def analyze_articles(nlp, output_file=CORPUS_FILE):\n",
        "    with open(output_file, 'a', encoding=\"utf-8\") as s_file:\n",
        "        for d in info:\n",
        "            s_file.write('<doc language=\"{}\" portal=\"{}\" link=\"{}\" title=\"{}\" author=\"{}\" time=\"{}\">\\n'.format(d[\"language\"], d[\"portal\"], d[\"link\"], d[\"title\"], d[\"author\"], d[\"time\"]))\n",
        "            for s in nlp(d[\"text\"]).sentences:\n",
        "                s_file.write(f'<s>\\n')\n",
        "                for w in s.words:\n",
        "                    s_file.write(f'{w.text}\\t{w.upos}\\t{w.lemma}\\n')\n",
        "                s_file.write(\"</s>\\n\")\n",
        "            s_file.write(\"</doc>\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "NoERhit_lG_K"
      },
      "outputs": [],
      "source": [
        "def fetch_lang(language, stanza_code=\"\"):\n",
        "    seen.clear()\n",
        "    articles.clear()\n",
        "    if not os.path.exists(os.getcwd()+\"/\"+language):\n",
        "        os.mkdir(os.getcwd()+\"/\"+language)\n",
        "    dw_lang = DW_URL + \"/\" + language\n",
        "    fetch_articles(dw_lang)\n",
        "    scraper(articles)\n",
        "\n",
        "    if not stanza_code:\n",
        "        stanza_code = language\n",
        "    stanza.download(stanza_code)\n",
        "    nlp = stanza.Pipeline(stanza_code, processors='tokenize,pos,lemma')\n",
        "    analyze_articles(nlp)\n",
        "    info.clear()\n",
        "\n",
        "# fetch_lang(\"fr\")\n",
        "info.clear()\n",
        "open(CORPUS_FILE, \"w\").close()\n",
        "for lang in LANGUAGES:\n",
        "    fetch_lang(lang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHrsFXmDJQJT"
      },
      "source": [
        "## Quick corpus analysis\n",
        "\n",
        "Here we look at how many tokens and lemmas are present in the texts of each language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VqUnWcsiFqnp"
      },
      "outputs": [],
      "source": [
        "def print_corpus_info():\n",
        "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\") as corpus:\n",
        "        lang = \"\"\n",
        "        df = pd.DataFrame(columns=['lang','token','pos','lemma'])\n",
        "\n",
        "        for line in corpus:\n",
        "            if line.startswith(\"<doc\"):\n",
        "                lang = line.split(\"\\\"\")[1]\n",
        "            elif not line.startswith(\"<\") or line.startswith(\"<\\t\"):\n",
        "                token = line.strip().split(\"\\t\")\n",
        "                tok = {'lang': lang, 'token': token[0], 'pos': token[1], 'lemma': token[2]}\n",
        "                df.loc[len(df)] = tok\n",
        "\n",
        "        print(df.head())\n",
        "        return df\n",
        "\n",
        "df = print_corpus_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJs3FV7DWXv6"
      },
      "outputs": [],
      "source": [
        "# Print out each language data\n",
        "for lang in LANGUAGES:\n",
        "    print(f\"Language: {lang}\")\n",
        "    print(f\"Total tokens: {len(df[df['lang'] == lang])}\")\n",
        "    print(f\"Unique tokens: {df[df['lang'] == lang]['token'].nunique()}\")\n",
        "    print(f\"Unique lemmas: {df[df['lang'] == lang]['lemma'].nunique()}\")\n",
        "    # print(df[(df['lang'] == lang) & (df['pos'] != \"PUNCT\")]['lemma'].value_counts()[:10])\n",
        "    print(\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}