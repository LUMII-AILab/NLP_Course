{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/BSSDH2024.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "rJ39YBdj6dcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acquiring plain-text data for the corpus"
      ],
      "metadata": {
        "id": "xXCn7YyLyYrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the source documents"
      ],
      "metadata": {
        "id": "vsiXCG2X8th7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Via RSS feeds"
      ],
      "metadata": {
        "id": "jaUL3Vn3DpBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider [Europe Media Monitor](https://emm.newsbrief.eu) feeds:\n",
        "\n",
        "* [Current top 10 stories](https://emm.newsbrief.eu/NewsBrief/clusteredition/en/latest_en.html) (in English) ⇒ [machine-readable feed](https://emm.newsbrief.eu/rss/rss?type=rtn&language=en&duplicates=false) (RSS/XML)\n",
        "* [Biggest 10 Stories Over Last 24h](https://emm.newsbrief.eu/NewsBrief/clusteredition/en/24hrs_en.html) ⇒ [machine-readable feed](https://emm.newsbrief.eu/rss/rss?type=24hrs&language=en&duplicates=false) (RSS/XML)\n",
        "\n",
        "The Really Simple Syndication (RSS) standard and its XML format: https://www.w3schools.com/xml/xml_rss.asp\n",
        "\n",
        "The Python `feedparser` library: https://feedparser.readthedocs.io"
      ],
      "metadata": {
        "id": "bXrY-K99LG0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "id": "wl4j3-f_HTjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "ADe9jBLCDx5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANG = 'en'  # TODO: choose another language"
      ],
      "metadata": {
        "id": "cc4UB-mYSj1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_current = f'https://emm.newsbrief.eu/rss/rss?type=rtn&language={LANG}&duplicates=false'\n",
        "url_last24h = f'https://emm.newsbrief.eu/rss/rss?type=24hrs&language={LANG}&duplicates=false'\n",
        "\n",
        "feed = feedparser.parse(url_current)  # TODO: compare to the last 24h feed\n",
        "\n",
        "LINKS = [entry.link for entry in feed.entries]\n",
        "\n",
        "for link in LINKS: print(link)\n",
        "print(len(LINKS))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WOt9U1fbM-p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter = 'telegraph.co.uk'  # TODO: adjust to your case\n",
        "\n",
        "FILTERED_LINKS = [link for link in LINKS if filter in link]\n",
        "\n",
        "for link in FILTERED_LINKS: print(link)\n",
        "print(len(FILTERED_LINKS))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xElRh4QtUITn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data analysis"
      ],
      "metadata": {
        "id": "MZnrkHQoxkJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### First attempt"
      ],
      "metadata": {
        "id": "SCWEl5oflP_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "portals = [urlparse(link).netloc for link in LINKS]\n",
        "\n",
        "frequencies = Counter(portals)\n",
        "\n",
        "for portal, count in frequencies.items():\n",
        "    print(f'{portal}: {count}')"
      ],
      "metadata": {
        "id": "F46J1fhMZmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Second attempt"
      ],
      "metadata": {
        "id": "iyGP1ltilSlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "portals = [urlparse(link).netloc for link in LINKS]\n",
        "\n",
        "frequencies = Counter(portals)\n",
        "\n",
        "pruned = {portal: count for portal, count in frequencies.items() if count > 1}\n",
        "\n",
        "for portal, count in Counter(pruned).most_common():\n",
        "    print(f'{portal}: {count}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lExc8BDSWgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Via web crawling"
      ],
      "metadata": {
        "id": "_5fjeWCeDydF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (optionally)"
      ],
      "metadata": {
        "id": "iUiOuq3gD3g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting useful content"
      ],
      "metadata": {
        "id": "CMKGfr8I84o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule-based approach"
      ],
      "metadata": {
        "id": "XMDr3eUvsgw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Python `requests` library: https://requests.readthedocs.io\n",
        "\n",
        "The Python `beautifulsoup4` library: https://beautiful-soup-4.readthedocs.io\n",
        "\n",
        "The Python `json` library: https://docs.python.org/3/library/json.html"
      ],
      "metadata": {
        "id": "rEHEw9pKBKJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "WFWiuoSPG5I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "NesLQt-A9Qx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction strategies:\n",
        "1. Try to remove the unnecessary elements, keep the rest.\n",
        "2. Pick out only the useful elements, ignore the rest.\n",
        "\n",
        "The set(s) of rules:\n",
        "1. Common patterns across news portals.\n",
        "2. Portal-specific patterns."
      ],
      "metadata": {
        "id": "XK4gkSd94D4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_plain_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    paragraphs = soup.find_all('p')\n",
        "    text = ' '.join([p.get_text().strip() for p in paragraphs])\n",
        "    # TODO: try and compare this instead: text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "    # TODO: do more elaborate filtering and preprocessing\n",
        "\n",
        "    # TODO: use a specialised library instead of the generic bs4, e.g.:\n",
        "    #       https://newspaper.readthedocs.io\n",
        "    #       https://goose3.readthedocs.io\n",
        "    #       https://trafilatura.readthedocs.io\n",
        "    #       https://pypi.org/project/news-please/\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "xzhMiaKGdLO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See also: https://github.com/LUMII-AILab/NLP_Course/blob/main/notebooks/TextExtraction.ipynb (HTML-to-Text)"
      ],
      "metadata": {
        "id": "5rn2lv9pDcsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for link in FILTERED_LINKS:\n",
        "    content = extract_plain_text(link)\n",
        "    print(content[:200], '\\n' + '='*200)  # Just for the quick testing purposes\n",
        "\n",
        "    article = {\n",
        "        'language': LANG,\n",
        "        'portal': urlparse(link).netloc,\n",
        "        'link': link,\n",
        "        'text': content\n",
        "    }\n",
        "\n",
        "    dataset.append(article)\n",
        "\n",
        "with open('corpus.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(dataset, json_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jR5j6cr0a4u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero/few-shot learning approach"
      ],
      "metadata": {
        "id": "lWjuyv1Csuhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (optionally)"
      ],
      "metadata": {
        "id": "yoPb5sp0s3zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some challenges"
      ],
      "metadata": {
        "id": "qrIREjte-kRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Messy HTML source code"
      ],
      "metadata": {
        "id": "TRPQUoR7-riX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (optionally)"
      ],
      "metadata": {
        "id": "THnpgAll-xbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF documents"
      ],
      "metadata": {
        "id": "7wEVdJ5R-yYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO (optionally)"
      ],
      "metadata": {
        "id": "o6AxU_oJ_pu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See: https://github.com/LUMII-AILab/NLP_Course/blob/main/notebooks/TextExtraction.ipynb (PDF-to-Text)"
      ],
      "metadata": {
        "id": "s1sxkZkYEOfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an annotated text corpus"
      ],
      "metadata": {
        "id": "4axMbGt9yf_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Syntactic parsing"
      ],
      "metadata": {
        "id": "vfDny89IAAz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentation:\n",
        "* Available models per language: https://stanfordnlp.github.io/stanza/models.html\n",
        "* Supported processors and pipelines: https://stanfordnlp.github.io/stanza/pipeline.html\n",
        "* Data objects: https://stanfordnlp.github.io/stanza/data_objects.html"
      ],
      "metadata": {
        "id": "63cEhuXjlK8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "j4NQ9FPCjydA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza"
      ],
      "metadata": {
        "id": "ysatLmrbw83K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(LANG)"
      ],
      "metadata": {
        "id": "PGpeuW6vw_63",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLP_PIPE = stanza.Pipeline(lang=LANG, processors='tokenize,mwt,pos,lemma,depparse')"
      ],
      "metadata": {
        "id": "ev145Pm9xV3P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS = []\n",
        "\n",
        "with open('corpus.json', 'r', encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "    for article in data:\n",
        "        CORPUS.append({\n",
        "            'language': article['language'],\n",
        "            'portal': article['portal'],\n",
        "            'link': article['link'],\n",
        "            'document': NLP_PIPE(article['text'])  # All the NLP happens here!\n",
        "        })"
      ],
      "metadata": {
        "id": "jhwiF1zopORQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoNLL-U output"
      ],
      "metadata": {
        "id": "RZKlEGaGosip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format: https://universaldependencies.org/docs/format.html"
      ],
      "metadata": {
        "id": "gOtgL5Igz890"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('corpus.conllu', 'w', encoding='utf-8') as conllu_file:\n",
        "    for article in CORPUS:\n",
        "\n",
        "        for s in article['document'].sentences:\n",
        "            conllu_file.write(f'# text = {s.text}\\n')\n",
        "\n",
        "            for w in s.words:\n",
        "                conllu_file.write(\n",
        "                    f'{w.id}\\t'\n",
        "                    f'{w.text}\\t'\n",
        "                    f'{w.lemma}\\t'\n",
        "                    f'{w.upos}\\t'\n",
        "                    f'{w.xpos}\\t'\n",
        "                    '_\\t'\n",
        "                    f'{w.head}\\t'\n",
        "                    f'{w.deprel}\\t'\n",
        "                    '_\\t'\n",
        "                    '_\\n'\n",
        "                )\n",
        "\n",
        "            conllu_file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "I_9DNHfoy8Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VERT output"
      ],
      "metadata": {
        "id": "GBFvgkSHO3mE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format: https://www.sketchengine.eu/my_keywords/vertical/\n",
        "\n",
        "NoSketch Engine configuration file: https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/BSSDH2024/corpus.config (**comply with it!** - field names, order)"
      ],
      "metadata": {
        "id": "_5RuYjGpFjo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('corpus.vert', 'w', encoding='utf-8') as vert_file:\n",
        "    for article in CORPUS:\n",
        "        vert_file.write(f'<doc>\\n')\n",
        "        # TODO: add doc-level metadata: language, portal, link\n",
        "        # Hint: article[\"language\"], article[\"portal\"], article[\"link\"]\n",
        "\n",
        "        for s in article['document'].sentences:\n",
        "            vert_file.write(f'<s>\\n')\n",
        "\n",
        "            for w in s.words:\n",
        "                vert_file.write(\n",
        "                    f'{w.text}\\t'\n",
        "                    f'{w.upos}\\t'\n",
        "                    f'{w.lemma}\\t'\n",
        "                    '_\\t'\n",
        "                    f'{s.words[w.head-1].upos if w.head > 0 else \"_\"}\\t'\n",
        "                    '_\\t'\n",
        "                    '_\\n'\n",
        "                )  # TODO: fill the missing word-level features: dep, dep_head_lemma, dep_head_dep\n",
        "\n",
        "            vert_file.write(\"</s>\\n\")\n",
        "\n",
        "        vert_file.write(\"</doc>\\n\")"
      ],
      "metadata": {
        "id": "tHfx5cp3pKsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and copy your VERT file to https://drive.google.com/drive/folders/1aXM_AVDuoyBkc8M6t2tQlbYHvCSS6dYt\n",
        "\n",
        "Before copying, rename it to `corpus-<language>-<NameSurname>.vert`"
      ],
      "metadata": {
        "id": "pcdBXn-8M3q7"
      }
    }
  ]
}