{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/w2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOp459d616_I"
      },
      "source": [
        "#Vector Semantics and Word Embeddings\n",
        "## Word2vec modeļa izveide\n",
        "## Building and using Word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5GzRAO4T9Gle"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH36CAfpAGIi"
      },
      "source": [
        "### Training\n",
        "Teksta lejuplāde un sadalīšana rindiņās\n",
        "\n",
        "Preprocessing - download and line segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLRTiEwZ1_nq"
      },
      "outputs": [],
      "source": [
        "# ! pip install gensim\n",
        "import urllib\n",
        "import re\n",
        "import multiprocessing\n",
        "from time import time\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# change to your own path if you have downloaded the file locally\n",
        "# url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
        "url = \"https://repository.clarin.lv/repository/xmlui/bitstream/handle/20.500.12574/41/rainis_v20180716.txt?sequence=1&isAllowed=y\"\n",
        "# read file into list of lines\n",
        "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yed3eXbI2Wpl"
      },
      "source": [
        "Teksta priekšapstrāde - sadalīšana tekstvienībās\n",
        "\n",
        "Tokenization\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4zjA3I72dGr"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "for line in lines:\n",
        "   # remove punctuation\n",
        "   line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "#   line = line.lower()\n",
        "\n",
        "   # simple tokenizer\n",
        "   tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "\n",
        "   # only keep lines with at least one token\n",
        "   if len(tokens) > 1:\n",
        "      sentences.append(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBHjtlJd2rm9"
      },
      "source": [
        "\n",
        "\n",
        "Modeļa apmācība\n",
        "\n",
        "Training\n",
        "\n",
        "The parameters:\n",
        "\n",
        "*   min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "*   window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "* size = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
        "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whGNCwMt2vk9"
      },
      "outputs": [],
      "source": [
        "\n",
        "w2v_model = Word2Vec(\n",
        "         sentences,\n",
        "         min_count=3,   # Ignore words that appear less than this\n",
        "         vector_size=50,       # Dimensionality of word embeddings\n",
        "         sg = 1,        # skipgrams\n",
        "         window=5,      # Context window for words during training\n",
        "         epochs=40)       # Number of epochs training over corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiziDuQ2GA7z"
      },
      "source": [
        "Alternatīva: apmacība pa soļiem\n",
        "\n",
        "Training in several steps (alternative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4atVIppF_3A"
      },
      "outputs": [],
      "source": [
        "# import multiprocessing\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     vector_size=50,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=cores-1)\n",
        "\n",
        "\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA6Cm0AAIPEZ"
      },
      "outputs": [],
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iQuYz4n3knk"
      },
      "source": [
        "Modelis darbībā\n",
        "\n",
        "Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii2MQPnP3joT"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar('mīla')\n",
        "# w2v_model.wv.most_similar('Romeo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HvY97BFQjfR"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(positive=[\"saule\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBPVJqIbnbJ4"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(negative=[\"saule\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eHbtZfzQxL4"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.similarity(\"saule\", \"pavasars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BR1-ixqRX7d"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.doesnt_match([\"ziema\", \"pavasaris\", \"saule\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCV5QhcpAV_X"
      },
      "source": [
        "Citi nopietnāki un mazāk nopietni materiāli:\n",
        "\n",
        "* Tensorflow Word2Vec Tutorial:https://www.tensorflow.org/text/tutorials/word2vec\n",
        "* Gensim Word2Vec Tutorial: https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook#Getting-Started\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPyb-k6lZVS"
      },
      "source": [
        "## Vārdu līdzības vizualizācija\n",
        "## Visualisation of word similarity\n",
        "Vizualizācijai pielietojam MatPlotLib un SeaBorn bibliotēkas. <br>\n",
        "Sklearn bibliotēka pielieto PCA un TSNE metodes, kas pārveido vārdus vektoru formā (skatīt vector_size pie word2vec), par punktu 2D telpā. <br>\n",
        "Šīs dimensiju redukcijas metodes pēc iespējas saglabā individuālo vārdu līdzības."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTT7CAjAlX_B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYgnZVugh6B8"
      },
      "outputs": [],
      "source": [
        "def tsnescatterplot(model, word, list_names):\n",
        "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
        "    its list of most similar words, and a list of words.\n",
        "    \"\"\"\n",
        "    arrays = np.empty((0, 50), dtype='f')\n",
        "    word_labels = [word]\n",
        "    color_list  = ['red']\n",
        "\n",
        "    # adds the vector of the query word\n",
        "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
        "\n",
        "    # gets list of most similar words\n",
        "    close_words = model.wv.most_similar([word])\n",
        "\n",
        "    # adds the vector for each of the closest words to the array\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('blue')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "\n",
        "    # adds the vector for each of the words from list_names to the array\n",
        "    for wrd in list_names:\n",
        "        wrd_vector = model.wv.__getitem__([wrd])\n",
        "        word_labels.append(wrd)\n",
        "        color_list.append('green')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "\n",
        "    # Reduces the dimensionality from 50 to 21 dimensions with PCA\n",
        "    reduc = PCA(n_components=21).fit_transform(arrays)\n",
        "\n",
        "    # Finds t-SNE coordinates for 2 dimensions\n",
        "    np.set_printoptions(suppress=True)\n",
        "\n",
        "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
        "\n",
        "    # Sets everything up to plot\n",
        "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
        "                       'y': [y for y in Y[:, 1]],\n",
        "                       'words': word_labels,\n",
        "                       'color': color_list})\n",
        "\n",
        "    fig, _ = plt.subplots()\n",
        "    fig.set_size_inches(9, 9)\n",
        "\n",
        "    # Basic plot\n",
        "    p1 = sns.regplot(data=df,\n",
        "                     x=\"x\",\n",
        "                     y=\"y\",\n",
        "                     fit_reg=False,\n",
        "                     marker=\"o\",\n",
        "                     scatter_kws={'s': 40,\n",
        "                                  'facecolors': df['color']\n",
        "                                 }\n",
        "                    )\n",
        "\n",
        "    # Adds annotations one by one with a loop\n",
        "    for line in range(0, df.shape[0]):\n",
        "         p1.text(df[\"x\"][line],\n",
        "                 df['y'][line],\n",
        "                 '  ' + df[\"words\"][line].title(),\n",
        "                 horizontalalignment='left',\n",
        "                 verticalalignment='bottom', size='medium',\n",
        "                 color=df['color'][line],\n",
        "                 weight='normal'\n",
        "                ).set_size(15)\n",
        "\n",
        "\n",
        "    plt.xlim(Y[:, 0].min()-np.absolute(Y[:, 0].min())*0.2, Y[:, 0].max()+np.absolute(Y[:, 0].max())*0.2)\n",
        "    plt.ylim(Y[:, 1].min()-np.absolute(Y[:, 1].min())*0.2, Y[:, 1].max()+np.absolute(Y[:, 1].max())*0.2)\n",
        "\n",
        "    plt.title('t-SNE visualization for {}'.format(word.title()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAHrvz6uiCWM"
      },
      "outputs": [],
      "source": [
        "# Red: Original word\n",
        "# Blue: 10 closest word matches to the original word\n",
        "# Green: 10 furthest word matches to the original word\n",
        "\n",
        "word = \"saule\"\n",
        "tsnescatterplot(w2v_model, word, [i[0] for i in w2v_model.wv.most_similar(negative=[word])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPJ3FvPbsLVM"
      },
      "source": [
        "# TF-IDF\n",
        "### term frequency–inverse document frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhSsSPYu8wz"
      },
      "source": [
        "Vispirms ir nepieciešams klāsts ar individuāliem failiem, kuriem veikt tf-idf analīzi. Pielietosim iepriekš izmantotos Šekspīra darbu datus. Sadalām tos pa lugām, nolasot, kur dokumentā sākas jauna luga jeb sākas pirmais cēliens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgFy-eC1uw8y"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
        "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")\n",
        "iter = 0\n",
        "for line in lines:\n",
        "    if line == \"\\\"ACT I\\\"\":\n",
        "#        outfile.close()\n",
        "        iter += 1\n",
        "        outfile = open(\"work_\"+str(iter)+\".txt\",\"w\")\n",
        "    outfile.write(line+\"\\n\")\n",
        "outfile.close()\n",
        "work_count = iter\n",
        "print(\"Divided into {} files\".format(work_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZhtC0RwTRMV"
      },
      "source": [
        "<br> Veicam tf-idf rezultāta aprēķināšanu ar pašu veidotām funkcijām"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "478xrx7JGXxK"
      },
      "outputs": [],
      "source": [
        "# Dokumentu biežums\n",
        "def number_of_docs_with_term(word):\n",
        "    found = 0\n",
        "    for i in range(1,work_count+1):\n",
        "        infile = open(\"work_\"+str(i)+\".txt\", \"r\")\n",
        "        for line in infile:\n",
        "            line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "            tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "            if word in tokens:\n",
        "                found += 1\n",
        "                break\n",
        "        infile.close()\n",
        "    return found\n",
        "\n",
        "\n",
        "def doc_appearance(word):\n",
        "    found = number_of_docs_with_term(word)\n",
        "    print(\"Word \\\"{}\\\" was found in {} out of {} files.\".format(word, found, work_count))\n",
        "\n",
        "doc_appearance(\"king\")\n",
        "doc_appearance(\"Romeo\")\n",
        "doc_appearance(\"said\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el6jHzVqLG98"
      },
      "outputs": [],
      "source": [
        "# Vārda biežums\n",
        "def word_frequency(word, doc_id):\n",
        "    freq = 0\n",
        "    infile = open(\"work_\"+str(doc_id)+\".txt\", \"r\")\n",
        "    for line in infile:\n",
        "        line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "        for token in tokens:\n",
        "            if token == word:\n",
        "                freq += 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "def frequency(word, doc_id):\n",
        "    w_freq = word_frequency(word, doc_id)\n",
        "    filename = \"work_\"+str(doc_id)+\".txt\"\n",
        "    print(\"Found word \\\"{}\\\" in file \\\"{}\\\" a total of {} times.\".format(word, filename, w_freq))\n",
        "\n",
        "frequency(\"king\", 28)\n",
        "frequency(\"Romeo\", 28)\n",
        "frequency(\"said\", 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U41Rdet4sfwk"
      },
      "outputs": [],
      "source": [
        "# Calculating tf-idf using formula\n",
        "import numpy as np\n",
        "def tf_idf(word, doc_id):\n",
        "    inverse_doc_freq = np.log((1+work_count)/(number_of_docs_with_term(word)+1))+1\n",
        "    score = word_frequency(word, doc_id) * inverse_doc_freq\n",
        "    return score\n",
        "\n",
        "\n",
        "def print_score(word, doc_id):\n",
        "    filename = \"work_\"+str(doc_id)+\".txt\"\n",
        "    score = tf_idf(word, doc_id)\n",
        "    print(\"The tf-idf score for word \\\"{}\\\" in file \\\"{}\\\" is: {}\".format(word, filename, score))\n",
        "\n",
        "print_score(\"king\", 28)\n",
        "print_score(\"Romeo\", 28)\n",
        "print_score(\"said\", 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRQtizsUTnqB"
      },
      "source": [
        "<br> <br>\n",
        "Alternatīvi varam pielietot scikit-learn bibliotēku tf-idf aprēķinu veikšanai. <br> <br>\n",
        "TfidfVectorizer papildus veic arī rezultātu smoothing (smooth_idf=True) un normalizēšanu(norm='l2'). <br>\n",
        "Tādēļ ar bibliotēku iegūtie tf-idf reultāti atšķiras no iepriekš aprēķinātajiem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQF3ae_aTxHU"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "text_files = []\n",
        "for i in range(1,work_count+1):\n",
        "    text_files.append(\"work_\"+str(i)+\".txt\")\n",
        "\n",
        "text_titles = [text.split(\".\")[0] for text in text_files]\n",
        "\n",
        "\n",
        "# Get tf-idf score data for word\n",
        "def vectorizer_score(word, doc_id):\n",
        "\n",
        "    # Initialize and run TfidfVectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
        "    tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
        "\n",
        "    # Create a DataFrame out of the resulting tf–idf vector\n",
        "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "    tfidf_df = tfidf_df.stack().reset_index()\n",
        "    tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
        "\n",
        "\n",
        "    tfidf_df = tfidf_df[tfidf_df['document'] == 'work_'+str(doc_id)]\n",
        "    print(tfidf_df[tfidf_df['term'] == word], \"\\n\")\n",
        "\n",
        "\n",
        "vectorizer_score(\"king\", 28)\n",
        "vectorizer_score(\"romeo\", 28)\n",
        "vectorizer_score(\"said\", 28)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fasttext"
      ],
      "metadata": {
        "id": "eQefx3DpIUQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fasttext\n",
        "# git clone https://github.com/facebookresearch/fastText.git\n",
        "import fasttext\n"
      ],
      "metadata": {
        "id": "dfylU1lAIX9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skipgram model :\n",
        "model = fasttext.train_unsupervised('work_1.txt', model='skipgram')"
      ],
      "metadata": {
        "id": "czFrB1siLPD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.words)   # list of words in dictionary\n",
        "print(model['king']) # get the vector of the word 'king'"
      ],
      "metadata": {
        "id": "rCH_aWNWMLfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_supervised('work_1.txt')\n",
        "print(model.words)\n",
        "print(model.labels)\n",
        "print(model['wind']) # get the vector of the word 'king'"
      ],
      "metadata": {
        "id": "aYUifyibMWhT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}