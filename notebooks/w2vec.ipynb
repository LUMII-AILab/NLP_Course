{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMT5pKVIle/60Ni/rtqil8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/w2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec modeļa izveide\n",
        "# Building and using Word2vec model"
      ],
      "metadata": {
        "id": "xOp459d616_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teksta lejuplāde un sadalīšana rindiņās\n",
        "\n",
        "Preprocessing - download and line segmentation"
      ],
      "metadata": {
        "id": "RH36CAfpAGIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import re\n",
        "from time import time\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# change to your own path if you have downloaded the file locally\n",
        "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
        "# url = \"https://repository.clarin.lv/repository/xmlui/bitstream/handle/20.500.12574/41/rainis_v20180716.txt?sequence=1&isAllowed=y\"\n",
        "# read file into list of lines\n",
        "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")"
      ],
      "metadata": {
        "id": "SLRTiEwZ1_nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teksta priekšapstrāde - sadalīšana tekstvienībās\n",
        "\n",
        "Tokenization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yed3eXbI2Wpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "for line in lines:\n",
        "   # remove punctuation\n",
        "   line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "\n",
        "   # simple tokenizer\n",
        "   tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "\n",
        "   # only keep lines with at least one token\n",
        "   if len(tokens) > 1:\n",
        "      sentences.append(tokens)"
      ],
      "metadata": {
        "id": "E4zjA3I72dGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Modeļa apmācība\n",
        "\n",
        "Training\n",
        "\n",
        "The parameters:\n",
        "\n",
        "*   min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "*   window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "* size = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
        "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "\n"
      ],
      "metadata": {
        "id": "bBHjtlJd2rm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w2v_model = Word2Vec(\n",
        "         sentences,\n",
        "         min_count=3,   # Ignore words that appear less than this\n",
        "         vector_size=50,       # Dimensionality of word embeddings\n",
        "         sg = 1,        # skipgrams\n",
        "         window=7,      # Context window for words during training\n",
        "         epochs=40)       # Number of epochs training over corpus"
      ],
      "metadata": {
        "id": "whGNCwMt2vk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatīva: apmacība pa soļiem\n",
        "\n",
        "Training in several steps (alternative)"
      ],
      "metadata": {
        "id": "SiziDuQ2GA7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     vector_size=50,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=cores-1)\n",
        "\n",
        "\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "M4atVIppF_3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "id": "GA6Cm0AAIPEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelis darbībā\n",
        "\n",
        "Application"
      ],
      "metadata": {
        "id": "-iQuYz4n3knk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#w2vcat=w2v_model.wv.most_similar('rīta')\n",
        "w2v_model.wv.most_similar('Romeo')"
      ],
      "metadata": {
        "id": "ii2MQPnP3joT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"Romeo\"])"
      ],
      "metadata": {
        "id": "-HvY97BFQjfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.similarity(\"Romeo\", \"love\")"
      ],
      "metadata": {
        "id": "7eHbtZfzQxL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.doesnt_match([\"love\", \"Romeo\", \"cat\"])"
      ],
      "metadata": {
        "id": "4BR1-ixqRX7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citi nopietnāki un mazāk nopietni materiāli:\n",
        "\n",
        "* Tensorflow Word2Vec Tutorial:https://www.tensorflow.org/text/tutorials/word2vec\n",
        "* Gensim Word2Vec Tutorial: https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook#Getting-Started\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BCV5QhcpAV_X"
      }
    }
  ]
}