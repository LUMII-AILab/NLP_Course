{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNA7mj+MdVhFSw6c0wtlPYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/w2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec modeļa izveide\n",
        "# Building and using Word2vec model"
      ],
      "metadata": {
        "id": "xOp459d616_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teksta lejuplāde un sadalīšana rindiņās\n",
        "\n",
        "Preprocessing - download and line segmentation"
      ],
      "metadata": {
        "id": "RH36CAfpAGIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import re\n",
        "from time import time\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# change to your own path if you have downloaded the file locally\n",
        "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
        "#url = \"https://repository.clarin.lv/repository/xmlui/bitstream/handle/20.500.12574/41/rainis_v20180716.txt?sequence=1&isAllowed=y\"\n",
        "\n",
        "# read file into list of lines\n",
        "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")"
      ],
      "metadata": {
        "id": "SLRTiEwZ1_nq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teksta priekšapstrāde - sadalīšana tekstvienībās\n",
        "\n",
        "Tokenization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yed3eXbI2Wpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "for line in lines:\n",
        "   # remove punctuation\n",
        "   line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "\n",
        "   # simple tokenizer\n",
        "   tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "\n",
        "   # only keep lines with at least one token\n",
        "   if len(tokens) > 1:\n",
        "      sentences.append(tokens)"
      ],
      "metadata": {
        "id": "E4zjA3I72dGr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Modeļa apmācība\n",
        "\n",
        "Training\n",
        "\n",
        "The parameters:\n",
        "\n",
        "*   min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "*   window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "* size = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
        "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "\n"
      ],
      "metadata": {
        "id": "bBHjtlJd2rm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w2v_model = Word2Vec(\n",
        "         sentences,\n",
        "         min_count=3,   # Ignore words that appear less than this\n",
        "         vector_size=50,       # Dimensionality of word embeddings\n",
        "         sg = 1,        # skipgrams\n",
        "         window=7,      # Context window for words during training\n",
        "         epochs=40)       # Number of epochs training over corpus"
      ],
      "metadata": {
        "id": "whGNCwMt2vk9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatīva: apmacība pa soļiem\n",
        "\n",
        "Training in several steps (alternative)"
      ],
      "metadata": {
        "id": "SiziDuQ2GA7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     vector_size=50,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=cores-1)\n",
        "\n",
        "\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4atVIppF_3A",
        "outputId": "df572640-0423-4893-d67f-b0af4329aa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to build vocab: 0.01 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA6Cm0AAIPEZ",
        "outputId": "461bb7d4-9e96-4b11-e6f9-f242d8690f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21426316, 52958850)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelis darbībā\n",
        "\n",
        "Application"
      ],
      "metadata": {
        "id": "-iQuYz4n3knk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#w2vcat=w2v_model.wv.most_similar('rīta')\n",
        "w2v_model.wv.most_similar('king')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii2MQPnP3joT",
        "outputId": "2962b138-84ba-46f2-a16a-e69000f875fd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prince', 0.7845737338066101),\n",
              " ('duke', 0.7715505361557007),\n",
              " ('queen', 0.7306444644927979),\n",
              " ('emperor', 0.7019931674003601),\n",
              " ('college', 0.6877387166023254),\n",
              " ('rightful', 0.6712759137153625),\n",
              " ('sovereign', 0.6658298373222351),\n",
              " ('Dauphin', 0.6556982398033142),\n",
              " ('Naples', 0.6545740365982056),\n",
              " ('newness', 0.6544961333274841)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"Romeo\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HvY97BFQjfR",
        "outputId": "088202b0-0292-4404-8c30-6e80614becb8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tybalt', 0.7864425778388977),\n",
              " ('murdered', 0.7040663361549377),\n",
              " ('dead', 0.683326780796051),\n",
              " ('Bassianus', 0.6790679693222046),\n",
              " ('Polonius', 0.6736945509910583),\n",
              " ('mountaineers', 0.6709297299385071),\n",
              " ('Carlisle', 0.6702226996421814),\n",
              " ('Antony', 0.6452981233596802),\n",
              " ('Clarence', 0.6452953815460205),\n",
              " ('banished', 0.6417002081871033)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.similarity(\"Romeo\", \"love\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eHbtZfzQxL4",
        "outputId": "df137f72-f56c-486e-fc85-0ced7110abd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4380147"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.doesnt_match([\"love\", \"Romeo\", \"cat\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4BR1-ixqRX7d",
        "outputId": "aa81b32f-1275-4784-cf9c-55a20130d3e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citi nopietnāki un mazāk nopietni materiāli:\n",
        "\n",
        "* Tensorflow Word2Vec Tutorial:https://www.tensorflow.org/text/tutorials/word2vec\n",
        "* Gensim Word2Vec Tutorial: https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook#Getting-Started\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BCV5QhcpAV_X"
      }
    }
  ]
}