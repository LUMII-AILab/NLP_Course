{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rīkkopas teksta priekšapstrādei | Toolkits for text preprocessing\n",
        "\n",
        "**LV**: Šajā nodarbībā turpināsim izmantot *Python* moduļus darbam ar regulārajām izteiksmēm, un aplūkosim NLP rīkkopas [NLTK](https://www.nltk.org/), [spaCy](https://spacy.io/) un [Stanza](https://stanfordnlp.github.io/stanza/), kā arī [Hugging Face](https://github.com/huggingface/tokenizers/) BPE bibliotēku.\n",
        "\n",
        "Nākamajās nodarbībās redzēsim, ka šeit aplūkotās daudzfunkcionālās rīkkopas ir izmantojamas ne vien teksta priekšapstrādei, bet arī teksta tālākajai analīzei."
      ],
      "metadata": {
        "id": "krvncS3Glj-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the packages\n",
        "!pip install nltk spacy spacy-transformers stanza\n",
        "\n",
        "# Import the packages\n",
        "import nltk\n",
        "import spacy\n",
        "import stanza"
      ],
      "metadata": {
        "id": "YXA9WEdHmvUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98cb5cf-f342-4746-cbf6-75582006afe0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: transformers<4.37.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (4.36.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers) (0.9.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (1.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.4.2)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Darbs ar NLTK datu kopām | Working with NLTK datasets\n",
        "\n",
        "**LV**: NLTK ietver daudzveidīgu teksta korpusu un leksisko resursu kolekciju. Šos korpusus u.c. resursus iespējams lejuplādēt un strādāt ar tiem, izmantojot `nltk.corpus` pakotni. Vairāk informācijas: https://www.nltk.org/data.html\n",
        "\n",
        "---\n",
        "\n",
        "**EN**: NLTK includes a diverse set of preprocessed text corpora and lexical resources that can be downloaded and manipulated using the `nltk.corpus` package. More info: https://www.nltk.org/data.html"
      ],
      "metadata": {
        "id": "cU8ze2lsTuOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the NLTK Downloader CLI\n",
        "nltk.download()\n",
        "\n",
        "# Download the Brown corpus (see https://en.wikipedia.org/wiki/Brown_Corpus)\n",
        "nltk.download('brown')\n",
        "\n",
        "# Import the Brown corpus\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "peaPz53jKAyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the size of the corpus\n",
        "total_size = len(brown.words())\n",
        "print(\"Total number of words:\", total_size)\n",
        "\n",
        "# List the text categories (genres) in the corpus\n",
        "for cat in brown.categories():\n",
        "    doc_per_cat = len(brown.fileids(categories=cat))\n",
        "    words_per_cat = len(brown.words(categories=cat))\n",
        "    cat_percentage = words_per_cat / total_size\n",
        "\n",
        "    print(f'\\t{cat:10}\\t{doc_per_cat}\\t{words_per_cat}\\t{cat_percentage:.2%}')"
      ],
      "metadata": {
        "id": "xa0iGN2sLBuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LV**: Piezīme: salīdziniet līdzsvarotā *Brown* (1964!!) korpusa žanru proporcijas un līdzvarotā latviešu valodas korpusa [LVK2022](https://korpuss.lv/id/LVK2022) [žanru proporcijas](https://nosketch.korpuss.lv/#wordlist?corpname=LVK2022&tab=attribute&onecolumn=1&wlattr=doc.section&wlminfreq=1&include_nonwords=1&showresults=1)."
      ],
      "metadata": {
        "id": "ofohRnK3pgIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Work with a sub-corpus\n",
        "print(\"humor:\", brown.fileids(categories='humor'))\n",
        "print(\"cr01:\", len(brown.words('cr01')), '\\n')\n",
        "\n",
        "# NLTK supports reading a corpus as a raw/annotated text..\n",
        "print(\"Raw text with POS tags:\", brown.raw('cr01').split('\\n\\n')[0], '\\n')\n",
        "\n",
        "# ..as well as a list of words, sentences, or paragraphs\n",
        "print(\"Words:\", brown.words('cr01'))\n",
        "print(\"Sentences:\", brown.sents('cr01'))\n",
        "print(\"Paragraphs:\", brown.paras('cr01'))"
      ],
      "metadata": {
        "id": "CheE1cfgjmul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dalīšana tekstvienībās un teikumos | Tokenization and sentence splitting"
      ],
      "metadata": {
        "id": "4q4zPBBZThUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegEx\n",
        "\n",
        "**LV**: Vispirms aplūkosim, kā varam mēģināt risināt tekstvienību un teikumu segmentēšanu, izmantojot tikai regulārās izteiksmes."
      ],
      "metadata": {
        "id": "hEAOZk3hb954"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's begin with the built-in re module\n",
        "import re\n",
        "\n",
        "# We will need BeautifulSoup again\n",
        "!pip install beautifulsoup4\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "mK1YYrcgjagF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download some non-NLTK corpora:\n",
        "!wget -O \"Rainis.txt\" https://repository.clarin.lv/repository/xmlui/bitstream/handle/20.500.12574/41/rainis_v20180716.txt\n",
        "!wget -O \"Romeo.txt\" https://www.gutenberg.org/cache/epub/1112/pg1112.txt"
      ],
      "metadata": {
        "id": "F6k6O5A8oDRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, upload files from your local file system\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "RLW1bSBkwCnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and clean up the downloaded corpora\n",
        "\n",
        "with open(\"Romeo.txt\", mode='r', encoding='utf-8') as file_en:\n",
        "    text_en = file_en.read()\n",
        "\n",
        "with open(\"Rainis.txt\", mode='r', encoding='utf-8') as file_lv:\n",
        "    text_lv = file_lv.read().split('</doc>')[0]\n",
        "    text_lv = BeautifulSoup(text_lv, 'html.parser').text\n",
        "\n",
        "text_en = text_en.strip()\n",
        "text_lv = text_lv.strip()"
      ],
      "metadata": {
        "id": "jYLWWe6uXQWo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize white spaces (incl. line breaks)\n",
        "text_en = re.sub(r'\\s+', ' ', text_en)\n",
        "text_lv = re.sub(r'\\s+', ' ', text_lv)\n",
        "\n",
        "print(text_en)\n",
        "print(text_lv)"
      ],
      "metadata": {
        "id": "016CQsl5EQ8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified (and lossy) tokenization\n",
        "\n",
        "words_en = re.split(r'\\W+', text_en)\n",
        "print(len(words_en), words_en)\n",
        "\n",
        "words_lv = re.split(r'\\W+', text_lv)\n",
        "print(len(words_lv), words_lv)\n",
        "\n",
        "words_en = re.findall('[A-z]+', text_en) # vs. \\w+\n",
        "print(len(words_en), words_en) # FIXME: https://en.wikipedia.org/wiki/ASCII#Character_set\n",
        "\n",
        "words_lv = re.findall('[A-Za-zĀāČčĒēĢģĪīĶķĻļŅņŌōŖŗŠšŪūŽž]+', text_lv)\n",
        "print(len(words_lv), words_lv)"
      ],
      "metadata": {
        "id": "zm2fE8RXo-on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LV**: Šādi izgūstam vārdus, bet pazaudējam citas tekstvienības un sadalām kompleksus vārdus (piem., \"Covid-19\").\n",
        "\n",
        "Mēģināsim uzlabot šablonu un izmantosim ārējo `regex` moduli, kas ir savietojams ar iebūvēto `re`, var uzlabot ātrdarbību, kā arī nodrošina papildu iespējas, piemēram, ērti definējamas *Unicode* rakstzīmju klases: https://en.wikipedia.org/wiki/Unicode_character_property#General_Category."
      ],
      "metadata": {
        "id": "l3soSF83Bl4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install regex\n",
        "\n",
        "import regex"
      ],
      "metadata": {
        "id": "plMKo3R6YrUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_en = regex.findall('\\p{L}+|\\p{P}', text_en)\n",
        "words_lv = regex.findall('\\p{L}+|\\p{P}', text_lv)\n",
        "\n",
        "print(text_en + '\\n' + ' '.join(words_en) + '\\n' + str(len(words_en)) + '\\n')\n",
        "print(text_lv + '\\n' + ' '.join(words_lv) + '\\n' + str(len(words_lv)) + '\\n')\n",
        "\n",
        "# TODO: wee'l, o'th, peoples', eye-sight, sald-sāpīgs, zil-ziediņi, sarkan-zili-zaļi"
      ],
      "metadata": {
        "id": "9ovaEaUs7ll0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LV**: Mēģināsim dalīt tekstu teikumos un rindkopās, izmantojot vienkāršas regulārās izteiksmes.\n",
        "\n",
        "---\n",
        "\n",
        "**EN**: An attempt to split a text into sentences and paragraph using simple regular expressions."
      ],
      "metadata": {
        "id": "cpCA2a4hPltV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Re-load text_en and text_lv\n",
        "\n",
        "# Naive assumptions:\n",
        "# (1) the full-stop characters indicate end-of-sentence;\n",
        "# (2) line breaks indicate end-of-paragraph.\n",
        "\n",
        "# Sentence splitting\n",
        "text_en = regex.sub(r'(?<=[.!?])[ ]+', '[SENT]', text_en)\n",
        "text_lv = regex.sub(r'(?<=[.!?])[ ]+', '[SENT]', text_lv)\n",
        "\n",
        "# Paragraph splitting\n",
        "text_en = regex.sub(r'\\n+([ ]+\\n+)?', '[PARA]', text_en)\n",
        "text_lv = regex.sub(r'\\n+([ ]+\\n+)?', '[PARA]', text_lv)\n",
        "\n",
        "# Normalization of the remaining white spaces\n",
        "text_en = regex.sub(r'\\s+', ' ', text_en)\n",
        "text_lv = regex.sub(r'\\s+', ' ', text_lv)\n",
        "\n",
        "print(text_en)\n",
        "print(text_lv)"
      ],
      "metadata": {
        "id": "q-1X48tXphxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK\n",
        "\n",
        "**EN**: The NLTK sentence splitter (`sent_tokenize`) and tokenizer (`word_tokenize`) use pre-trained models and heuristics that take into account the complexity and irregularities of natural language text.\n",
        "\n",
        "Before using these functions, ensure you have downloaded the *Punkt* tokenizer models. This NLTK data package includes a pre-trained model for English and some other languages.\n",
        "\n",
        "*Punkt* uses an unsupervised algorithm to build a tokenization and sentence splitting model. It must be trained on a large plain-text corpus in the target language.\n",
        "\n",
        "See https://www.nltk.org/api/nltk.tokenize.punkt.html"
      ],
      "metadata": {
        "id": "thuCr0KGUD9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# To list the available Punkt models:\n",
        "\n",
        "import os\n",
        "\n",
        "punkt_path = os.path.join(nltk.data.find('tokenizers/punkt'), '')\n",
        "punkt_files = [f for f in os.listdir(punkt_path) if f.endswith('.pickle')]\n",
        "\n",
        "print([f.replace('.pickle', '') for f in punkt_files])"
      ],
      "metadata": {
        "id": "yIAXhgHJpw-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_text_en = '''\n",
        "    Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
        "    do not mark sentence boundaries. And sometimes sentences\n",
        "    start with non-capitalized words. i is a good variable\n",
        "    name.\n",
        "    You may copy it, give it away or re-use it under the terms\n",
        "    of the Project Gutenberg License on-line\n",
        "    at www.gutenberg.org. If you're not in the US,\n",
        "    you'll have to check the laws where you are located\n",
        "    before using this e-Book.\n",
        "    '''\n",
        "\n",
        "short_text_lv = '''\n",
        "    TĀLAS NOSKAŅAS ZILĀ VAKARĀ\n",
        "    Vēlreiz garā tuvajiem mīļā dzimtenē sirsnīgus sveicienus!\n",
        "    Daudz simtu jūdžu tāļumā,\n",
        "    aiz tīreļiem, purviem un siliem,\n",
        "    guļ mana dzimtene diendusā.\n",
        "    Tā aizsegta debešiem ziliem,\n",
        "    zil-saulainiem debešu palagiem\n",
        "    pret dvesmām un strāvām, un negaisiem...\n",
        "    Piemērs tapis 2024. gadā. Šis u.c. piemēri.\n",
        "    '''"
      ],
      "metadata": {
        "id": "Hzm1eKh3XwGg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "sents_en = sent_tokenize(short_text_en)\n",
        "sents_lv = sent_tokenize(short_text_lv, language='polish')\n",
        "\n",
        "for s in sents_en:\n",
        "    s = regex.sub(r'\\s+', ' ', s.strip())\n",
        "    w = word_tokenize(s)\n",
        "    print(s + '\\n' + ' '.join(w) + '\\n')\n",
        "\n",
        "print(\"-----\\n\")\n",
        "\n",
        "for s in sents_lv:\n",
        "    s = regex.sub(r'\\s+', ' ', s.strip())\n",
        "    w = word_tokenize(s, language='polish')\n",
        "    print(s + '\\n' + ' '.join(w) + '\\n')\n",
        "\n",
        "# TODO: a potential mini-project - train and use a Punkt model for another language"
      ],
      "metadata": {
        "id": "rUjGBMC_uz_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy"
      ],
      "metadata": {
        "id": "YYR3KFe2g5CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we must download language models trained for the languages of interest.\n",
        "# See https://spacy.io/usage/models\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download lt_core_news_sm\n",
        "!python -m spacy download xx_sent_ud_sm\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_lt = spacy.load(\"lt_core_news_sm\")\n",
        "nlp_xx = spacy.load(\"xx_sent_ud_sm\")\n",
        "\n",
        "# Assumption: LT should be the closest one to LV.\n",
        "# Compare to the multilingual (XX) models.\n",
        "# Compare to the large models (lg and rtf)!\n",
        "\n",
        "# PS. The xx_ent_wiki_sm model does not include a component for setting sentence\n",
        "# boundaries by default - the sentencizer has to be added to the nlp_xx pipeline\n",
        "# before using it: nlp_xx.add_pipe('sentencizer').\n",
        "# Still, the xx_sent_ud_sm model is more accurate for LV sentence splitting."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCm4V6-7C8C4",
        "outputId": "5cd65ea0-9859-4e43-8387-efe85f0caf6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting lt-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/lt_core_news_sm-3.7.0/lt_core_news_sm-3.7.0-py3-none-any.whl (13.2 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from lt-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->lt-core-news-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('lt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting xx-sent-ud-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.7.0/xx_sent_ud_sm-3.7.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from xx-sent-ud-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->xx-sent-ud-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers\n",
        "!python -m spacy download en_core_web_trf\n",
        "nlp_en = spacy.load(\"en_core_web_trf\")"
      ],
      "metadata": {
        "id": "i-0RelIRUB4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download lt_core_news_lg\n",
        "nlp_lt = spacy.load(\"lt_core_news_lg\")"
      ],
      "metadata": {
        "id": "VVxYqBY5Y-An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass a plain-text to the respective language model,\n",
        "# which returns a processed spaCy document.\n",
        "doc_en = nlp_en(short_text_en) # TODO: vs. en-trf\n",
        "doc_lv = nlp_xx(short_text_lv) # TODO: vs. lt-lg\n",
        "\n",
        "# For each sentence in the EN text, tokenize it and pretty-print\n",
        "for sent in doc_en.sents:\n",
        "    sent_text = ' '.join(tok.text for tok in sent)\n",
        "    print(regex.sub(r'\\s+', ' ', sent_text.strip()))\n",
        "\n",
        "print(\"\\n-----\")\n",
        "\n",
        "# For each sentence in the LV text, tokenize it and pretty-print\n",
        "for sent in doc_lv.sents:\n",
        "    sent_text = ' '.join(tok.text for tok in sent)\n",
        "    print(regex.sub(r'\\s+', ' ', sent_text.strip()))"
      ],
      "metadata": {
        "id": "UkMaL10Ig3wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stanza"
      ],
      "metadata": {
        "id": "HTrruniedB2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the necessary models\n",
        "# See https://stanfordnlp.github.io/stanza/performance.html\n",
        "\n",
        "stanza.download('en')\n",
        "stanza.download('lv')"
      ],
      "metadata": {
        "id": "uPQ3sbbodFj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create NLP pipelines\n",
        "nlp_en = stanza.Pipeline('en')\n",
        "nlp_lv = stanza.Pipeline('lv')"
      ],
      "metadata": {
        "id": "3X7Iomz_e_r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the texts\n",
        "doc_en = nlp_en(short_text_en)\n",
        "doc_lv = nlp_lv(short_text_lv)"
      ],
      "metadata": {
        "id": "mDYQ3Mglfoqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the EN sentences and tokens\n",
        "for i, sent in enumerate(doc_en.sentences):\n",
        "    print(i, ' '.join(tok.text for tok in sent.tokens))"
      ],
      "metadata": {
        "id": "8Mb8Vg5AmDWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the LV sentences and tokens\n",
        "for i, sent in enumerate(doc_lv.sentences):\n",
        "    print(i, ' '.join(tok.text for tok in sent.tokens))"
      ],
      "metadata": {
        "id": "3PeJ8QvhmBDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE tokenizācija\n",
        "\n",
        "*HuggingFace* BPE tokenizācijas bibliotēka: https://github.com/huggingface/tokenizers/"
      ],
      "metadata": {
        "id": "xa1-4lOGhiwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ],
      "metadata": {
        "id": "RdBMFD2xiy_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tokenizer with the BPE model\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Use a pre-tokenizer to split the input into words\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Create a trainer for the tokenizer\n",
        "trainer = BpeTrainer(vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\"])\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train(files=[\"Rainis.txt\"], trainer=trainer)\n",
        "\n",
        "print(\"Vocabulary size:\", tokenizer.get_vocab_size())\n",
        "\n",
        "# Save the tokenizer on disk\n",
        "tokenizer.save(\"Rainis_BPR_tokenizer.json\")"
      ],
      "metadata": {
        "id": "OCISYVGOhr9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"Rainis_BPR_tokenizer.json\")\n",
        "\n",
        "test = tokenizer.encode(short_text_lv)\n",
        "print(test.tokens) # Compare to the Rainis' word frequency list at Korpuss.lv\n",
        "print(test.ids)\n",
        "\n",
        "print()\n",
        "\n",
        "test = tokenizer.encode(\"Цей текст українською мовою.\")\n",
        "print(test.tokens) # See Rainis.txt\n",
        "print(test.ids)"
      ],
      "metadata": {
        "id": "Hbk40n67mRht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Celmošana | Stemming"
      ],
      "metadata": {
        "id": "6nFhzDGSkErb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK PorterStemmer for English\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "test_sentence_en = \"These boys running were fastest off-road runners when they smiled.\"\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(t) for t in word_tokenize(test_sentence_en)]\n",
        "\n",
        "print(' '.join(stemmed_tokens))"
      ],
      "metadata": {
        "id": "b4gn2HRBkZUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11173b7d-124f-49a4-dd4d-a220f5faea37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "these boy run were fastest off-road runner when they smile .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK SnowballStemmer supports 15+ languages\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "print(SnowballStemmer.languages)"
      ],
      "metadata": {
        "id": "9cEB8PxXmdlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f1b87c-f6f9-4f67-baa4-cd3d9c0344aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "stemmed_tokens = [stemmer.stem(t) for t in word_tokenize(test_sentence_en)]\n",
        "print(\"EN:\", ' '.join(stemmed_tokens))\n",
        "\n",
        "test_sentence_se = \"Dessa fiskar smakar gott.\"\n",
        "\n",
        "stemmer = SnowballStemmer('swedish')\n",
        "stemmed_tokens = [stemmer.stem(t) for t in word_tokenize(test_sentence_se)]\n",
        "print(\"SE:\", ' '.join(stemmed_tokens))"
      ],
      "metadata": {
        "id": "3XQtgatgmiA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatizācija"
      ],
      "metadata": {
        "id": "mDpnXZkWkjAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization with NLTK\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in word_tokenize(test_sentence_en)]\n",
        "\n",
        "print(test_sentence_en)\n",
        "print(' '.join(lemmatized_tokens))\n",
        "\n",
        "# TODO: POS-tagging first..."
      ],
      "metadata": {
        "id": "Dzi08jDPksk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Corresponding POS tags (WordNet tags)\n",
        "pos_tags = ['s', 'n', 'v', 's', 'a', 'a', 'n', 'r', 's', 'v']\n",
        "# 's' tags are incorrect - used instead of None\n",
        "\n",
        "for w, p in zip(word_tokenize(test_sentence_en), pos_tags):\n",
        "    lemma = lemmatizer.lemmatize(w, pos=p)\n",
        "    print(f\"{w:10}\\t{lemma}\")"
      ],
      "metadata": {
        "id": "FbrTDdfozaKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization with Spacy\n",
        "\n",
        "# Load the multilingual tokenizer, tagger, lemmatizer, etc.\n",
        "#nlp_xx = spacy.load(\"xx_sent_ud_sm\")\n",
        "\n",
        "# Process the EN sentence\n",
        "for token in nlp_en(test_sentence_en):\n",
        "    print(f\"{token.text}\\t{token.lemma_}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Process the SE sentence\n",
        "for token in nlp_xx(test_sentence_se):\n",
        "    print(f\"{token.text}\\t{token.lemma_}\")"
      ],
      "metadata": {
        "id": "sbs_1Dvsk6kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization with Stanza\n",
        "\n",
        "stanza.download('en')\n",
        "stanza.download('sv')\n",
        "\n",
        "nlp_en = stanza.Pipeline(lang='en')\n",
        "nlp_sv = stanza.Pipeline(lang='sv')"
      ],
      "metadata": {
        "id": "5xgM5zJC53YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_en = nlp_en(test_sentence_en)\n",
        "\n",
        "for s in doc_en.sentences:\n",
        "    for w in s.words:\n",
        "        print(f\"{w.text}\\t{w.lemma}\")"
      ],
      "metadata": {
        "id": "WOuo7cxu4667"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_sv = nlp_sv(test_sentence_se)\n",
        "\n",
        "for s in doc_sv.sentences:\n",
        "    for w in s.words:\n",
        "        print(f\"{w.text}\\t{w.lemma}\")"
      ],
      "metadata": {
        "id": "p-2nGeY_6LRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}