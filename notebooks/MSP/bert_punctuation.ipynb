{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1f7fc1",
   "metadata": {
    "id": "3c1f7fc1"
   },
   "source": [
    "# Comma Restoration with Token Classification using BERT\n",
    "This notebook fine-tunes a transformer encoder (e.g., mBERT, LvBERT) to restore commas in text as a token classification task. Each token receives one of two labels: COMMA (a comma should follow this word) or O (no comma). At inference time, existing commas are stripped, labels are predicted, and the sentence is rebuilt by inserting commas after tokens predicted as COMMA.\n",
    "Models to try:\n",
    "- https://huggingface.co/google-bert/bert-base-multilingual-cased\n",
    "- https://huggingface.co/AiLab-IMCS-UL/lvbert\n",
    "- https://huggingface.co/FacebookAI/xlm-roberta-base\n",
    "- https://huggingface.co/EMBEDDIA/litlat-bert\n",
    "- https://huggingface.co/jhu-clsp/mmBERT-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1891982",
   "metadata": {
    "id": "f1891982"
   },
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5JF6qA_UuX3s",
   "metadata": {
    "id": "5JF6qA_UuX3s"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Weights & Biases to enable logging and experiment tracking.\n",
    "# Comment out the following lines if you don't want to use W&B.\n",
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H872W0AqxhKQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H872W0AqxhKQ",
    "outputId": "1d2dcb14-0920-4935-836b-78c8dfc5bfd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "CUDA device: Tesla T4 (7, 5) bf16 False\n",
      "Memory: 14992.12 MB free / 15095.06 MB total\n"
     ]
    }
   ],
   "source": [
    "# Check if a CUDA device is available\n",
    "!pip install torch\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0), torch.cuda.get_device_capability(0), 'bf16', torch.cuda.is_bf16_supported(False))\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    print(f'Memory: {free_mem / 1024 ** 2:.2f} MB free / {total_mem / 1024 ** 2:.2f} MB total')\n",
    "else:\n",
    "    print('No CUDA device available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68105191",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68105191",
    "outputId": "d432c1f9-70c4-4fd7-a230-3e11d0115667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n",
      "pip 24.1.2 from /usr/local/lib/python3.12/dist-packages/pip (python 3.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.8.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.9)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "!pip -V\n",
    "!pip install numpy transformers[torch] scikit-learn datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fb23e",
   "metadata": {
    "id": "5a1fb23e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60237262",
   "metadata": {
    "id": "60237262",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Prepare dataset\n",
    "Raw sentences from the Latvian Universal Dependencies (LVTB) corpus: https://universaldependencies.org/treebanks/lv_lvtb/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ac0fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb6ac0fa",
    "outputId": "5cc90967-369c-46c9-9d70-ef8ef2f6e0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lengths before filtering: DEV 2080 TRAIN 15055\n",
      "Dataset sentence lengths: DEV 1912 TRAIN 13811\n",
      "To tu man stāstīji jau pirms divām nedēļām.\n",
      "Ka pieticis tikai autobusa biļetei un barankām.\n",
      "Uz skatuves kāpa skolas koris, pēc tam uzstājās arī dramatiskā pulciņa dalībnieki un divi bērnudārza audzēkņi.\n",
      "Burka esot jāizdekorē ar dillēm, mārrutku lapu, upeņu zariņu un ķiploka pusdaiviņām, jāsaliek gurķīši un jāaplej ar verdošu ūdeni, kurā iebērta ēdamkarote cukura un ēdamkarote sāls.\n",
      "Izaugs sava raža, nevajadzēs lieku reizi braukt uz tirgu.\n"
     ]
    }
   ],
   "source": [
    "def fetch_ud_texts(split, seed=42):\n",
    "    conllu = requests.get(f'https://raw.githubusercontent.com/UniversalDependencies/UD_Latvian-LVTB/r2.16/lv_lvtb-ud-{split}.conllu').text\n",
    "    texts = [line[9:].strip() for line in conllu.splitlines() if line.startswith('# text = ')]\n",
    "    if seed:\n",
    "        import random\n",
    "        random.Random(seed).shuffle(texts)\n",
    "    return texts\n",
    "\n",
    "def prepare_data(max_chars=200, dev_txt='dev.txt', train_txt='train.txt'):\n",
    "    # Download UD Latvian splits, filter by mBERT token count, and save plain .txt files.\n",
    "    dev_texts = fetch_ud_texts('dev')\n",
    "    train_texts = fetch_ud_texts('train')\n",
    "\n",
    "    if max_chars:\n",
    "        # Filter out long sentences to avoid truncation\n",
    "        print('Sentence lengths before filtering:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
    "        dev_texts = [t for t in dev_texts if len(t) <= max_chars]\n",
    "        train_texts = [t for t in train_texts if len(t) <= max_chars]\n",
    "    print('Dataset sentence lengths:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
    "\n",
    "    with open(dev_txt, 'w') as f:\n",
    "        for t in dev_texts: f.write(t + '\\n')\n",
    "    with open(train_txt, 'w') as f:\n",
    "        for t in train_texts: f.write(t + '\\n')\n",
    "\n",
    "    return dev_texts, train_texts\n",
    "\n",
    "dev_texts, train_texts = prepare_data()\n",
    "print(*train_texts[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df7d8",
   "metadata": {
    "id": "c18df7d8"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d7f30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b87d7f30",
    "outputId": "b8af6e65-53a9-4270-8473-78a0a42723c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vēl', 'O'),\n",
       " ('9', 'O'),\n",
       " ('%', 'O'),\n",
       " ('sacīja', 'COMMA'),\n",
       " (' ka', 'O'),\n",
       " ('nav', 'O'),\n",
       " ('izlēmuši', 'O'),\n",
       " ('kā', 'O'),\n",
       " ('balsot', 'COMMA'),\n",
       " (' bet', 'O'),\n",
       " ('3', 'COMMA'),\n",
       " ('2', 'O'),\n",
       " ('%', 'O'),\n",
       " ('atteicās', 'O'),\n",
       " ('atbildēt', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    # Tokenizes string into words and punctuation tokens.\n",
    "    return re.findall(r'\\s*(?:\\w+|\\S)', s)\n",
    "\n",
    "def tokenize_with_comma_labels(s):\n",
    "    tokens_with_labels = re.findall(r'(\\s*\\w+|[^\\s,])\\s*(,+)?', s)\n",
    "    tokens_with_labels = [(tok, 'COMMA' if comma else 'O') for tok, comma in tokens_with_labels]\n",
    "    return tokens_with_labels\n",
    "\n",
    "def remove_commas(s) -> str:\n",
    "    return re.sub(r'\\s*,+\\s*', ' ', s)\n",
    "\n",
    "tokenize_with_comma_labels('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2d561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5f2d561",
    "outputId": "ba510475-66ad-4e62-b678-60c73b7bcadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer stats AiLab-IMCS-UL/lvbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sample: {'input_ids': [2, 574, 684, 70, 417, 5, 16, 35, 29811, 24, 4622, 5, 27, 168, 5, 146, 70, 6862, 4850, 6, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Encoded sample - subword units: ['[CLS]', 'Vēl', '9', '%', 'sacīja', ',', 'ka', 'nav', 'izlēmuši', 'kā', 'balsot', ',', 'bet', '3', ',', '2', '%', 'atteicās', 'atbildēt', '.', '[SEP]']\n",
      "Max 65, min 3, avg 21.21772500181015\n",
      "95% length: 40\n",
      "99% length: 48\n",
      "99.9% length: 56\n",
      "Tokenizer stats jhu-clsp/mmBERT-small\n",
      "Encoded sample: {'input_ids': [2, 744, 229673, 235248, 235315, 235358, 6817, 236073, 1663, 235269, 5675, 5103, 9417, 135924, 2704, 27536, 52635, 70402, 562, 235269, 1285, 235248, 235304, 235269, 235284, 235358, 41643, 520, 28688, 696, 137369, 235265, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Encoded sample - subword units: ['<bos>', '▁V', 'ēl', '▁', '9', '%', '▁sac', 'ī', 'ja', ',', '▁ka', '▁nav', '▁iz', 'lē', 'mu', 'ši', '▁kā', '▁bals', 'ot', ',', '▁bet', '▁', '3', ',', '2', '%', '▁atte', 'ic', 'ās', '▁at', 'bildēt', '.', '<eos>']\n",
      "Max 90, min 3, avg 33.86677286221128\n",
      "95% length: 66\n",
      "99% length: 74\n",
      "99.9% length: 82\n"
     ]
    }
   ],
   "source": [
    "def test_tokenization(model=None):\n",
    "    s = 'Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.'\n",
    "    if model:\n",
    "        print('Tokenizer stats', model)\n",
    "        t = AutoTokenizer.from_pretrained(model)\n",
    "        print('Encoded sample:', t(s))\n",
    "        print('Encoded sample - subword units:', t.convert_ids_to_tokens(t.encode(s)))\n",
    "        lengths = sorted([len(t.encode(seq)) for seq in train_texts])\n",
    "        print(f'Max {max(lengths)}, min {min(lengths)}, avg {sum(lengths)/len(lengths)}')\n",
    "        print(f'95% length: {lengths[int(len(lengths) * 0.95)]}')\n",
    "        print(f'99% length: {lengths[int(len(lengths) * 0.99)]}')\n",
    "        print(f'99.9% length: {lengths[int(len(lengths) * 0.999)]}')\n",
    "\n",
    "test_tokenization('AiLab-IMCS-UL/lvbert')\n",
    "test_tokenization('jhu-clsp/mmBERT-small')\n",
    "\n",
    "\n",
    "LABELS = ['O', 'COMMA']\n",
    "LABEL2ID = {name: i for i, name in enumerate(LABELS)}\n",
    "ID2LABEL = {i: name for i, name in enumerate(LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dbedb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e61dbedb",
    "outputId": "860e9e1e-4fb6-46e3-dd97-4e812493d790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:          ('Viens', ' divi', '.')\n",
      "WORD_LABELS:    ('COMMA', 'O', 'O')\n",
      "WORD_IDS:       [None, 0, 1, 2, None]\n",
      "TOKEN_IDS:      [2, 1394, 516, 6, 3]\n",
      "TOKENS:         ['[CLS]', 'Viens', 'divi', '.', '[SEP]']\n",
      "ALIGNED_LABELS: [-100, 1, 0, 0, -100]\n",
      "{'input_ids': [2, 1394, 516, 6, 3], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [-100, 1, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text into subwords and align word-level labels to the correct subword positions\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, words, word_labels=None, label2id=None, debug=False, return_tensors=None):\n",
    "    # Tokenize with word boundaries preserved\n",
    "    enc = tokenizer(\n",
    "        list(words),\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=return_tensors,\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    # Map each token back to its source word index\n",
    "    word_ids = enc.word_ids()  # one per token position (None for specials)\n",
    "\n",
    "    # Figure out which token is the last subword of each word. Assign word labels only there; others get -100.\n",
    "    # HF Trainer and loss functions (like cross-entropy) automatically ignore -100, so you don't need to modify loss computation.\n",
    "    if word_labels is not None:\n",
    "        IGNORE = -100\n",
    "        labels = [IGNORE] * len(word_ids)\n",
    "        for i, wid in enumerate(word_ids):\n",
    "            if wid is None:\n",
    "                continue\n",
    "            next_wid = word_ids[i+1] if i+1 < len(word_ids) else None\n",
    "            if wid != next_wid:\n",
    "                # last subword of this word: assign the word label\n",
    "                labels[i] = label2id[word_labels[wid]]\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    if debug:\n",
    "        input_ids = enc['input_ids']\n",
    "        if return_tensors == 'pt':\n",
    "            input_ids = input_ids.tolist()[0]\n",
    "        print('WORDS:         ', words)\n",
    "        print('WORD_LABELS:   ', word_labels)\n",
    "        print('WORD_IDS:      ', word_ids)\n",
    "        print('TOKEN_IDS:     ', input_ids)\n",
    "        print('TOKENS:        ', tokenizer.convert_ids_to_tokens(input_ids))\n",
    "        print('ALIGNED_LABELS:', labels)\n",
    "\n",
    "    r = {\n",
    "        'input_ids': enc['input_ids'],\n",
    "        'attention_mask': enc['attention_mask'],\n",
    "    }\n",
    "    if labels is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            labels = torch.tensor([labels], dtype=torch.long)\n",
    "        r['labels'] = labels\n",
    "    return r\n",
    "\n",
    "print(tokenize_and_align_labels(AutoTokenizer.from_pretrained('AiLab-IMCS-UL/lvbert'), *zip(*tokenize_with_comma_labels('Viens, divi.')), LABEL2ID, debug=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L7mOk3y4v9Wc",
   "metadata": {
    "id": "L7mOk3y4v9Wc"
   },
   "source": [
    "# Tokenize and format dataset for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd737c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "0d696e3d73c341f2b77e5652e8434da6",
      "489a20734b0242828620a8817e8dc555",
      "f43fcc0f8c0d4705afcb469110857e41",
      "b8dfa9b1e2fc40a2ae81b2838aeb7973",
      "e935f3fc5598427d9605dea3da4a06bd",
      "980452d3f8874dc793e2d673be51fe22",
      "73986d552127441a892fe0aa2de633fc",
      "b3fa8e7d95d343fe9021e855a467d2c7",
      "9e3bb5e8fcf64ca7ad27df005cdcf6e8",
      "b0ca489d812e4265a8a26436c0095f2b",
      "67f24305db9b4810b39af30f67522475",
      "31a4c33d10ae4f9bb03fa0211099f764",
      "3e474cffa0614e24b432bc7dc52b7108",
      "d39d649fe9524e09a37e7a0d1baaf4d3",
      "e6fdd5baa17e4f03872789d9c526ff73",
      "d03d9b43a4564ad0967145281b9bcb81",
      "acdd8896d9fd43848442c646a64e23d8",
      "865f1e26361e48debf98220c8e775fb2",
      "4b49248331024bb0a2caa791ab4fa719",
      "2db68ae1a3c0482a9f132258d3bfe5c2",
      "24700ab74095431f94388447a0d56e25",
      "ef89d61fc54549ec854c30e01cf23e39",
      "5974fc0b6ccd4992a8944d75b31ce89f",
      "f68c6082a57f452abefe2b24443c78eb",
      "90b3ed009d2d436abbbe1e3fb09170d1",
      "b923207394ba447d92299cf91cc73323",
      "89f951ddce0449a79ef29c56e68bb8d1",
      "065868f92a4e4f74b6ec723d2787d9f6",
      "e431d4165cbb4016be64785f820c1745",
      "fe859de877c74798a0573a798c4e0403",
      "52b6321bc10b402f9853f5a83d9bbff4",
      "1ba445e022144729a76b3fbfd90fc970",
      "82c32a54cba04913b14f179dc22b6448",
      "9ec0ae92c66e4680a5adb5eac45d0cde",
      "fcc6d47a227348a19ece5e0ea4c4c56c",
      "da64302695f44534870e52031f7f4541",
      "f2b26b73ec1f45e6bdb2fc78ac6c2a5f",
      "7aad8fabd0554b5a924bc71704e69a58",
      "decc085dd00e4cf4b65b17119e88ced3",
      "36b711696e014fa4909671b9f4d85163",
      "2b8dbaebc1af4403a06857d1fa918cce",
      "ec3655b328c44e0e9a6b5a7bffc286f7",
      "2e5b65059d9346779de549555f4ac500",
      "04113054a6af4f4193223901851ce036",
      "ad7f733d613641338fd4eb5c29eb64e5",
      "140334d700ea44e494392f0b890ee34c",
      "7bdb98f93abd4f608acb750b6adef4de",
      "46e7e3718dc14578a08ab5acafa70dee",
      "e6f05b7f160846d3a3daf77ac73c44bb",
      "077875320a8d4cfe8dd090dff52ac3dc",
      "0d2a4526e8024e58b71c0c102ca11f2c",
      "1a82a9e8ead94b8fbfcefc3210ff5ca5",
      "4726cbd948e94bf5b65b5830019ab152",
      "36b8d1c365eb4a41b65825e2e0c5e5dc",
      "7be2694f279248588d2c4c7f6c94ab96",
      "05ca0febe4da494f9a70ce91746034b5",
      "a5b9d594616c4094b918e0ee7c5e7f77",
      "49ab230413624c1f9c4de0e4dc7daf32",
      "dbeb424b04164a829c6a9ff0ae425a28",
      "b3a3e8bf26a24ac4a9c55a10b0549180",
      "12ead0351502438caa3860a613ebadf6",
      "297424ec87a2483f93f92aaf0d4550d7",
      "71ba4030918f429c9fe0a93a3924c68a",
      "b9a9a553b55c4807bac67493a6407020",
      "0df5543e35744cf3bc5fcc5430715618",
      "81bd75ec78b044b2b60a17ff64304656"
     ]
    },
    "id": "4bd737c4",
    "outputId": "ce886991-23eb-4714-e27a-279c79a7c98d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d696e3d73c341f2b77e5652e8434da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a4c33d10ae4f9bb03fa0211099f764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5974fc0b6ccd4992a8944d75b31ce89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec0ae92c66e4680a5adb5eac45d0cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7f733d613641338fd4eb5c29eb64e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ca0febe4da494f9a70ce91746034b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,   317,   277,   100, 26927,    38,   134,  1516,  7068,     6,\n",
      "             3,     0,     0,     0,     0],\n",
      "        [    2,  1105,    41,  1464,    61,    55,  8697, 10471,    12,     8,\n",
      "          1984,  1209,  4887,     6,     3]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "         -100, -100, -100],\n",
      "        [-100,    0, -100, -100,    0,    0,    0, -100,    0,    0, -100, -100,\n",
      "            0,    0, -100]])}\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(*, tokenizer, train_file='train.txt', dev_file='dev.txt', train_samples=None, dev_samples=None, max_length=100, label2id=None):\n",
    "    ds = load_dataset('text', data_files={'train': train_file, 'dev': dev_file})\n",
    "    if train_samples:\n",
    "        ds['train'] = ds['train'].take(train_samples)\n",
    "    if dev_samples:\n",
    "        ds['dev'] = ds['dev'].take(dev_samples)\n",
    "\n",
    "    def _map(example):\n",
    "        words, word_labels = zip(*tokenize_with_comma_labels(example['text']))\n",
    "        return tokenize_and_align_labels(tokenizer, words=words, word_labels=word_labels, label2id=label2id)\n",
    "\n",
    "    ds_tokenized = ds.map(_map, remove_columns=ds['train'].column_names)\n",
    "\n",
    "    if max_length is not None:\n",
    "        ds_tokenized = ds_tokenized.filter(lambda ex: len(ex['input_ids']) <= max_length)\n",
    "\n",
    "    return ds_tokenized\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('AiLab-IMCS-UL/lvbert')\n",
    "ds = build_dataset(tokenizer=tok, train_samples=2, dev_samples=2, label2id=LABEL2ID)\n",
    "loader = DataLoader(ds['train'], batch_size=2, shuffle=False, collate_fn=DataCollatorForTokenClassification(tok))\n",
    "batch = next(iter(loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e1fe7",
   "metadata": {
    "id": "9f0e1fe7"
   },
   "source": [
    "# Metrics for token classification.\n",
    "*Accuracy* can be misleading for imbalanced tasks:\n",
    "  - In our data, most tokens are \"O\" (no comma).\n",
    "  - A dumb model that always predicts \"O\" could reach very high accuracy (e.g. 95%+) simply by never predicting commas at all.\n",
    "\n",
    "*F1-score* (the harmonic mean of precision and recall) specifically for the COMMA class gives a more honest view of model quality:\n",
    "  - Precision: when the model predicts COMMA, is it right?\n",
    "  - Recall: does the model catch most of the true commas?\n",
    "  - F1: balances both, penalizing if one is much lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2fe37",
   "metadata": {
    "id": "baa2fe37"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_fn(p):\n",
    "    # Model outputs: shape [batch_size, seq_len, num_labels]\n",
    "    # -> pick the most likely label for each token\n",
    "    preds = np.argmax(p.predictions, axis=-1)\n",
    "\n",
    "    # True labels: shape [batch_size, seq_len]\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Flatten but skip positions marked with -100\n",
    "    y_ref = []\n",
    "    y_pred = []\n",
    "    for ref_seq, pred_seq in zip(labels, preds):\n",
    "        for t, p_ in zip(ref_seq, pred_seq):\n",
    "            if t == -100:\n",
    "                continue\n",
    "            y_ref.append(t)\n",
    "            y_pred.append(p_)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_ref, y_pred,\n",
    "        average='binary', pos_label=1, # for binary classification (COMMA vs O)\n",
    "        # average='micro', # for multi-class classification\n",
    "    )\n",
    "    acc = accuracy_score(y_ref, y_pred)\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'p': precision,\n",
    "        'r': recall,\n",
    "        'acc': acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee51e8a",
   "metadata": {
    "id": "5ee51e8a"
   },
   "source": [
    "# Inference\n",
    "Given plain text, we strip commas, tokenize with word boundaries, run the model, and insert commas after tokens labeled COMMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013c61a",
   "metadata": {
    "id": "8013c61a"
   },
   "outputs": [],
   "source": [
    "def process_text(text, model, tokenizer, verbose=True):\n",
    "    #  Preprocess: remove commas, split into words\n",
    "    input_text = remove_commas(text)\n",
    "\n",
    "    # Tokenize with subword alignment\n",
    "    words = tokenize(input_text)\n",
    "    enc = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=False\n",
    "    )\n",
    "    word_ids = enc.word_ids()\n",
    "    # Move to the same device\n",
    "    device = next(model.parameters()).device\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits  # [1, seq_len, label_count]\n",
    "        pred_ids = torch.argmax(logits, dim=-1).squeeze(0).tolist()\n",
    "\n",
    "    # Collapse subwords -> last subword gets the label\n",
    "    word_preds = {}\n",
    "    for i, wid in enumerate(word_ids):\n",
    "        if wid is None:  # skip [CLS], [SEP], etc.\n",
    "            continue\n",
    "        next_wid = word_ids[i + 1] if i + 1 < len(word_ids) else None\n",
    "        if wid != next_wid:  # last subword of the word\n",
    "            word_preds[wid] = pred_ids[i]\n",
    "\n",
    "    # Return word-level predictions\n",
    "    results = [(w, model.config.id2label[word_preds[i]]) for i, w in enumerate(words)]\n",
    "    output_text = ''.join([w + (',' if label == 'COMMA' else '') for w, label in results])\n",
    "\n",
    "    if verbose:\n",
    "        print(f'REF: {text}')\n",
    "        print(f' IN: {input_text}')\n",
    "        print(f'OUT: {output_text}')\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add874a",
   "metadata": {
    "id": "8add874a"
   },
   "source": [
    "# Model fine-tuning\n",
    "- Track loss curves, gradient norms, and evaluation metrics over time\n",
    "- Use an appropriate optimizer and learning rate schedule (e.g., warmup + decay)\n",
    "- Watch for overfitting (gap between train and eval performance)\n",
    "- Adjust batch size, accumulation steps, or precision (fp16/bf16) if needed\n",
    "- Save best checkpoints based on validation metric (e.g., F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5e6f8",
   "metadata": {
    "id": "dcb5e6f8",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    name,\n",
    "    base_model='AiLab-IMCS-UL/lvbert',\n",
    "    max_len=100,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "    lr=5e-6,\n",
    "    bs=32,\n",
    "    train_samples=None,\n",
    "    dev_samples=None,\n",
    "    epochs=3,\n",
    "    report_wandb=True,\n",
    "    wandb_group=None,\n",
    "    save=True,\n",
    "):\n",
    "    if report_wandb and not wandb.api.api_key:\n",
    "        print('Not authenticated with W&B')\n",
    "        report_wandb = False\n",
    "\n",
    "    with wandb.init(project='punctuator', group=wandb_group, name=name) if report_wandb else nullcontext():\n",
    "        print('Train:', locals())\n",
    "        set_seed(seed)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "        ds = build_dataset(tokenizer=tokenizer, train_samples=train_samples, dev_samples=dev_samples, max_length=max_len, label2id=LABEL2ID)\n",
    "\n",
    "        # Initialize base model for token classification task\n",
    "        model = AutoModelForTokenClassification.from_pretrained(base_model, num_labels=len(LABELS), id2label=ID2LABEL, label2id=LABEL2ID)\n",
    "        model.config.use_cache = False\n",
    "\n",
    "        # Define training hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=name,\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            num_train_epochs=epochs,\n",
    "            eval_strategy='epoch',\n",
    "            save_strategy='epoch' if save else 'no',\n",
    "            load_best_model_at_end=save,\n",
    "            metric_for_best_model='f1',\n",
    "            greater_is_better=True,\n",
    "            warmup_ratio=0.05,\n",
    "            gradient_accumulation_steps=1,\n",
    "            fp16=True,\n",
    "\n",
    "            logging_steps=20,\n",
    "            report_to='wandb' if report_wandb else 'none',\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds['train'],\n",
    "            eval_dataset=ds['dev'],\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "            compute_metrics=compute_metrics_fn,\n",
    "        )\n",
    "\n",
    "        # Actual training\n",
    "        trainer.train()\n",
    "        if save:\n",
    "            trainer.save_model(name)\n",
    "            tokenizer.save_pretrained(name)\n",
    "\n",
    "        process_text('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.', trainer.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W6a4_ozqlhu9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ab1aea2753974b7294dae49734382599",
      "9946b65fc7754c8b92fe5f26e9361a2e",
      "f1718237cf4f469f83559466e41b0a75",
      "24938b4428e0407d9f645a645579c67e",
      "485f76ef28114104a014f1acb19ad252",
      "6586e6fc36e54e11a75c3fd44c5371ac",
      "b121250393ed40f2864075a7e18cc068",
      "def705f5d56c48b0966526a52e11fb8c",
      "6073df6fcb254d97a35b57ad1d71121b",
      "29d00024a98f4e51aa29483a62b1f8e9",
      "c8a27b424c104cc488c194457a45fccc",
      "7203d112f441482ca3eee72507255752",
      "bb4b35d3f2e541519698d9db595cd2e4",
      "14b621e0a5ac4cefb5aafed6f6689f2d",
      "b945079903a747609c893ab26c6d2518",
      "4c019f84fab84084b779050758fe3e56",
      "9e08a7b036734db7bc51ace70f02345e",
      "b1ca016dcddc410d9bb8b34281c92e18",
      "47392636aae24cf0b72a2e8cc1ada2da",
      "38310d170cea47b6921a59300afd81eb",
      "26c4d5eef460462ba4f797e7f1a67097",
      "104e8909acbc4fcab2ef04bb67067057",
      "ee3a8d995d444e8aa215647b9a9837d9",
      "8b7112d0d24648d2bf7f1a1a8190afa9",
      "f6cb3d136f3440038ac3abca126c423a",
      "81cf1cd04dd44e91a9fa70edbdeb819e",
      "bef5a9b79653421e89f20beea8a54e69",
      "ae9db7486c4b4d269cfc293daf02c876",
      "9b8efea039954dff8a7c260578696c35",
      "ecdb67df623c49ae9ef1acb93fc0c403",
      "f25d1d3dd0b345388c2f944ff2acde35",
      "411fe63bed78495d97db19bf5705ee45",
      "06b75f0538414b92bcacfea2c5c8e8b8",
      "a91b2d68e8cb43dabc443bcd0b508d5d",
      "aae43aa999704498850b0c4f1489dc56",
      "51eb4fed852f4928b31171e31bb675e3",
      "f74a15975f124e73a5cbf34ec63ffae2",
      "0fdb639c8a6a4ef7a4f42acdab55b15c",
      "a4acee8cd3834e67ae90125ba0836755",
      "d3d81a6337c14a3da6e0ce27e5316d73",
      "60f2d521f0dc4bcc837362fae9f0a15a",
      "f8b7247b7ef041088e75c48242ddb209",
      "ab8ce810142341bbbe32a3cd5b102108",
      "b00e7564773f4fd6919f5222c09d55f2"
     ]
    },
    "id": "W6a4_ozqlhu9",
    "outputId": "8087907f-82a7-4705-860b-915c4e5fa4f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251006_093939-5wiyrd41</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/artursz/punctuator/runs/5wiyrd41' target=\"_blank\">bert_punctuator_sample</a></strong> to <a href='https://wandb.ai/artursz/punctuator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/artursz/punctuator' target=\"_blank\">https://wandb.ai/artursz/punctuator</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/artursz/punctuator/runs/5wiyrd41' target=\"_blank\">https://wandb.ai/artursz/punctuator/runs/5wiyrd41</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'name': 'bert_punctuator_sample', 'base_model': 'AiLab-IMCS-UL/lvbert', 'max_len': 100, 'seed': 42, 'verbose': True, 'lr': 5e-06, 'bs': 32, 'train_samples': 3000, 'dev_samples': None, 'epochs': 3, 'report_wandb': True, 'wandb_group': None, 'save': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1aea2753974b7294dae49734382599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7203d112f441482ca3eee72507255752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3a8d995d444e8aa215647b9a9837d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91b2d68e8cb43dabc443bcd0b508d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at AiLab-IMCS-UL/lvbert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.131554</td>\n",
       "      <td>0.679846</td>\n",
       "      <td>0.890007</td>\n",
       "      <td>0.549978</td>\n",
       "      <td>0.953270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.095823</td>\n",
       "      <td>0.784766</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.695691</td>\n",
       "      <td>0.965574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.089201</td>\n",
       "      <td>0.804215</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.729009</td>\n",
       "      <td>0.967979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.\n",
      " IN: Vēl 9% sacīja ka nav izlēmuši kā balsot bet 3 2% atteicās atbildēt.\n",
      "OUT: Vēl 9% sacīja, ka nav izlēmuši, kā balsot, bet 3, 2% atteicās atbildēt.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/acc</td><td>▁▇█</td></tr><tr><td>eval/f1</td><td>▁▇█</td></tr><tr><td>eval/loss</td><td>█▂▁</td></tr><tr><td>eval/p</td><td>▁█▆</td></tr><tr><td>eval/r</td><td>▁▇█</td></tr><tr><td>eval/runtime</td><td>█▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▆█</td></tr><tr><td>eval/steps_per_second</td><td>▁▆█</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/acc</td><td>0.96798</td></tr><tr><td>eval/f1</td><td>0.80421</td></tr><tr><td>eval/loss</td><td>0.0892</td></tr><tr><td>eval/p</td><td>0.89672</td></tr><tr><td>eval/r</td><td>0.72901</td></tr><tr><td>eval/runtime</td><td>1.3395</td></tr><tr><td>eval/samples_per_second</td><td>1427.427</td></tr><tr><td>eval/steps_per_second</td><td>44.794</td></tr><tr><td>total_flos</td><td>191873474680704.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert_punctuator_sample</strong> at: <a href='https://wandb.ai/artursz/punctuator/runs/5wiyrd41' target=\"_blank\">https://wandb.ai/artursz/punctuator/runs/5wiyrd41</a><br> View project at: <a href='https://wandb.ai/artursz/punctuator' target=\"_blank\">https://wandb.ai/artursz/punctuator</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251006_093939-5wiyrd41/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use only 3000 samples for training to run a quick experiment\n",
    "main('bert_punctuator_sample', train_samples=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad81c5",
   "metadata": {
    "id": "b0ad81c5"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b0ecb",
   "metadata": {
    "id": "cd5b0ecb"
   },
   "outputs": [],
   "source": [
    "m = AutoModelForTokenClassification.from_pretrained('bert_punctuator_sample')\n",
    "t = AutoTokenizer.from_pretrained('bert_punctuator_sample')\n",
    "process_text('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.', m, t)\n",
    "process_text('Nogalināt nedrīkst, apžēlot!', m, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be43210",
   "metadata": {
    "id": "1be43210"
   },
   "source": [
    "# Hyperparameter optimization\n",
    "- Use smaller experiments (1 epoch, limited data) for faster iteration  \n",
    "- Try random or Bayesian search for hyperparameter tuning\n",
    "- Limit training/eval samples when testing setups  \n",
    "- Scale up once the pipeline works end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f158725",
   "metadata": {
    "id": "8f158725",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for lr in [1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3]:\n",
    "    main(f'bert_sweep_lr{lr:.2e}', lr=lr, train_samples=3000, dev_samples=100, epochs=1, wandb_group='bert_sweep', save=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "colab,id,outputId,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
