{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LLMs usage with Ollama**"
      ],
      "metadata": {
        "id": "jGWJXnFx-oaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************\n",
        "Choose: Runtime > CPU\n",
        "\n",
        "Install Ollama (server + CLI)"
      ],
      "metadata": {
        "id": "TYnnunZZ_SN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvqN9yH_X5nz",
        "outputId": "744521a3-7c9f-431e-d46b-f794b1c96113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# \"!\" at the beginning means this command runs in the system shell\n",
        "#   (works in Colab/Jupyter, not regular Python).\n",
        "\n",
        "# Install the Ollama server and CLI (Command Line Interface).\n",
        "# - curl -fsSL:\n",
        "#     -f : fail on HTTP errors\n",
        "#     -s : run silently\n",
        "#     -S : show errors (even when -s is used)\n",
        "#     -L : follow redirects\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the Python client"
      ],
      "metadata": {
        "id": "Uev7bgrT_Yys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Ollama Python client (for using ollama.chat / ollama.generate).\n",
        "# Note: this does NOT start the Ollama server.\n",
        "# https://ollama.com/search\n",
        "!pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_4gZrZk_s87",
        "outputId": "7214dbe2-6e4c-43ff-a778-86d26422144b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.5.4)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the Ollama server"
      ],
      "metadata": {
        "id": "tDn_8UZ__h9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Ollama server in the background:\n",
        "# - nohup : keep it running even if the notebook/terminal stops\n",
        "# - > /tmp/ollama.log 2>&1 : redirect all output (stdout & stderr) to a log file\n",
        "# - & : run in the background so the notebook stays usable\n",
        "!nohup ollama serve > /tmp/ollama.log 2>&1 &\n",
        "\n",
        "# Give the server a couple seconds to start up before using it\n",
        "!sleep 2"
      ],
      "metadata": {
        "id": "pZZtO1jyYV6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the server is running"
      ],
      "metadata": {
        "id": "lfnoMwYR_yks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List processes related to \"ollama\" to check if the server is running.\n",
        "# Note: this will also show the \"grep ollama\" command itself — that's normal.\n",
        "!ps aux | grep ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYpLpoUEsViQ",
        "outputId": "dc3a2f00-1af9-4566-bc58-3e03f2db025e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2983  2.5  0.2 1707884 28096 ?       Sl   09:54   0:00 ollama serve\n",
            "root        3033  0.0  0.0   7376  3496 ?        S    09:54   0:00 /bin/bash -c ps aux | grep ollama\n",
            "root        3035  0.0  0.0   6484  2284 ?        S    09:54   0:00 grep ollama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a model"
      ],
      "metadata": {
        "id": "VWMrWgZx_lCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model from the Ollama registry.\n",
        "# - Requires the Ollama server to be running.\n",
        "# - This pulls the model weights into the local cache; it won’t start the model yet.\n",
        "# - You can later run it with:  ollama run <model>   or   via ollama.chat in Python.\n",
        "# - On Colab, models are saved under: /root/.ollama/models\n",
        "\n",
        "!ollama pull gemma3:4b # !ollama pull llama3.2:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmwS7CbZRKp",
        "outputId": "a324e449-9b9b-42c1-9095-c5913a122fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************\n",
        "*Example: Zero-shot prompting using Ollama from Python (chat API)*"
      ],
      "metadata": {
        "id": "T9Dyl2HWAIRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1: Import Ollama client"
      ],
      "metadata": {
        "id": "tWs3w_wsPmsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python client that talks to the local Ollama server.\n",
        "# By default, it connects to http://127.0.0.1:11434\n",
        "import ollama"
      ],
      "metadata": {
        "id": "khkT_L86Pthe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 2: Define the conversation messages"
      ],
      "metadata": {
        "id": "H9bZxOlQP1PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the \"system\" message:\n",
        "# - This sets the assistant’s role, style, or instructions.\n",
        "system_content = \"You are a friendly lecturer who explains things simply and concretely.\"\n",
        "\n",
        "# Define the \"user\" message:\n",
        "# - This is the actual question or request we want answered.\n",
        "user_content = \"Explain large language models in one sentence.\""
      ],
      "metadata": {
        "id": "BaXYSp-qP6HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 3: Send the chat request"
      ],
      "metadata": {
        "id": "048bCfqDQVI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send a chat request to the Ollama server.\n",
        "# - model=<model> chooses a small, CPU-friendly model.\n",
        "#   (make sure you already pulled it with:  !ollama pull <model>)\n",
        "# - messages is a list of role-tagged turns; order matters (system → user → ...).\n",
        "# - If your server is running on another machine, add host=\"http://<ip>:11434\"\n",
        "\n",
        "llm_model = \"gemma3:4b\" # \"llama3.2:1b\"\n",
        "\n",
        "response = ollama.chat(\n",
        "    model=llm_model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "t7e0PjI9QZVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 4: Display the LLM’s reply"
      ],
      "metadata": {
        "id": "KWJU-puKQb4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The response comes back as a Python dictionary.\n",
        "# The generated text is stored under: response['message']['content']\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwZ61_VKQhes",
        "outputId": "6e8ee34d-bc6a-4c11-a18d-34d989b47ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's tackle that! \n",
            "\n",
            "Essentially, a large language model is like a really, *really* good student who's read a massive library of text and learned to predict what words should come next – that's how it generates its responses! \n",
            "\n",
            "Does that make sense as a starting point? We can definitely break it down further if you’d like!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************\n",
        "*Example: Few-shot prompting using Ollama from Python (chat API)*"
      ],
      "metadata": {
        "id": "QZKN40OW0z8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1: Import Ollama client"
      ],
      "metadata": {
        "id": "5GC-Rf3a1TXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python client that talks to the local Ollama server.\n",
        "# By default, it connects to http://127.0.0.1:11434\n",
        "import ollama"
      ],
      "metadata": {
        "id": "x-M5EwH31VRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 2: Define the system prompt"
      ],
      "metadata": {
        "id": "S3y23e4B1YcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The \"system\" message sets the assistant’s overall role and behavior.\n",
        "system_content = \"You are a friendly lecturer who explains things simply and concretely.\""
      ],
      "metadata": {
        "id": "V5mgf4kx1h0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 3: Provide few-shot examples"
      ],
      "metadata": {
        "id": "FdusydiT12_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few-shot prompting = showing the model a few example Q&A pairs\n",
        "# before asking the real question.\n",
        "# This helps guide the style and depth of the answer.\n",
        "few_shots = [\n",
        "    {\"role\": \"user\", \"content\": \"What is overfitting?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Overfitting is when a model memorizes the training data so well that it fails to generalize to new data.\"},\n",
        "\n",
        "    {\"role\": \"user\", \"content\": \"Explain gradient descent in one sentence.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Gradient descent repeatedly nudges model parameters in the direction that most reduces error, based on the current slope of the loss.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "OQRed29n1759"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 4: Add the actual prompt"
      ],
      "metadata": {
        "id": "ly5Te6151-Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The real user question (after the examples).\n",
        "user_content = \"Explain large language models in one sentence.\""
      ],
      "metadata": {
        "id": "x8rlwZWg2FqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 5: Send the chat request"
      ],
      "metadata": {
        "id": "jBvVUiS02HbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send a chat request to the Ollama server.\n",
        "# - model=<model> selects a small, CPU-friendly model.\n",
        "#   (make sure you’ve pulled it first with: !ollama pull <model>)\n",
        "# - messages include: system prompt → few-shot examples → user question\n",
        "\n",
        "response = ollama.chat(\n",
        "    model=llm_model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        *few_shots,\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "k5LMpMsT2LId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 6: Show the LLM's reply"
      ],
      "metadata": {
        "id": "FVJnRMHr2Nld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The response is a dictionary.\n",
        "# The generated text is stored inside response[\"message\"][\"content\"]\n",
        "print(response[\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtccU41G2Tj2",
        "outputId": "83b7e17e-bae1-4a59-a738-1efcfb63d8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large language models are essentially super-smart computer programs trained on massive amounts of text to predict the next word in a sequence, allowing them to generate human-like text. \n",
            "\n",
            "Does that make sense, or would you like me to break it down a little further?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************\n",
        "*Example: Minimal RAG using Ollama from Python (chat API)*"
      ],
      "metadata": {
        "id": "aRdv1OqZ7HQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1: Pull the embedding model (once)"
      ],
      "metadata": {
        "id": "zzIVH2sw7Xuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the embedding model used for vector search.\n",
        "# (Also make sure you've pulled your generator model, e.g., !ollama pull <model>)\n",
        "\n",
        "!ollama pull nomic-embed-text # embedding model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd5vdx697f4i",
        "outputId": "43ab5f70-a52b-4cd4-d81c-fa4602c3f8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 2: Imports + config"
      ],
      "metadata": {
        "id": "infMRbw_7nk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Talk to the local Ollama server and use NumPy for vector math\n",
        "import ollama, numpy as np\n",
        "\n",
        "# Generator (LLM) and embedding model\n",
        "GEN_MODEL = llm_model          # small, CPU-friendly generator\n",
        "EMB_MODEL = \"nomic-embed-text\"     # embeddings for retrieval\n",
        "\n",
        "# System prompt controls tone/behavior of the assistant\n",
        "system_content = \"You are a friendly lecturer who explains things simply and accurately.\""
      ],
      "metadata": {
        "id": "9yL5ZZEw7wQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 3: Tiny knowledge base (your source texts)"
      ],
      "metadata": {
        "id": "OFHXSash7yYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents you want the model to ground its answers on.\n",
        "# Replace these with chunks from your notes, PDFs, web pages, etc.\n",
        "docs = [\n",
        "    {\"id\": \"note1\", \"text\": \"A large language model (LLM) is a neural network trained on vast text to predict the next token and perform language tasks.\"},\n",
        "    {\"id\": \"note2\", \"text\": \"Tokenization splits text into subword units; LLMs learn statistical patterns over these tokens.\"},\n",
        "    {\"id\": \"note3\", \"text\": \"During inference, the model autoregressively generates tokens conditioned on the prompt and previous outputs.\"},\n",
        "    {\"id\": \"note4\", \"text\": \"Pretraining uses self-supervised objectives (like next-token prediction) on large corpora; fine-tuning adapts to specific tasks.\"},\n",
        "]"
      ],
      "metadata": {
        "id": "Ike_zI2s8AWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 4: Embedding + retrieval helpers"
      ],
      "metadata": {
        "id": "xOrlVnWe8J16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(texts):\n",
        "    \"\"\"\n",
        "    Convert a list of strings into embedding vectors using Ollama.\n",
        "    Returns: numpy array of shape (n_texts, embedding_dim).\n",
        "    \"\"\"\n",
        "    vecs = []\n",
        "    for t in texts:\n",
        "        # Ask Ollama to produce an embedding vector for this text\n",
        "        e = ollama.embeddings(model=EMB_MODEL, prompt=t)[\"embedding\"]\n",
        "        vecs.append(e)\n",
        "    # Stack all embeddings into a single NumPy array\n",
        "    return np.array(vecs, dtype=np.float32)\n",
        "\n",
        "\n",
        "def cosine_sim_matrix(A, b):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between:\n",
        "      - A = 2D array of many embeddings (shape: [n_docs, dim])\n",
        "      - b = 1D array, a single query embedding (shape: [dim])\n",
        "    Returns: similarity scores (length = n_docs)\n",
        "    \"\"\"\n",
        "    # Normalize each vector in A to unit length\n",
        "    A_norm = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
        "    # Normalize query vector\n",
        "    b_norm = b / (np.linalg.norm(b) + 1e-12)\n",
        "    # Cosine similarity = dot product of normalized vectors\n",
        "    return A_norm @ b_norm\n",
        "\n",
        "\n",
        "# --- Indexing step (done once at startup) ---\n",
        "# Extract raw text from docs\n",
        "doc_texts = [d[\"text\"] for d in docs]\n",
        "# Precompute embeddings for all docs (so we don’t recompute each query)\n",
        "doc_vecs = embed(doc_texts)\n",
        "\n",
        "\n",
        "def retrieve(query, k=3):\n",
        "    \"\"\"\n",
        "    Given a query string:\n",
        "      1. Embed the query\n",
        "      2. Compute cosine similarity against all document embeddings\n",
        "      3. Return the top-k most similar docs (with IDs, text, and scores)\n",
        "    \"\"\"\n",
        "    # Step 1: Embed the query (returns [1, dim], so take the first row)\n",
        "    q_vec = embed([query])[0]\n",
        "    # Step 2: Compare query embedding to all doc embeddings\n",
        "    sims = cosine_sim_matrix(doc_vecs, q_vec)\n",
        "    # Step 3: Pick indices of top-k most similar docs (highest scores)\n",
        "    idxs = np.argsort(-sims)[:k]\n",
        "    # Step 4: Return matching docs with similarity scores\n",
        "    return [\n",
        "        {\"id\": docs[i][\"id\"], \"text\": docs[i][\"text\"], \"score\": float(sims[i])}\n",
        "        for i in idxs\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "zxcS45I78QW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 5: Build a context block the LLM will read"
      ],
      "metadata": {
        "id": "L-cff0WK8U-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_context_block(retrieved):\n",
        "    \"\"\"\n",
        "    Take a list of retrieved documents (with id, text, score)\n",
        "    and format them into a readable block of text that we can\n",
        "    pass into the model as context.\n",
        "    \"\"\"\n",
        "    lines = []  # will hold formatted strings for each retrieved doc\n",
        "\n",
        "    # Loop over retrieved docs, numbering them starting at 1\n",
        "    for i, r in enumerate(retrieved, 1):\n",
        "        # Each entry shows:\n",
        "        # - the document number in this batch (Doc 1, Doc 2, …)\n",
        "        # - the doc ID (from our original docs list)\n",
        "        # - the similarity score (3 decimals)\n",
        "        # - the actual text content\n",
        "        lines.append(f\"[Doc {i} | {r['id']} | score={r['score']:.3f}]\\n{r['text']}\")\n",
        "\n",
        "    # Join all docs together into one string, separated by blank lines\n",
        "    return \"\\n\\n\".join(lines)"
      ],
      "metadata": {
        "id": "iJzcFjwx8Zjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 6: Ask a question -> retrieve -> generate (RAG)"
      ],
      "metadata": {
        "id": "IEsThKuU8cFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your question\n",
        "user_content = \"Explain large language models in one sentence.\"\n",
        "\n",
        "# Retrieve supporting snippets\n",
        "retrieved = retrieve(user_content, k=2)\n",
        "context_block = build_context_block(retrieved)\n",
        "\n",
        "# Instruct the model to rely on the retrieved context\n",
        "rag_instructions = (\n",
        "    \"Use ONLY the context to answer if possible. \"\n",
        "    \"If the answer isn't in the context, say so briefly. \"\n",
        "    \"Cite the doc numbers you used like [Doc 1]. Keep it concise.\"\n",
        ")\n",
        "\n",
        "# Compose messages (system + user with context + question)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_content},\n",
        "    {\"role\": \"user\", \"content\": f\"{rag_instructions}\\n\\n=== Context ===\\n{context_block}\\n\\n=== Question ===\\n{user_content}\"},\n",
        "]\n",
        "\n",
        "# Generate the grounded answer\n",
        "response = ollama.chat(model=GEN_MODEL, messages=messages)\n",
        "answer = response[\"message\"][\"content\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbOLZSSE8ezq",
        "outputId": "c92dbaaf-7530-4ccf-ae5f-694b9796f787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large language models are neural networks trained to predict the next token in a sequence of text [Doc 1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 7: (Optional) Inspect what was retrieved"
      ],
      "metadata": {
        "id": "qWBd11FN8sKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See which documents were fed to the model and how similar they were.\n",
        "for r in retrieved:\n",
        "    print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HFwMMbv8t5G",
        "outputId": "9bf30072-f268-4932-d946-8027a6f1702b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'note1', 'text': 'A large language model (LLM) is a neural network trained on vast text to predict the next token and perform language tasks.', 'score': 0.7794513702392578}\n",
            "{'id': 'note2', 'text': 'Tokenization splits text into subword units; LLMs learn statistical patterns over these tokens.', 'score': 0.6357552409172058}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******************************************************************\n",
        "Stop any running Ollama server processes"
      ],
      "metadata": {
        "id": "VD9_sBAZ-YWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop any running Ollama server processes (if they exist).\n",
        "# - pkill -f \"ollama serve\" : looks for processes matching the string \"ollama serve\" and kills them\n",
        "# - || true : ensures this command never errors out\n",
        "#              (so if no server is running, the cell still succeeds quietly)\n",
        "!pkill -f \"ollama serve\" || true\n",
        "\n",
        "# Wait a moment to let the process fully shut down\n",
        "!sleep 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU7VEtdBr9MD",
        "outputId": "39d5c357-62d8-4331-a783-32dd2c119c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Ollama server is running"
      ],
      "metadata": {
        "id": "l27pOQsYoT5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all running processes that mention \"ollama\" in their command line.\n",
        "# - ps aux : show details of all processes\n",
        "# - grep ollama : filter the list to only lines containing \"ollama\"\n",
        "# Note: this will also show the \"grep ollama\" command itself — that's normal.\n",
        "!ps aux | grep ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrlZZOCDv5ly",
        "outputId": "df86e042-0532-4c0e-b299-0b6656ffdf05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        3777  0.0  0.0   7376  3572 ?        S    09:57   0:00 /bin/bash -c ps aux | grep ollama\n",
            "root        3779  0.0  0.0   6484  2260 ?        S    09:57   0:00 grep ollama\n"
          ]
        }
      ]
    }
  ]
}