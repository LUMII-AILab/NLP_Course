{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01470473",
   "metadata": {
    "id": "01470473"
   },
   "source": [
    "# Comma Restoration with Encoder-Decoder Models (Seq2Seq)\n",
    "This notebook fine-tunes sequence-to-sequence transformers to restore commas by generating the punctuated sentence from an unpunctuated input. During training, commas are removed from the source text; the target is the original, correctly punctuated sentence. At inference, the model takes clean, comma-stripped text and outputs the version with commas inserted end-to-end—no token-level labels needed.\n",
    "\n",
    "Models to try:\n",
    "- https://huggingface.co/google/mt5-small\n",
    "- https://huggingface.co/google/byt5-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dc197",
   "metadata": {
    "id": "4c8dc197"
   },
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sD7SnreC22k9",
   "metadata": {
    "id": "sD7SnreC22k9"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Weights & Biases to enable logging and experiment tracking.\n",
    "# Comment out the following lines if you don't want to use W&B.\n",
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddf0fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2ddf0fa",
    "outputId": "0f7bb9a9-cbf1-4664-ad48-919823bf0ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "CUDA device: Tesla T4 (7, 5) bf16 False\n",
      "Memory: 14992.12 MB free / 15095.06 MB total\n"
     ]
    }
   ],
   "source": [
    "# Check if a CUDA device is available\n",
    "!pip install torch\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0), torch.cuda.get_device_capability(0), 'bf16', torch.cuda.is_bf16_supported(False))\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    print(f'Memory: {free_mem / 1024 ** 2:.2f} MB free / {total_mem / 1024 ** 2:.2f} MB total')\n",
    "else:\n",
    "    print('No CUDA device available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6fe0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34b6fe0c",
    "outputId": "d159b890-5e24-4c8e-c6ad-65cdd9640c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.11\n",
      "pip 24.1.2 from /usr/local/lib/python3.12/dist-packages/pip (python 3.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.8.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.9)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!python -V\n",
    "!pip -V\n",
    "!pip install numpy transformers[torch] scikit-learn datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651c6be",
   "metadata": {
    "id": "2651c6be"
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from contextlib import nullcontext\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed, PreTrainedTokenizerBase, Seq2SeqTrainer,\n",
    ")\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5e365",
   "metadata": {
    "id": "f0f5e365",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Prepare dataset\n",
    "Raw sentences from the Latvian Universal Dependencies (LVTB) corpus: https://universaldependencies.org/treebanks/lv_lvtb/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795b03c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2795b03c",
    "lines_to_next_cell": 1,
    "outputId": "b67a8c66-50d7-4637-dc00-414df2232a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lengths before filtering: DEV 2080 TRAIN 15055\n",
      "Dataset sentence lengths: DEV 1912 TRAIN 13811\n",
      "To tu man stāstīji jau pirms divām nedēļām.\n",
      "Ka pieticis tikai autobusa biļetei un barankām.\n",
      "Uz skatuves kāpa skolas koris, pēc tam uzstājās arī dramatiskā pulciņa dalībnieki un divi bērnudārza audzēkņi.\n",
      "Burka esot jāizdekorē ar dillēm, mārrutku lapu, upeņu zariņu un ķiploka pusdaiviņām, jāsaliek gurķīši un jāaplej ar verdošu ūdeni, kurā iebērta ēdamkarote cukura un ēdamkarote sāls.\n",
      "Izaugs sava raža, nevajadzēs lieku reizi braukt uz tirgu.\n"
     ]
    }
   ],
   "source": [
    "def fetch_ud_texts(split, seed=42):\n",
    "    conllu = requests.get(f'https://raw.githubusercontent.com/UniversalDependencies/UD_Latvian-LVTB/r2.16/lv_lvtb-ud-{split}.conllu').text\n",
    "    texts = [line[9:].strip() for line in conllu.splitlines() if line.startswith('# text = ')]\n",
    "    if seed:\n",
    "        import random\n",
    "        random.Random(seed).shuffle(texts)\n",
    "    return texts\n",
    "\n",
    "def prepare_data(max_chars=200, dev_txt='dev.txt', train_txt='train.txt'):\n",
    "    # Download UD Latvian splits, filter by mBERT token count, and save plain .txt files.\n",
    "    dev_texts = fetch_ud_texts('dev')\n",
    "    train_texts = fetch_ud_texts('train')\n",
    "\n",
    "    if max_chars:\n",
    "        # Filter out long sentences to avoid truncation\n",
    "        print('Sentence lengths before filtering:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
    "        dev_texts = [t for t in dev_texts if len(t) <= max_chars]\n",
    "        train_texts = [t for t in train_texts if len(t) <= max_chars]\n",
    "    print('Dataset sentence lengths:', 'DEV', len(dev_texts), 'TRAIN', len(train_texts))\n",
    "\n",
    "    with open(dev_txt, 'w') as f:\n",
    "        for t in dev_texts: f.write(t + '\\n')\n",
    "    with open(train_txt, 'w') as f:\n",
    "        for t in train_texts: f.write(t + '\\n')\n",
    "\n",
    "    return dev_texts, train_texts\n",
    "\n",
    "dev_texts, train_texts = prepare_data()\n",
    "print(*train_texts[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19d374",
   "metadata": {
    "id": "4b19d374"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4c142",
   "metadata": {
    "id": "60e4c142",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_commas(s) -> str:\n",
    "    return re.sub(r'\\s*,+\\s*', ' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a9889",
   "metadata": {
    "id": "464a9889",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a5a68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660,
     "referenced_widgets": [
      "69c3f3cbb6d248ffb357919c75c623b4",
      "22b1d1d441fb4a5da38105a595866710",
      "cb9520df81834a98a0717f41aceb6923",
      "0866aa03e9ab4e5f8457c119120f5e9f",
      "c1a7b5a4afe54fae8f5eddb81d9cbaac",
      "1fa7ab5dc78f4d0a90a455d4b31c9c6a",
      "50a285d2d2c14073a9bf12f8e9e21b98",
      "7fb0d5e31cc94f14a34a0c8ed1a58be5",
      "996f19711057435cb2f7791684cf70f0",
      "8cec3f42cceb4f8a8038a226f474cc68",
      "b1ab6087029949d5b93e3eadfa898587",
      "5b15f85ff950428f9e9ee141c26985e9",
      "49d3314c3f0b4a7dbdc705b38c15f723",
      "147ae37d55d34f9ab3c306127db079f8",
      "83cd309e1d604eb8a22c64fd78fbb9fa",
      "cf3f9210ca1d42c3a61743ebd1995e3f",
      "06c9ac2969b642f19e4ccb163364a093",
      "60946046add141a2ad5b6b5c8a530e40",
      "a34e1adeffca4aa5a98113b66471e23d",
      "9241473652b34333abef89023193266c",
      "53b2b902dbd04500bc21089bfe7dbbd1",
      "ea214e0a25ca4a9582984fe781bed1cb",
      "a564b8bbbfa241e1bf459290ea67a1c2",
      "c7b699311bc14fea9115da2bed77a568",
      "2b3e85878b634b1f8a7fdbec69156fcd",
      "f505718841184749a06fdcfc41987a4b",
      "0bc0b45a39c249cd9f4955d50af9f8c9",
      "05c47154d39e4629abc3f5d6399ab391",
      "cb702956417b4db49a814945e3f99433",
      "685ed8f70a9f4238a1b2d7ce477bc582",
      "d7d605aad6264006af7e8a6d60f1162b",
      "d8508065afc740748da6e3222326a465",
      "96130ba4694343b4a6afbab09e7f1d38",
      "905013ec7aae46f295a8ebb13a72074c",
      "cdb30e4cb6c143a482f65adc9fd10086",
      "574a2ecca641466cb308fc6bd3b5dcf7",
      "33cb626ca0a24dffbc389030b486699d",
      "8e6d62de929944178054f534c532b469",
      "7f76e18a331445f4baa54785b14d10e7",
      "db6e58586b634058abbbeaa1a14185a9",
      "a428fbede4b64a43ad97b9e5645c95ad",
      "3a28a109941a4d75ac21cbad5c289ad6",
      "5a33a4b23d2f46fa82951e417e126326",
      "e0c1237d0cfe4b1fb86e0b1624ca251d",
      "cbdeb54fe5734e4ba2d34e841e3ffebb",
      "7c63438ea2dd4c4c95fbfade708705f1",
      "b131acfaf47340df97677017c47ee5e4",
      "3fae2a41a9734ab493994a7d13374440",
      "73177d6be8134d97a9a30fac212f8535",
      "8f5d48372d96491d856657e46dc19529",
      "4d0c8a7b0a984b4a9c1b5d7bc6579e79",
      "5893792a778c4e0fae360b9ab23efd9c",
      "b0443b99a60d409791fc15b3c9238c7c",
      "8673eb7cfba24d7d838594ffcf176884",
      "237e574f9b7a4b7d96a5be4d6e7a2082",
      "b3e09e5f917e4247912ab78ed9e879a0",
      "95548a6e938a450ca08de7eab6c531ae",
      "45bbe079a5e540fab348afb11cac6e44",
      "15f5726b5ba7408eb6403081475e4de2",
      "92e5f39842c9427988299bb1165832d9",
      "9ee4c4584c1e4fffa65d6381abc80675",
      "56c2d3964733443ca594c7d6ec7f15e8",
      "cc9808dd89bb4433997dfdd00f09625d",
      "6e398ed4d1b84de89e2a1533c8c6d25e",
      "cf258ea2a8b34bd8bb9851c6c0c558d7",
      "151bbfc86603411f92d4fbc6dc116289",
      "4daba9a102fb45cba17ccc21adf949f4",
      "7f2682826bcd47999f6ee4819f13f957",
      "043c6cdc7fc549a58a9865f59a9e08af",
      "d53cd63bd66b44a5a88d1519de598455",
      "2788430cde254ae995464b9d41d7930f",
      "13021d7836864fc0b91217878c4eb0da",
      "82108210cd9e43bdb5a1382a8bc97168",
      "53534b9da7854a0e9f8ae752f8cad824",
      "6175e267628748528e6144e9e5607365",
      "87ff27a6c8d54c6faa5f6a74cc561e63",
      "d0c04c105e4f45e9a4e144b29e35db6c"
     ]
    },
    "id": "bc9a5a68",
    "outputId": "db2c262d-01cc-4aac-9024-6cf52c62e942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer google/mt5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c3f3cbb6d248ffb357919c75c623b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b15f85ff950428f9e9ee141c26985e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a564b8bbbfa241e1bf459290ea67a1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905013ec7aae46f295a8ebb13a72074c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sample: {'input_ids': [434, 35079, 259, 11859, 29953, 44897, 261, 427, 3546, 1184, 50856, 42786, 2849, 46813, 1460, 261, 2045, 381, 106373, 344, 135878, 5861, 58648, 11537, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Encoded sample - subword units: ['▁V', 'ēl', '▁', '9%', '▁sac', 'īja', ',', '▁ka', '▁nav', '▁iz', 'lēm', 'uši', '▁kā', '▁bals', 'ot', ',', '▁bet', '▁3', ',2%', '▁at', 'teic', 'ās', '▁atbild', 'ēt', '.', '</s>']\n",
      "Max 80, min 2, avg 29.943740496705523\n",
      "95% length: 58\n",
      "99% length: 66\n",
      "99.9% length: 73\n",
      "Tokenizer google/byt5-small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdeb54fe5734e4ba2d34e841e3ffebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e09e5f917e4247912ab78ed9e879a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daba9a102fb45cba17ccc21adf949f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sample: {'input_ids': [89, 199, 150, 111, 35, 60, 40, 35, 118, 100, 102, 199, 174, 109, 100, 47, 35, 110, 100, 35, 113, 100, 121, 35, 108, 125, 111, 199, 150, 112, 120, 200, 164, 108, 35, 110, 199, 132, 35, 101, 100, 111, 118, 114, 119, 47, 35, 101, 104, 119, 35, 54, 47, 53, 40, 35, 100, 119, 119, 104, 108, 102, 199, 132, 118, 35, 100, 119, 101, 108, 111, 103, 199, 150, 119, 49, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Encoded sample - subword units: ['V', 'Ä', '\\x93', 'l', ' ', '9', '%', ' ', 's', 'a', 'c', 'Ä', '«', 'j', 'a', ',', ' ', 'k', 'a', ' ', 'n', 'a', 'v', ' ', 'i', 'z', 'l', 'Ä', '\\x93', 'm', 'u', 'Å', '¡', 'i', ' ', 'k', 'Ä', '\\x81', ' ', 'b', 'a', 'l', 's', 'o', 't', ',', ' ', 'b', 'e', 't', ' ', '3', ',', '2', '%', ' ', 'a', 't', 't', 'e', 'i', 'c', 'Ä', '\\x81', 's', ' ', 'a', 't', 'b', 'i', 'l', 'd', 'Ä', '\\x93', 't', '.', '</s>']\n",
      "Max 228, min 3, avg 95.03366881471291\n",
      "95% length: 192\n",
      "99% length: 214\n",
      "99.9% length: 222\n"
     ]
    }
   ],
   "source": [
    "def test_tokenization(model=None):\n",
    "    s = 'Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.'\n",
    "    if model:\n",
    "        print('Tokenizer', model)\n",
    "        t = AutoTokenizer.from_pretrained(model)\n",
    "        print('Encoded sample:', t(s))\n",
    "        print('Encoded sample - subword units:', t.convert_ids_to_tokens(t.encode(s)))\n",
    "        lengths = sorted([len(t.encode(seq)) for seq in train_texts])\n",
    "        print(f'Max {max(lengths)}, min {min(lengths)}, avg {sum(lengths)/len(lengths)}')\n",
    "        print(f'95% length: {lengths[int(len(lengths) * 0.95)]}')\n",
    "        print(f'99% length: {lengths[int(len(lengths) * 0.99)]}')\n",
    "        print(f'99.9% length: {lengths[int(len(lengths) * 0.999)]}')\n",
    "\n",
    "test_tokenization('google/mt5-small')\n",
    "test_tokenization('google/byt5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716d714",
   "metadata": {
    "id": "c716d714",
    "lines_to_next_cell": 1
   },
   "source": [
    "Tokenize and format dataset for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40227d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755,
     "referenced_widgets": [
      "d82c967731524ebab7ad1a6fcc7c2819",
      "7d29d52d00cf4dcd974c81401de5994c",
      "ca05bad67ade4169bf78da871be1d188",
      "78cfac41e783418cb9029c70b9ab8f5a",
      "6217056944e240c4ad0b4ec9965755a0",
      "1d4acabb3eff456c9dc1d97e831cfdbd",
      "9053e5e5a492412a90c827e9062dd92b",
      "81c65021998d406692cfde6414a60e88",
      "a44213e8d5c4481c9beef84ff620d41f",
      "1cdb319a0c4142e6abc6899c20449233",
      "4d862e8eb4be4c22b6b5a1791c600574",
      "5d58fe4c0ec74d7c853f590b5e05dab8",
      "08205d9ea783435ea1391d72814b187a",
      "692f9dc0fb9c4597bf2924f973869b1c",
      "7ddcc2d470084d10b9674c634deaea7c",
      "d0b620503caf49aa916b3a445e8f5c39",
      "0fdda2c99ce648588693b51a557c6796",
      "cbc8284c41684d588eaf59b41cf3d8af",
      "c07a749ad85841149c5955883a4b95b7",
      "210090d642ac4714b2fe0b940ee90318",
      "67a6e214396f4daaa7287edbdf29f69f",
      "6775e8c71ef94690add28dd9f5160be6",
      "26a3b0ab5edb4507b3e96b2f15ccddb4",
      "2b57ae0e737648d18126161aea8c2653",
      "5f774661532a460ab81cc67ac6bd01d7",
      "5efb9936b29f4436ad8a88db9ae74dcd",
      "b523759d6f9245d2be80775f5b0612a7",
      "3de68d55137b4dcf83cfab14ab7f18fb",
      "3e7dc46f7bfe432db628c275005617ee",
      "5789db5ffc2f4741ba6ee7e74b4d09ab",
      "1caa7ef42d954047b37b044e820fc318",
      "a0d9cede6964488ea4a756c488233e68",
      "cedabfec8ba94c50b64835e1952527c2",
      "2c49f3983c0d4e68a5e78d11c6961106",
      "39787ea799d94bbdb26bc629e987b9ec",
      "238f85370c7d41479175c7f9d0e0d664",
      "5a88c8ba5f8341dc93b166738411af68",
      "eceebcb079764bceac0fd607c98eacbd",
      "cb104ffc2ff545be8b7fc426d7cc7347",
      "a2be6c6436424870a1af3bdbad3bc2c9",
      "cd1e1f143d9b4188bdbceade12c3a071",
      "b93a6c52e54d4a78bd0065f160bd1ea8",
      "46da12096d414e478fdb0026ab0cf015",
      "31c087458b644f1a880a556255849948"
     ]
    },
    "id": "c40227d8",
    "outputId": "600ca810-dda8-4e1e-eed4-7ed80e55a9af"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82c967731524ebab7ad1a6fcc7c2819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d58fe4c0ec74d7c853f590b5e05dab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a3b0ab5edb4507b3e96b2f15ccddb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c49f3983c0d4e68a5e78d11c6961106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   926,    719,    674,    259,  61180, 171881,   6168,   7602,    263,\n",
      "           6562,   4769,    448,  16521,  28981,    282,    260,      1,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [  1362,   2533,  82736,    263,    259,  13212,  31541,    262,    837,\n",
      "          10999, 124749,    335,    259,  34422, 159443,    260,      1,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [ 16389,    259, 119665,   4570,    259,  53065,    262,    259,  74755,\n",
      "         116533,    263,    259,   7659,   2335,   1678, 142198,   5861,    259,\n",
      "           4815, 102020,  51670,    259, 146473,  17825,    350,  64041,  49955,\n",
      "            335,  94591,  20914,    273,  86999,    745,    259, 104661,    314,\n",
      "          45685,    260,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   926,    719,    674,    259,  61180, 171881,   6168,   7602,    263,\n",
      "           6562,   4769,    448,  16521,  28981,    282,    260,      1,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100],\n",
      "        [  1362,   2533,  82736,    263,    259,  13212,  31541,    262,    837,\n",
      "          10999, 124749,    335,    259,  34422, 159443,    260,      1,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100],\n",
      "        [ 16389,    259, 119665,   4570,    259,  53065,    262,    259,  74755,\n",
      "         116533,    263,    261,    259,   7659,   2335,   1678, 142198,   5861,\n",
      "            259,   4815, 102020,  51670,    259, 146473,  17825,    350,  64041,\n",
      "          49955,    335,  94591,  20914,    273,  86999,    745,    259, 104661,\n",
      "            314,  45685,    260,      1]])}\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(*, tokenizer, train_file='train.txt', dev_file='dev.txt', train_samples=None, dev_samples=None, max_length=100):\n",
    "    ds = load_dataset('text', data_files={'train': train_file, 'dev': dev_file})\n",
    "    if train_samples:\n",
    "        ds['train'] = ds['train'].take(train_samples)\n",
    "    if dev_samples:\n",
    "        ds['dev'] = ds['dev'].take(dev_samples)\n",
    "\n",
    "    def _encode_examples(batch):\n",
    "        targets = batch['text']\n",
    "        sources = [remove_commas(t) for t in targets]\n",
    "        enc_in = tokenizer(sources, max_length=max_length, truncation=True)\n",
    "        enc_out = tokenizer(text_target=targets, max_length=max_length, truncation=True)\n",
    "        enc_in['labels'] = enc_out['input_ids']\n",
    "        return enc_in\n",
    "\n",
    "    ds_encoded = ds.map(_encode_examples, batched=True, remove_columns=ds['train'].column_names)\n",
    "\n",
    "    return ds_encoded\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "ds = build_dataset(tokenizer=tok, train_samples=10, dev_samples=10)\n",
    "collator = DataCollatorForSeq2Seq(tok)\n",
    "loader = DataLoader(ds['train'], batch_size=3, shuffle=False, collate_fn=collator)\n",
    "batch = next(iter(loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cc7d7",
   "metadata": {
    "id": "bb8cc7d7",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Metrics for token classification.\n",
    "*F1-score* (the harmonic mean of precision and recall) specifically for the COMMA class gives a more honest view of model quality:\n",
    "  - Precision: when the model predicts COMMA, is it right?\n",
    "  - Recall: does the model catch most of the true commas?\n",
    "  - F1: balances both, penalizing if one is much lower.\n",
    "\n",
    "*Changes* - the percentage of sentences where the model introduced modifications that were **not desired**.  \n",
    "This highlights over-correction: even if the model achieves good precision/recall on commas, a high *Changes* value means it is altering sentences unnecessarily, reducing usability in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143feb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6143feb4",
    "lines_to_next_cell": 2,
    "outputId": "73078f7a-25f3-40c4-a4df-1d7d027ef85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.75, 'p': 0.75, 'r': 0.75, 'changes': 0.0, 'exact': 0.0, 'tp': 3, 'fp': 1, 'fn': 1}\n",
      "{'f1': 0.6666666666666666, 'p': 1.0, 'r': 0.5, 'changes': 1.0, 'exact': 0.0, 'tp': 2, 'fp': 0, 'fn': 2}\n"
     ]
    }
   ],
   "source": [
    "def align_and_count_commas(hyp: str, ref: str) -> tuple[int, int, int]:\n",
    "    sm = difflib.SequenceMatcher(a=hyp, b=ref, autojunk=False)\n",
    "    tp = fp = fn = 0\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'equal':\n",
    "            tp += hyp[i1:i2].count(',')\n",
    "        else:\n",
    "            fp += hyp[i1:i2].count(',')\n",
    "            fn += ref[j1:j2].count(',')\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def eval_commas(refs: list[str], preds: list[str], verbose=False) -> dict[str, float]:\n",
    "    verbose_changes_limit = 5\n",
    "    tp = fp = fn = changes = exact = 0\n",
    "    for hyp, ref in zip(preds, refs):\n",
    "        tpp, fpp, fnn = align_and_count_commas(hyp, ref)\n",
    "        tp += tpp; fp += fpp; fn += fnn\n",
    "        if hyp == ref:\n",
    "            exact += 1\n",
    "        is_changed = re.sub(r'[\\s,]', '', hyp) != re.sub(r'[\\s,]', '', ref)\n",
    "        if is_changed:\n",
    "            changes += 1\n",
    "\n",
    "        if verbose and verbose_changes_limit > 0 and is_changed:\n",
    "            print('--- Changed')\n",
    "            print('REF:', ref)\n",
    "            print('OUT:', hyp)\n",
    "            verbose_changes_limit -= 1\n",
    "\n",
    "    p = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    r = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
    "    return {\n",
    "        'f1': f1, 'p': p, 'r': r,\n",
    "        'changes': (changes / len(preds) if preds else 0.0),\n",
    "        'exact':  (exact / len(preds) if preds else 0.0),\n",
    "        'tp': tp, 'fp': fp, 'fn': fn,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer, verbose=False):\n",
    "    preds, labels = eval_preds\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Replace ignore index in preds\n",
    "    labels = np.where(labels != -100, labels, pad_id)\n",
    "    preds = np.where(preds != -100, preds, pad_id)\n",
    "\n",
    "    decoded_preds  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    return eval_commas(decoded_labels, decoded_preds, verbose=verbose)\n",
    "\n",
    "print(eval_commas(\n",
    "    ['Labi atpūšamies, draugi mīļie, un lai veiksmīga, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona!'],\n",
    "    ['Labi atpūšamies, draugi, mīļie un lai veiksmīga, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona!'],\n",
    "))\n",
    "print(eval_commas(\n",
    "    ['Labi atpūšamies, draugi mīļie, un lai veiksmīga, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona!'],\n",
    "    ['Labi atpūšamies draugi mīļie un lai veiksmīgas, sportiska, panākumiem bagāta mums visiem jaunā vasaras sezona.'],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd81c8d",
   "metadata": {
    "id": "6fd81c8d",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Inference\n",
    "Given plain text, we strip commas, tokenize with word boundaries, run the model, and insert commas after tokens labeled COMMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e8b94",
   "metadata": {
    "id": "241e8b94"
   },
   "outputs": [],
   "source": [
    "def process_text(text, model, tokenizer: PreTrainedTokenizerBase, max_len=120, verbose=True):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    source = remove_commas(text)\n",
    "    inputs = tokenizer([source], return_tensors='pt', truncation=True, max_length=max_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_len,\n",
    "        )\n",
    "    result = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    if verbose:\n",
    "        print(f'REF: {text}')\n",
    "        print(f' IN: {source}')\n",
    "        print(f'OUT: {result}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d8b3a",
   "metadata": {
    "id": "400d8b3a",
    "lines_to_next_cell": 1
   },
   "source": [
    "# Model fine-tuning\n",
    "- Track loss curves, gradient norms, and evaluation metrics over time\n",
    "- Use an appropriate optimizer and learning rate schedule (e.g., warmup + decay)\n",
    "- Watch for overfitting (gap between train and eval performance)\n",
    "- Adjust batch size, accumulation steps, or precision (fp16/bf16) if needed\n",
    "- Save best checkpoints based on validation metric (e.g., F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b22c26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f27e9a21392548728be1fc552dc3923f",
      "48732220dc044226b66a1e08a8a20ea0",
      "537922d812a444ff9397dfe8a57cf67c",
      "4389d142be5a4cb082b4b025aeb353cc",
      "5689766cce974178b19a2baf90a03b7c",
      "3ec0fe8d0cc14eb0921306693b8db333",
      "b8c990428cfb4faa9dab6c4e215b503c",
      "c20ff32d7c794031b1bef2b6c7e152a6",
      "88d1d6b03339413aa5bb68cf0014dc72",
      "f6c139cf39594d47ab264fc50d26d56e",
      "39f1d461c958442ab67924f4648cad6b",
      "0b7fc155705c46e2bf112d4804f138a2",
      "1357f8367aad4d0ead8c8b17f1169003",
      "fbba19277bb64c2daf2a16ade4c700a1",
      "a73ed8d787304c90ac9cfa76389baea4",
      "1687a2c5c2704e0c8face56280bf7e90",
      "518123f6f2464a97b23bf5369107ea9c",
      "4306113afbb14a8bbbbaf61e1ebf353f",
      "6afee5e047e34a39a3a00fa546e48099",
      "b01f3fc3d65f47bab3455ddaa8a81891",
      "878983e342814acbabfeabc5eaeba399",
      "ef520cf7675b4758943b3044687441bb",
      "8212eb3a56df4f889e29570b41608087",
      "8c13e59b45a642ae8fb8612c96ec84ec",
      "1d64e07f10a04afcab783ca2dc195f95",
      "b39db08fb3424bac9b000c2b390a1e06",
      "aae6cbcd48d74ad5af6e6f89293dd241",
      "32e1cd94e32e4974a87b2fce059bc5aa",
      "376b3e47ad0e47c491eaa5c99c981753",
      "a09363e51b0a4e8bb1879490b21dd95a",
      "7999467faa424ae2a5cd31bf75d35875",
      "56a7074c241a42d8a941b58a244b80ac",
      "bbdaf7c722294cf8bad4c8d90f7314a2",
      "6699f3f25aaa4db3896235d11806bf0b",
      "e719e8f5106e488395bae9131b8bb326",
      "2a70f7dadcd04e918f0a9533bfd1a452",
      "1367394d126f40f0a2e9568052093eb4",
      "c7e2dd07c42d4f5689436230278dc61e",
      "d433e4db9366479c840cdc7625d11b24",
      "ca314b158eff47749c721db742bfd0c2",
      "1846bc66056e43ec946d99aa6be704e6",
      "f632bb9d20b04e049db43180647a1f6f",
      "08e02eebb24845fb849aeef300335486",
      "462dc170e2ea417abbb593adc5f7ae4f",
      "0917043d9ead4208af39c010b17f72c5",
      "b1544e8e2af44f679efcc46d7d727c3e",
      "7cad9bab67ba498da2431275bc7543fe",
      "9f4b6521685245efbff255cf4ad3249c",
      "2e44ccda907647e299ee1244236ffebd",
      "88f5c8b1a84244f89a98121c44dc35b8",
      "97ee2c2eaf154851ade57bff8cbfbb5c",
      "f4fcfe81b98c406c90ebc3d1f102326d",
      "f0076d020d704c39add35bc4bbf73216",
      "c4e4eb71a6834c8dacbd029a4e54b2c0",
      "5944e067393b4320bb4f295abf8d8fb4"
     ]
    },
    "id": "02b22c26",
    "outputId": "3b81b68c-fc7e-4771-a8c1-33b972089d70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251006_093618-qvlfsjia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/artursz/punctuator/runs/qvlfsjia' target=\"_blank\">mt5_punctuator_sample</a></strong> to <a href='https://wandb.ai/artursz/punctuator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/artursz/punctuator' target=\"_blank\">https://wandb.ai/artursz/punctuator</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/artursz/punctuator/runs/qvlfsjia' target=\"_blank\">https://wandb.ai/artursz/punctuator/runs/qvlfsjia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'name': 'mt5_punctuator_sample', 'base_model': 'google/mt5-small', 'max_len': 80, 'seed': 42, 'verbose': True, 'lr': 0.001, 'bs': 8, 'train_samples': 1000, 'dev_samples': 100, 'epochs': 3, 'report_wandb': True, 'wandb_group': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27e9a21392548728be1fc552dc3923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7fc155705c46e2bf112d4804f138a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8212eb3a56df4f889e29570b41608087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6699f3f25aaa4db3896235d11806bf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0917043d9ead4208af39c010b17f72c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 03:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>Changes</th>\n",
       "      <th>Exact</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Fp</th>\n",
       "      <th>Fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.736200</td>\n",
       "      <td>0.267341</td>\n",
       "      <td>0.480874</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.341085</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.166062</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Changed\n",
      "REF: Polietilēna maisiņā tās ir sasmakušas un tāpat nav ēdamas.\n",
      "OUT: Pilietilēna maisiņā tās ir sasmakušas un tāpat nav ēdamas.\n",
      "--- Changed\n",
      "REF: 1997.g. Nīderlandē bezdarba līmenis bija zemāks nekā vidēji ES – nedaudz vairāk par 6% [Visser, Hemerijck, 9].\n",
      "OUT: Neīderlandē bezdarba līmenis bija zemāks nekā vidēji ES – nedaudz vairāk par 6% [Visser Hemerijck 9].\n",
      "--- Changed\n",
      "REF: Tāpat Kuks vērš uzmanību uz rakstniekiem, kurus ir samaitājušas Apgaismības idejas, kuri cionismam piešķir Apgaismības neprāta daļu, izraujot Toru no ebreju apziņas.\n",
      "OUT: Topat Kuks vērš uzmanību uz rakstniekiem kurus ir samaitājušas Apgaismības idejas, kuri cionismam piešķir Apgaismības neprāta daļu izraujot Toru no ebreju apziņas.\n",
      "--- Changed\n",
      "REF: Tika jautāts par iespēju šķirot atkritumus.\n",
      "OUT: Tasa jautāts par iespēju šķirot atkritumus.\n",
      "--- Changed\n",
      "REF: Vairākās pašvaldībās iedzīvotāji vērsušies Konkurences padomē, jo sūdzas, ka pēc tam, kad pašvaldība izvēlējusies „ZAAO\" pakalpojumus, cena par atkritumu apsaimniekošanu pieaugusi.\n",
      "OUT: Vairākās pašvaldībās iedzīvotāji vērsušies Konkurences padomē, jo sūdzas, ka pašvaldība izvēlējusies „ZAAO\" pakalpojumus cena par atkritumu apsaimniekošanu pieaugusi.\n",
      "--- Changed\n",
      "REF: 22 mēnešus vecais princis Džordžs cieši tur mazo māsu, nekautrējas to noskūpstīt un veltīt kamerai savu burvīgo smaidu.\n",
      "OUT: 22. mēnešus vecais princis Džordžs cieši tur mazo māsu nekautrējas to noskūpstīt un veltīt kamerai savu burvīgo smaidu.\n",
      "--- Changed\n",
      "REF: Ja mēs esam slimi vai nevarīgi, ja esam spiesti lietot zāles, ja esam spiesti stāvēt rindās uz izmeklējumiem, pie ārstiem, aptiekās pēc zālēm – kas var padarīt pašsajūtu labāku?\n",
      "OUT: Ja mēs esam slimi vai nevarīgi, ja esam spiesti stāvēt rindās uz izmeklējumiem pie ārstiem aptiekās pēc zālēm – kas var padarīt pašsajūtu labāku?\n",
      "--- Changed\n",
      "REF: Jaunāko dēlu Skots nesa uz rokām, bet Kortnija stūma ratiņus ar sešus gadus veco Meisonu, kamēr četrus gadus vecā Penelope gāja kājām.\n",
      "OUT: Jaunāko dēlu Skots nesa uz rokām, bet Kortnija stūma ratiņus ar sešus gadus vecā Penelope gāja kājām.\n",
      "--- Changed\n",
      "REF: Augustā Afleks sociālajos tīklos publicēja video no uzņemšanas laukuma, kur Manganello bija iejuties ļaunā varoņa Dedstroka lomā.\n",
      "OUT: Augā Afleks sociālajos tīklos publicēja video no uzņemšanas laukuma, kur Manganello bija iejuties ļaunā varoņa Dedstroka lomā.\n",
      "--- Changed\n",
      "REF: 22 mēnešus vecais princis Džordžs cieši tur mazo māsu, nekautrējas to noskūpstīt un veltīt kamerai savu burvīgo smaidu.\n",
      "OUT: 22. mēnešus vecais princis Džordžs cieši tur mazo māsu nekautrējas to noskūpstīt un veltīt kamerai savu burvīgo smaidu.\n",
      "--- Changed\n",
      "REF: Augustā Afleks sociālajos tīklos publicēja video no uzņemšanas laukuma, kur Manganello bija iejuties ļaunā varoņa Dedstroka lomā.\n",
      "OUT: Augā Afleks sociālajos tīklos publicēja video no uzņemšanas laukuma, kur Manganello bija iejuties ļaunā varoņa Dedstroka lomā.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.\n",
      " IN: Vēl 9% sacīja ka nav izlēmuši kā balsot bet 3 2% atteicās atbildēt.\n",
      "OUT: Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3 2% atteicās atbildēt.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/changes</td><td>█▁▁</td></tr><tr><td>eval/exact</td><td>▁▇█</td></tr><tr><td>eval/f1</td><td>▁▄█</td></tr><tr><td>eval/fn</td><td>█▆▁</td></tr><tr><td>eval/fp</td><td>█▁▁</td></tr><tr><td>eval/loss</td><td>█▂▁</td></tr><tr><td>eval/p</td><td>▁██</td></tr><tr><td>eval/r</td><td>▁▃█</td></tr><tr><td>eval/runtime</td><td>█▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇</td></tr><tr><td>+7</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/changes</td><td>0.02</td></tr><tr><td>eval/exact</td><td>0.5</td></tr><tr><td>eval/f1</td><td>0.55738</td></tr><tr><td>eval/fn</td><td>78</td></tr><tr><td>eval/fp</td><td>3</td></tr><tr><td>eval/loss</td><td>0.14672</td></tr><tr><td>eval/p</td><td>0.94444</td></tr><tr><td>eval/r</td><td>0.39535</td></tr><tr><td>eval/runtime</td><td>18.0573</td></tr><tr><td>eval/samples_per_second</td><td>5.538</td></tr><tr><td>+12</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mt5_punctuator_sample</strong> at: <a href='https://wandb.ai/artursz/punctuator/runs/qvlfsjia' target=\"_blank\">https://wandb.ai/artursz/punctuator/runs/qvlfsjia</a><br> View project at: <a href='https://wandb.ai/artursz/punctuator' target=\"_blank\">https://wandb.ai/artursz/punctuator</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251006_093618-qvlfsjia/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(\n",
    "    name='punctuator',\n",
    "    base_model='google/mt5-small',\n",
    "    max_len=80,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "    lr=1e-3,\n",
    "    bs=8,\n",
    "    train_samples=None,\n",
    "    dev_samples=100,\n",
    "    epochs=3,\n",
    "    report_wandb=True,\n",
    "    wandb_group=None\n",
    "):\n",
    "    if report_wandb and not wandb.api.api_key:\n",
    "        print('Not authenticated with W&B')\n",
    "        report_wandb = False\n",
    "\n",
    "    with wandb.init(project='punctuator', group=wandb_group, name=name) if report_wandb else nullcontext():\n",
    "        print('Train:', locals())\n",
    "        set_seed(seed)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "        # Load dataset\n",
    "        ds = build_dataset(tokenizer=tokenizer, train_samples=train_samples, dev_samples=dev_samples, max_length=max_len)\n",
    "\n",
    "        # Initialize base model for tokenize sequence to sequence task\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
    "        model.config.use_cache = False\n",
    "\n",
    "        # Define training hyperparameters\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=name,\n",
    "            report_to='wandb' if report_wandb else 'none',\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            num_train_epochs=epochs,\n",
    "            warmup_ratio=0.05,\n",
    "            gradient_accumulation_steps=1,\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=True,\n",
    "            logging_steps=50,\n",
    "            save_total_limit=1,\n",
    "            save_strategy='epoch',\n",
    "            eval_strategy='epoch',\n",
    "            eval_accumulation_steps=1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='f1',\n",
    "            greater_is_better=True,\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=max_len * 2,\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds['train'],\n",
    "            eval_dataset=ds['dev'],\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "            compute_metrics=lambda p: compute_metrics(p, tokenizer, verbose=verbose),\n",
    "        )\n",
    "\n",
    "        # Actual training\n",
    "        trainer.train()\n",
    "        trainer.save_model(name)\n",
    "        tokenizer.save_pretrained(name)\n",
    "\n",
    "        process_text('Vēl 9% sacīja, ka nav izlēmuši kā balsot, bet 3,2% atteicās atbildēt.', trainer.model, tokenizer, max_len=max_len)\n",
    "\n",
    "main('mt5_punctuator_sample', train_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996926c",
   "metadata": {
    "id": "a996926c"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe76c7",
   "metadata": {
    "id": "83fe76c7",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "m = AutoModelForSeq2SeqLM.from_pretrained('mt5_punctuator_sample')\n",
    "t = AutoTokenizer.from_pretrained('mt5_punctuator_sample')\n",
    "process_text('Nogalināt nedrīkst, apžēlot!', m, t)\n",
    "process_text('Palielināt izdevumus nedrīkst taupīt!', m, t)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
