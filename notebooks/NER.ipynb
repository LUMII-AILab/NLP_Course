{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwmo9F8YAKg27aRVMBAFyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "fElewPNO5geV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SjpzOn9h2ipL"
      },
      "outputs": [],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Sentence\n",
        "from flair.nn import Classifier\n",
        "from flair.data import Corpus\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.models import SequenceTagger\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "cJ_o8iLi5aC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "NER tagging"
      ],
      "metadata": {
        "id": "oXpIVJP2mRTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_tag(sentence, model='ner'):\n",
        "    # make a sentence\n",
        "    sentence = Sentence(sentence)\n",
        "\n",
        "    # load the NER tagger\n",
        "    tagger = Classifier.load(model)\n",
        "\n",
        "    # run NER over sentence\n",
        "    tagger.predict(sentence)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "Uoh2moi_5-rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"George Washington was the first president of the United States of America.\"\n",
        "sentence = ner_tag(example)\n",
        "# print the sentence with all annotations\n",
        "print(sentence)"
      ],
      "metadata": {
        "id": "oj4JNy8T6Zpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "metadata": {
        "id": "66cf1qSI7VBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "**Different NER models offered by Flair**\n",
        "___\n",
        "Standard Flair NER model offers 4 classes:\n",
        "* PER (person),\n",
        "* ORG (organization),\n",
        "* LOC (location),\n",
        "* MISC (miscellanious)\n",
        "\n",
        "\n",
        "Alternatively the 'ner-ontonotes-large' offers 18 seperate classes."
      ],
      "metadata": {
        "id": "PLu-_uwPl0By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'On September 1st George won 1 dollar while watching Game of Thrones.'\n",
        "\n",
        "# Standard Flair NER model offer\n",
        "ner_tag(sentence)\n",
        "# Expanded NER model\n",
        "ner_tag(sentence, 'ner-ontonotes-large')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4KmD260Gl5VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a small custom Flair NER model\n",
        "\n",
        "\n",
        "Example of code for training English NER model: https://github.com/flairNLP/flair/blob/master/resources/docs/EXPERIMENTS.md"
      ],
      "metadata": {
        "id": "V5UqmbBRw-6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CONLL_03\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
        "from typing import List\n",
        "\n",
        "!mkdir train"
      ],
      "metadata": {
        "id": "3PX3rOSJAAn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "def matcher(string, pattern):\n",
        "    '''\n",
        "    Return the start and end index of any pattern present in the text.\n",
        "    '''\n",
        "    match_list = []\n",
        "    pattern = pattern.strip()\n",
        "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
        "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
        "    if (match.size == len(pattern)):\n",
        "        start = match.a\n",
        "        end = match.a + match.size\n",
        "        match_tup = (start, end)\n",
        "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
        "        match_list.append(match_tup)\n",
        "\n",
        "    return match_list, string\n",
        "\n",
        "def mark_sentence(s, match_list):\n",
        "    '''\n",
        "    Marks all the entities in the sentence as per the BIO scheme.\n",
        "    '''\n",
        "    word_dict = {}\n",
        "    for word in s.split():\n",
        "        word_dict[word] = 'O'\n",
        "\n",
        "    for start, end, e_type in match_list:\n",
        "        temp_str = s[start:end]\n",
        "        tmp_list = temp_str.split()\n",
        "        if len(tmp_list) > 1:\n",
        "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
        "            for w in tmp_list[1:]:\n",
        "                word_dict[w] = 'I-' + e_type\n",
        "        else:\n",
        "            word_dict[temp_str] = 'B-' + e_type\n",
        "    return word_dict\n",
        "\n",
        "def clean(text):\n",
        "    '''\n",
        "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
        "    '''\n",
        "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
        "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
        "    for i in text:\n",
        "        if i in filters:\n",
        "            text = text.replace(i, \" \" + i)\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_data(df, filepath):\n",
        "    '''\n",
        "    The function responsible for the creation of data in the said format.\n",
        "    '''\n",
        "    with open(filepath , 'w') as f:\n",
        "        for text, annotation in zip(df.text, df.annotation):\n",
        "            text = clean(text)\n",
        "            text_ = text\n",
        "            match_list = []\n",
        "            for i in annotation:\n",
        "                a, text_ = matcher(text, i[0])\n",
        "                match_list.append((a[0][0], a[0][1], i[1]))\n",
        "\n",
        "            d = mark_sentence(text, match_list)\n",
        "\n",
        "            for i in d.keys():\n",
        "                f.writelines(i + ' ' + d[i] +'\\n')\n",
        "            f.writelines('\\n')\n",
        "\n",
        "def create_set(file, data):\n",
        "    ## path to save the txt file.\n",
        "    filepath = file\n",
        "    ## creating the file.\n",
        "    create_data(data, filepath)\n",
        "\n",
        "\n",
        "\n",
        "def example_set():\n",
        "    # Train data\n",
        "    data = pd.DataFrame([[\"Horses are too tall and they pretend to care about your feelings\", [(\"Horses\", \"ANIMAL\")]],\n",
        "                  [\"Who is Shaka Khan?\", [(\"Shaka Khan\", \"PERSON\")]],\n",
        "                  [\"I like London and Berlin.\", [(\"London\", \"LOCATION\"), (\"Berlin\", \"LOCATION\")]],\n",
        "                  [\"There is a banyan tree in the courtyard\", [(\"banyan tree\", \"TREE\")]],\n",
        "                  [\"John Doe lives near Central Park.\", [(\"John Doe\", \"PERSON\"), (\"Central Park\", \"LOCATION\")]]], columns=['text', 'annotation'])\n",
        "    create_set('train/train.txt', data)\n",
        "\n",
        "    # Test data\n",
        "    data = pd.DataFrame([[\"I have 6 horses in my barn.\", [(\"horses\", \"ANIMAL\")]],\n",
        "                  [\"Did John go to Berlin?\", [(\"John\", \"PERSON\"), (\"Berlin\", \"LOCATION\")]]], columns=['text', 'annotation'])\n",
        "    create_set('train/test.txt', data)\n",
        "\n",
        "\n",
        "    # Dev data\n",
        "    data = pd.DataFrame([[\"I love visiting London!\", [(\"London\", \"LOCATION\")]],\n",
        "                  [\"Shaka Khan , where are my horses?\", [(\"Shaka Khan\", \"PERSON\"), (\"horses\", \"ANIMAL\")]]], columns=['text', 'annotation'])\n",
        "    create_set('train/dev.txt', data)\n",
        "\n",
        "\n",
        "example_set()"
      ],
      "metadata": {
        "id": "OOPszjnDISeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define columns\n",
        "columns = {0 : 'text', 1 : 'ner'}\n",
        "\n",
        "# directory where the data resides\n",
        "data_folder = 'train/'\n",
        "\n",
        "# initializing the corpus\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file = 'train.txt',\n",
        "                              test_file = 'test.txt',\n",
        "                              dev_file = 'dev.txt')"
      ],
      "metadata": {
        "id": "Krh7jsQuIife"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tag to predict\n",
        "tag_type = 'ner'\n",
        "\n",
        "# make tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_label_dictionary(label_type=tag_type)"
      ],
      "metadata": {
        "id": "_OyIIheEK15J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize embeddings\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "\n",
        "    # GloVe embeddings\n",
        "    WordEmbeddings('glove'),\n",
        "\n",
        "    # contextual string embeddings, forward\n",
        "    PooledFlairEmbeddings('news-forward', pooling='min'),\n",
        "\n",
        "    # contextual string embeddings, backward\n",
        "    PooledFlairEmbeddings('news-backward', pooling='min'),\n",
        "]\n",
        "\n",
        "\n",
        "embeddings : StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                       embeddings=embeddings,\n",
        "                                       tag_dictionary=tag_dictionary,\n",
        "                                       tag_type=tag_type,\n",
        "                                       use_crf=True)\n",
        "print(tagger)"
      ],
      "metadata": {
        "id": "4q_iRCRgLArc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              train_with_dev=True,\n",
        "              max_epochs=50)"
      ],
      "metadata": {
        "id": "DNChcrWNMYTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the trained model\n",
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "\n",
        "# create example sentence\n",
        "sentence = Sentence('I love Hokkaido')\n",
        "\n",
        "# predict the tags\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())"
      ],
      "metadata": {
        "id": "ZYg3Vx-OLxIY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}