{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPJ3FvPbsLVM"
      },
      "source": [
        "# TF-IDF\n",
        "### term frequency–inverse document frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing\n"
      ],
      "metadata": {
        "id": "jjKA0xlV1lN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import re"
      ],
      "metadata": {
        "id": "JVGrdtJN1FvK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhSsSPYu8wz"
      },
      "source": [
        "Vispirms ir nepieciešams klāsts ar individuāliem failiem, kuriem veikt tf-idf analīzi. Pielietosim iepriekš izmantotos Šekspīra darbu datus. Sadalām tos pa lugām, nolasot, kur dokumentā sākas jauna luga jeb sākas pirmais cēliens.\n",
        "\n",
        "We read and split Shakespeare works in seperate files, one work= one file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NgFy-eC1uw8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9dd865-b8bf-4f2d-bfbd-15dc7224e54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Divided into 36 files\n"
          ]
        }
      ],
      "source": [
        "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
        "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")\n",
        "iter = 0\n",
        "for line in lines:\n",
        "    if line == \"\\\"ACT I\\\"\":\n",
        "#        outfile.close()\n",
        "        iter += 1\n",
        "        outfile = open(\"work_\"+str(iter)+\".txt\",\"w\")\n",
        "    outfile.write(line+\"\\n\")\n",
        "outfile.close()\n",
        "work_count = iter\n",
        "print(\"Divided into {} files\".format(work_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZhtC0RwTRMV"
      },
      "source": [
        "##Veicam tf-idf rezultāta aprēķināšanu ar pašu veidotām funkcijām\n",
        "##Writing functions for tf-idf calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "478xrx7JGXxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e83533-ca6d-4943-dd4c-c683158215c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word \"Romeo\" was found in 1 out of 36 files.\n",
            "Word \"battle\" was found in 21 out of 36 files.\n",
            "Word \"said\" was found in 36 out of 36 files.\n"
          ]
        }
      ],
      "source": [
        "# Dokumentu biežums\n",
        "# Frequency in documents\n",
        "\n",
        "def number_of_docs_with_term(word):\n",
        "    found = 0\n",
        "    for i in range(1,work_count+1):\n",
        "        infile = open(\"work_\"+str(i)+\".txt\", \"r\")\n",
        "        for line in infile:\n",
        "            line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "            tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "            if word in tokens:\n",
        "                found += 1\n",
        "                break\n",
        "        infile.close()\n",
        "    return found\n",
        "\n",
        "\n",
        "def doc_appearance(word):\n",
        "    found = number_of_docs_with_term(word)\n",
        "    print(\"Word \\\"{}\\\" was found in {} out of {} files.\".format(word, found, work_count))\n",
        "\n",
        "doc_appearance(\"Romeo\")\n",
        "doc_appearance(\"battle\")\n",
        "doc_appearance(\"said\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "el6jHzVqLG98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a8f08a-e3df-4b39-caf5-56832e5f53f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found word \"king\" in file \"work_28.txt\" a total of 1 times.\n",
            "Found word \"Romeo\" in file \"work_28.txt\" a total of 128 times.\n",
            "Found word \"said\" in file \"work_28.txt\" a total of 14 times.\n"
          ]
        }
      ],
      "source": [
        "# Vārda biežums\n",
        "# Word frequncy in document\n",
        "def word_frequency(word, doc_id):\n",
        "    freq = 0\n",
        "    infile = open(\"work_\"+str(doc_id)+\".txt\", \"r\")\n",
        "    for line in infile:\n",
        "        line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "        for token in tokens:\n",
        "            if token == word:\n",
        "                freq += 1\n",
        "    return freq\n",
        "\n",
        "\n",
        "def frequency(word, doc_id):\n",
        "    w_freq = word_frequency(word, doc_id)\n",
        "    filename = \"work_\"+str(doc_id)+\".txt\"\n",
        "    print(\"Found word \\\"{}\\\" in file \\\"{}\\\" a total of {} times.\".format(word, filename, w_freq))\n",
        "\n",
        "frequency(\"king\", 28)\n",
        "frequency(\"Romeo\", 28)\n",
        "frequency(\"said\", 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U41Rdet4sfwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0775045-1cd4-4282-ebd3-183fb7ba51a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tf-idf score for word \"battle\" in file \"work_28.txt\" is: 0.0\n",
            "The tf-idf score for word \"Romeo\" in file \"work_28.txt\" is: 501.4746537067877\n",
            "The tf-idf score for word \"Juliet\" in file \"work_28.txt\" is: 165.0783643268774\n"
          ]
        }
      ],
      "source": [
        "# Calculating tf-idf using formula\n",
        "import numpy as np\n",
        "def tf_idf(word, doc_id):\n",
        "    inverse_doc_freq = np.log((1+work_count)/(number_of_docs_with_term(word)+1))+1\n",
        "    score = word_frequency(word, doc_id) * inverse_doc_freq\n",
        "    return score\n",
        "\n",
        "\n",
        "def print_score(word, doc_id):\n",
        "    filename = \"work_\"+str(doc_id)+\".txt\"\n",
        "    score = tf_idf(word, doc_id)\n",
        "    print(\"The tf-idf score for word \\\"{}\\\" in file \\\"{}\\\" is: {}\".format(word, filename, score))\n",
        "\n",
        "print_score(\"battle\", 28)\n",
        "print_score(\"Romeo\", 28)\n",
        "print_score(\"Juliet\", 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRQtizsUTnqB"
      },
      "source": [
        "##Alternative: scikit-learn library for tf-idf calculation\n",
        "TfidfVectorizer papildus veic arī rezultātu smoothing (smooth_idf=True) un normalizēšanu(norm='l2'). <br>\n",
        "Tādēļ ar bibliotēku iegūtie tf-idf reultāti atšķiras no iepriekš aprēķinātajiem.\n",
        "\n",
        "This library includes smoothing and normalisation, thus results differs from the above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XQF3ae_aTxHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cfe04a-baa8-46fd-c9e0-9e8de6242335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       document    term     tfidf\n",
            "612231  work_28  juliet  0.225776 \n",
            "\n",
            "       document   term     tfidf\n",
            "618023  work_28  romeo  0.583629 \n",
            "\n",
            "       document  term     tfidf\n",
            "618237  work_28  said  0.015305 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "text_files = []\n",
        "for i in range(1,work_count+1):\n",
        "    text_files.append(\"work_\"+str(i)+\".txt\")\n",
        "\n",
        "text_titles = [text.split(\".\")[0] for text in text_files]\n",
        "\n",
        "\n",
        "# Get tf-idf score data for word\n",
        "def vectorizer_score(word, doc_id):\n",
        "\n",
        "    # Initialize and run TfidfVectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
        "    tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
        "\n",
        "    # Create a DataFrame out of the resulting tf–idf vector\n",
        "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "    tfidf_df = tfidf_df.stack().reset_index()\n",
        "    tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
        "\n",
        "\n",
        "    tfidf_df = tfidf_df[tfidf_df['document'] == 'work_'+str(doc_id)]\n",
        "    print(tfidf_df[tfidf_df['term'] == word], \"\\n\")\n",
        "\n",
        "\n",
        "vectorizer_score(\"juliet\", 28)\n",
        "vectorizer_score(\"romeo\", 28)\n",
        "vectorizer_score(\"said\", 28)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}