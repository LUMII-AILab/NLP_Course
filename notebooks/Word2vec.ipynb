{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOp459d616_I"
      },
      "source": [
        "# Word2vec modeļa izveide\n",
        "# Building and using Word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing\n"
      ],
      "metadata": {
        "id": "UvsIfw7O-4rg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH36CAfpAGIi"
      },
      "source": [
        "Teksta lejuplāde un sadalīšana rindiņās\n",
        "\n",
        "Preprocessing - download and line segmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code cell to install gensim library and restart the session\n",
        "# After the session is restarted, comment this cell\n",
        "!pip install --upgrade gensim\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "z8-sDhyGtZVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SLRTiEwZ1_nq"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import re\n",
        "import multiprocessing\n",
        "from time import time\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# change to your own path if you have downloaded the file locally\n",
        "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
        "#url = \"https://repository.clarin.lv/repository/xmlui/bitstream/handle/20.500.12574/41/rainis_v20180716.txt?sequence=1&isAllowed=y\"\n",
        "# read file into list of lines\n",
        "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yed3eXbI2Wpl"
      },
      "source": [
        "Teksta priekšapstrāde - sadalīšana tekstvienībās\n",
        "\n",
        "Tokenization\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E4zjA3I72dGr"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "\n",
        "for line in lines:\n",
        "   # remove punctuation\n",
        "   line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
        "#   line = line.lower()\n",
        "\n",
        "   # simple tokenizer\n",
        "   tokens = re.findall(r'\\b\\w+\\b', line)\n",
        "\n",
        "   # only keep lines with at least one token\n",
        "   if len(tokens) > 1:\n",
        "      sentences.append(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n"
      ],
      "metadata": {
        "id": "MpA9loiS-95n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBHjtlJd2rm9"
      },
      "source": [
        "\n",
        "\n",
        "Modeļa apmācība\n",
        "\n",
        "Training\n",
        "\n",
        "The parameters:\n",
        "\n",
        "*   min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
        "*   window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
        "* size = int - Dimensionality of the feature vectors. - (50, 300)\n",
        "* sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
        "* alpha = float - The initial learning rate - (0.01, 0.05)\n",
        "* min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
        "* negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
        "* workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "whGNCwMt2vk9"
      },
      "outputs": [],
      "source": [
        "\n",
        "w2v_model = Word2Vec(\n",
        "         sentences,\n",
        "         min_count=3,   # Ignore words that appear less than this\n",
        "         vector_size=50,       # Dimensionality of word embeddings\n",
        "         sg = 1,        # skipgrams\n",
        "         window=7,      # Context window for words during training\n",
        "         epochs=40)       # Number of epochs training over corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiziDuQ2GA7z"
      },
      "source": [
        "Alternatīva: apmacība pa soļiem\n",
        "\n",
        "Training in several steps (alternative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4atVIppF_3A"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     vector_size=50,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=cores-1)\n",
        "\n",
        "\n",
        "t = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA6Cm0AAIPEZ"
      },
      "outputs": [],
      "source": [
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iQuYz4n3knk"
      },
      "source": [
        "##Modelis darbībā\n",
        "\n",
        "##Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii2MQPnP3joT"
      },
      "outputs": [],
      "source": [
        "#w2v_model.wv.most_similar('mīla')\n",
        "w2v_model.wv.most_similar('Romeo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-HvY97BFQjfR"
      },
      "outputs": [],
      "source": [
        "# w2v_model.wv.most_similar(positive=[\"saule\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBPVJqIbnbJ4"
      },
      "outputs": [],
      "source": [
        "# w2v_model.wv.most_similar(negative=[\"saule\"])\n",
        "w2v_model.wv.most_similar(negative=[\"Romeo\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7eHbtZfzQxL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83780074-ad08-4a68-f6f8-ff9f07600668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.543802"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# w2v_model.wv.similarity(\"saule\", \"pavasars\")\n",
        "w2v_model.wv.similarity(\"Juliet\", \"Romeo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BR1-ixqRX7d"
      },
      "outputs": [],
      "source": [
        "# w2v_model.wv.doesnt_match([\"brīve\", \"pavasars\", \"saule\"])\n",
        "w2v_model.wv.doesnt_match([\"Juliet\", \"battle\", \"Romeo\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Papildus, varam aplūkot semantiskās analoģijas:"
      ],
      "metadata": {
        "id": "JuR2Ncq1tZn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: 'king' - 'man' + 'woman'\n",
        "result = w2v_model.wv['king'] - w2v_model.wv['man']  + w2v_model.wv['woman'] # Vector addition/subraction\n",
        "w2v_model.wv.similar_by_vector(result, topn=5)"
      ],
      "metadata": {
        "id": "Aip5Req2jqc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A different method of determining analogies using cosine similarity\n",
        "w2v_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
      ],
      "metadata": {
        "id": "Mpxiuqc4j3lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCV5QhcpAV_X"
      },
      "source": [
        "Citi nopietnāki un mazāk nopietni materiāli:\n",
        "\n",
        "* Tensorflow Word2Vec Tutorial:https://www.tensorflow.org/text/tutorials/word2vec\n",
        "* Gensim Word2Vec Tutorial: https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook#Getting-Started\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPyb-k6lZVS"
      },
      "source": [
        "## Vārdu līdzības vizualizācija\n",
        "## Visualisation\n",
        "Vizualizācijai pielietojam MatPlotLib un SeaBorn bibliotēkas. <br>\n",
        "Sklearn bibliotēka pielieto PCA un TSNE metodes, kas pārveido vārdus vektoru formā (skatīt vector_size pie word2vec), par punktu 2D telpā. <br>\n",
        "Šīs dimensiju redukcijas metodes pēc iespējas saglabā individuālo vārdu līdzības."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rTT7CAjAlX_B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IYgnZVugh6B8"
      },
      "outputs": [],
      "source": [
        "def tsnescatterplot(model, word, list_names):\n",
        "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
        "    its list of most similar words, and a list of words.\n",
        "    \"\"\"\n",
        "    arrays = np.empty((0, 50), dtype='f')\n",
        "    word_labels = [word]\n",
        "    color_list  = ['red']\n",
        "\n",
        "    # adds the vector of the query word\n",
        "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
        "\n",
        "    # gets list of most similar words\n",
        "    close_words = model.wv.most_similar([word])\n",
        "\n",
        "    # adds the vector for each of the closest words to the array\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('blue')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "\n",
        "    # adds the vector for each of the words from list_names to the array\n",
        "    for wrd in list_names:\n",
        "        wrd_vector = model.wv.__getitem__([wrd])\n",
        "        word_labels.append(wrd)\n",
        "        color_list.append('green')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "\n",
        "    # Reduces the dimensionality from 50 to 21 dimensions with PCA\n",
        "    reduc = PCA(n_components=21).fit_transform(arrays)\n",
        "\n",
        "    # Finds t-SNE coordinates for 2 dimensions\n",
        "    np.set_printoptions(suppress=True)\n",
        "\n",
        "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
        "\n",
        "    # Sets everything up to plot\n",
        "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
        "                       'y': [y for y in Y[:, 1]],\n",
        "                       'words': word_labels,\n",
        "                       'color': color_list})\n",
        "\n",
        "    fig, _ = plt.subplots()\n",
        "    fig.set_size_inches(9, 9)\n",
        "\n",
        "    # Basic plot\n",
        "    p1 = sns.regplot(data=df,\n",
        "                     x=\"x\",\n",
        "                     y=\"y\",\n",
        "                     fit_reg=False,\n",
        "                     marker=\"o\",\n",
        "                     scatter_kws={'s': 40,\n",
        "                                  'facecolors': df['color']\n",
        "                                 }\n",
        "                    )\n",
        "\n",
        "    # Adds annotations one by one with a loop\n",
        "    for line in range(0, df.shape[0]):\n",
        "         p1.text(df[\"x\"][line],\n",
        "                 df['y'][line],\n",
        "                 '  ' + df[\"words\"][line].title(),\n",
        "                 horizontalalignment='left',\n",
        "                 verticalalignment='bottom', size='medium',\n",
        "                 color=df['color'][line],\n",
        "                 weight='normal'\n",
        "                ).set_size(15)\n",
        "\n",
        "\n",
        "    plt.xlim(Y[:, 0].min()-np.absolute(Y[:, 0].min())*0.2, Y[:, 0].max()+np.absolute(Y[:, 0].max())*0.2)\n",
        "    plt.ylim(Y[:, 1].min()-np.absolute(Y[:, 1].min())*0.2, Y[:, 1].max()+np.absolute(Y[:, 1].max())*0.2)\n",
        "\n",
        "    plt.title('t-SNE visualization for {}'.format(word.title()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAHrvz6uiCWM"
      },
      "outputs": [],
      "source": [
        "# Red: Original word\n",
        "# Blue: 10 closest word matches to the original word\n",
        "# Green: 10 furthest word matches to the original word\n",
        "\n",
        "word = \"Romeo\"\n",
        "tsnescatterplot(w2v_model, word, [i[0] for i in w2v_model.wv.most_similar(negative=[word])])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}