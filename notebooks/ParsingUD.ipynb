{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/ParsingUD.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "rJ39YBdj6dcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REST APIs for UD parsing"
      ],
      "metadata": {
        "id": "0KWZaHvNyNkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import urllib\n",
        "import json"
      ],
      "metadata": {
        "id": "Skq90jPoySM0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lv = \"Ģertrūdes ielas teātris aicina uz Baltijas neatkarīgo teātru viesizrādēm, kas notiks 3. aprīlī.\"\n",
        "text_en = \"Gertrude Street Theater invites you to guest performances of Baltic independent theaters, which will take place on April 3.\""
      ],
      "metadata": {
        "id": "Rsd8eze_0QNY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LV-PIPE\n",
        "\n",
        "https://nlp.ailab.lv"
      ],
      "metadata": {
        "id": "YksTZga9n7Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calls the LV-PIPE REST API\n",
        "def process_lvpipe(text, steps=None, api_url=\"https://nlp.ailab.lv/api/nlp\"):\n",
        "    steps = steps or ['tokenizer', 'morpho', 'parser'] # +ner\n",
        "\n",
        "    response = requests.post(api_url, json={'data': {'text': text}, 'steps': steps})\n",
        "    return response.json()['data']"
      ],
      "metadata": {
        "id": "IB0A8oA_KwOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = process_lvpipe(text_lv)\n",
        "\n",
        "with open(\"lvpipe_output.json\", 'w') as f:\n",
        "    json.dump(doc, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open(\"lvpipe_output.json\", 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "s0KA-ZWIAg1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most of the work in NLP is to convert data from one format to another ;)\n",
        "\n",
        "# This code block allows for customisation and format conversion,\n",
        "# since other NLP pipelines might use different data formats and naming conventions.\n",
        "\n",
        "ID     = \"index\"\n",
        "FORM   = \"form\"\n",
        "LEMMA  = \"lemma\"\n",
        "UPOS   = \"upos\"\n",
        "XPOS   = \"tag\"\n",
        "FEATS  = \"ufeats\"\n",
        "HEAD   = \"parent\"\n",
        "DEPREL = \"deprel\"\n",
        "DEPS   = \"deps\"\n",
        "MISC   = \"misc\"\n",
        "\n",
        "FORMAT = [ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC]\n",
        "\n",
        "\n",
        "# For each token, creates a TAB-separated list of its UD features\n",
        "def token_features(token):\n",
        "    line = \"\"\n",
        "    cols = FORMAT\n",
        "    for x in cols:\n",
        "        if token.get(x): line += str(token[x]) + '\\t'\n",
        "        else: line += \"_\\t\"\n",
        "    return line.lstrip('\\t') + '\\n'\n",
        "\n",
        "\n",
        "# De-tokenizes a tokenized sentence\n",
        "def detokenize_sentence(sentence):\n",
        "    text = \"\"\n",
        "    for token in sentence[\"tokens\"]:\n",
        "        text += token[FORM] + ' ' # Naive glueing\n",
        "    return text.lstrip(' ')\n",
        "\n",
        "\n",
        "# Reads a JSON file produced by LV-PIPE and converts it to the CoNLL-U format\n",
        "def json_to_conll(json_file, conll_file):\n",
        "    with open(json_file, encoding=\"utf-8\") as j_file:\n",
        "        data = json.load(j_file)\n",
        "\n",
        "    with open(conll_file, 'w', encoding=\"utf-8\") as c_file:\n",
        "        for sentence in data[\"sentences\"]:\n",
        "            c_file.write(\"# text = {}\\n\".format(detokenize_sentence(sentence)))\n",
        "\n",
        "            for token in sentence[\"tokens\"]:\n",
        "                c_file.write(token_features(token))\n",
        "\n",
        "            c_file.write('\\n')"
      ],
      "metadata": {
        "id": "O3yIN6UQnz93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty-prints a CoNLL-U file\n",
        "def print_conllu(conll_file):\n",
        "    with open(conll_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    structured_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"#\"):\n",
        "            # Comment lines\n",
        "            structured_lines.append(line.strip())\n",
        "        elif line.strip():\n",
        "            # Non-empty non-comment lines\n",
        "            structured_lines.append(line.strip().split(\"\\t\"))\n",
        "        else:\n",
        "            # Empty lines - sentence breaks\n",
        "            structured_lines.append('')\n",
        "\n",
        "    # Find the maximum width of each column\n",
        "    data_rows = [row for row in structured_lines if isinstance(row, list)]\n",
        "    max_widths = [max(len(row[i]) for row in data_rows) for i in range(len(data_rows[0]))]\n",
        "\n",
        "    for line in structured_lines:\n",
        "        if isinstance(line, list):\n",
        "            # A data row\n",
        "            print(' '.join(word.ljust(max_widths[i]) for i, word in enumerate(line)))\n",
        "        else:\n",
        "            # A comment or empty line\n",
        "            print(line)\n",
        "            if line == \"\": print()"
      ],
      "metadata": {
        "id": "xpJHR2FI1uor"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_conll(\"lvpipe_output.json\", \"lvpipe_output.conll\")\n",
        "\n",
        "print_conllu(\"lvpipe_output.conll\")"
      ],
      "metadata": {
        "id": "msV5SyEWqUCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UDpipe\n",
        "\n",
        "https://ufal.mff.cuni.cz/udpipe/2"
      ],
      "metadata": {
        "id": "KpCqM8zJn2ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calls the UDpipe REST API\n",
        "def process_udpipe(text, model, steps=None, api_url=\"https://lindat.mff.cuni.cz/services/udpipe/api/process\"):\n",
        "    model = \"model=\" + model\n",
        "    steps = steps or \"tokenizer&tagger&parser\"\n",
        "    text = \"data=\" + urllib.parse.quote(text)\n",
        "\n",
        "    response = requests.get(api_url + '?' + model + '&' + steps + '&' + text)\n",
        "    return response.json()['result']"
      ],
      "metadata": {
        "id": "Oi2GW7cmcZzK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lv = process_udpipe(text_lv, \"lv\")\n",
        "doc_en = process_udpipe(text_en, \"en\")\n",
        "\n",
        "with open(\"udpipe_output.conll\", 'w', encoding=\"utf-8\") as c_file:\n",
        "    c_file.write(doc_lv)\n",
        "    c_file.write(doc_en)\n",
        "\n",
        "print_conllu(\"udpipe_output.conll\")"
      ],
      "metadata": {
        "id": "HaOgHysNstNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code block allows for the conversion from CoNLL-U to LV-PIPE's JSON.\n",
        "# Again, most of the code is for converting data back and forth.\n",
        "\n",
        "def create_token(data_row):\n",
        "    return {\n",
        "        \"index\":    data_row[0],\n",
        "        \"form\":     data_row[1],\n",
        "        \"lemma\":    data_row[2],\n",
        "        \"upos\":     data_row[3],\n",
        "        \"pos\":      data_row[4],  # XPOS\n",
        "        \"features\": data_row[5],  # FEATS\n",
        "        \"parent\":   data_row[6],  # HEAD\n",
        "        \"deprel\":   data_row[7],\n",
        "        \"deps\":     data_row[8],  # +DEPS\n",
        "        \"misc\":     data_row[9]   # +MISC\n",
        "    }\n",
        "\n",
        "\n",
        "def create_sentence(text, tokens):\n",
        "    return {\n",
        "        \"tokens\": tokens,\n",
        "        \"text\": text.strip()\n",
        "    }\n",
        "\n",
        "\n",
        "def create_doc(sentences, full_text):\n",
        "    return {\n",
        "        \"sentences\": sentences,\n",
        "        \"text\": full_text.strip()\n",
        "    }\n",
        "\n",
        "\n",
        "def conll_to_json(conll_file):\n",
        "    with open(conll_file, 'r', encoding=\"utf-8\") as c_file:\n",
        "        lines = c_file.readlines()\n",
        "\n",
        "    sentences = []\n",
        "    full_text = \"\"\n",
        "    tokens = []\n",
        "    text = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        data_row = line.split('\\t')\n",
        "\n",
        "        if data_row[0].isdigit():\n",
        "            tokens.append(create_token(data_row))\n",
        "        elif line and line.startswith(\"# text\"):\n",
        "            text = line[9:]\n",
        "        elif not line and text and tokens:\n",
        "            sentences.append(create_sentence(text, tokens))\n",
        "            full_text += ' ' + text\n",
        "            tokens = []\n",
        "            text = \"\"\n",
        "\n",
        "    return create_doc(sentences, full_text)"
      ],
      "metadata": {
        "id": "LMrYTMwBlL7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"udpipe_output.json\", 'w') as f:\n",
        "    json.dump(conll_to_json(\"udpipe_output.conll\"), f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(\"udpipe_output.json\", 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "IjGybUOPvLC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python modules"
      ],
      "metadata": {
        "id": "xXCn7YyLyYrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stanza (revisited)\n",
        "\n",
        "https://stanfordnlp.github.io/stanza/performance.html"
      ],
      "metadata": {
        "id": "4axMbGt9yf_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "\n",
        "import stanza"
      ],
      "metadata": {
        "id": "ysatLmrbw83K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download('lv')\n",
        "stanza.download('en')"
      ],
      "metadata": {
        "id": "PGpeuW6vw_63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_lv = stanza.Pipeline(lang='lv', processors='tokenize,pos,lemma,depparse')\n",
        "nlp_en = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')"
      ],
      "metadata": {
        "id": "ev145Pm9xV3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "\n",
        "docs.append(nlp_lv(text_lv))\n",
        "docs.append(nlp_en(text_en))\n",
        "\n",
        "with open(\"stanza_output.conll\", 'w', encoding=\"utf-8\") as s_file:\n",
        "    # For each sentence in a document,\n",
        "    # iterate over the tokens and get their features.\n",
        "\n",
        "    for d in docs:\n",
        "        for s in d.sentences:\n",
        "            s_file.write(f'# text = {s.text}\\n')\n",
        "            for w in s.words:\n",
        "                s_file.write(f'{w.id}\\t{w.text}\\t{w.upos}\\t{w.xpos}\\t{w.head}\\t{s.words[w.head-1].text if w.head > 0 else \"_\"}\\t{w.deprel}\\n')\n",
        "            s_file.write(\"\\n\")\n",
        "\n",
        "print_conllu(\"stanza_output.conll\")"
      ],
      "metadata": {
        "id": "I_9DNHfoy8Ru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}