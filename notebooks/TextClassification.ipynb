{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/LUMII-AILab/NLP_Course/blob/main/notebooks/TextClassification.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "fkWv6p4KNGlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using existing libs for language detection"
      ],
      "metadata": {
        "id": "xcVNYWCBasNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Language identification\n",
        "# LangID (2016): lightweight, trained on Wikipedia etc., character n-grams, NB classifier\n",
        "# See also fastText, etc.\n",
        "\n",
        "!pip install langid\n",
        "\n",
        "import langid"
      ],
      "metadata": {
        "id": "NV42lMW0C2Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(langid.classify(\"this is a test\"))\n",
        "print(langid.classify(\"šis ir tests\"))"
      ],
      "metadata": {
        "id": "fehEcwe7ULSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langid.set_languages(['en','lv'])\n",
        "\n",
        "print(langid.classify(\"this is a test\"))\n",
        "print(langid.classify(\"šis ir tests\"))"
      ],
      "metadata": {
        "id": "c05XyqzaUgMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langid.set_languages(['en','lv','uk','ru'])\n",
        "\n",
        "print(langid.classify(\"Краматорськ зазнав ракетного удару агресора\"))\n",
        "print(langid.classify(\"Краматорськ зазнав ракетного удару агресора: є загиблі\"))"
      ],
      "metadata": {
        "id": "-6T2Hpw1XAK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fastText:\n",
        "# * a library for text classification and word embeddings\n",
        "# * pre-trained models for 176 languages\n",
        "# * robust language detection (incl. short texts)\n",
        "\n",
        "!pip install fasttext\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "\n",
        "import fasttext"
      ],
      "metadata": {
        "id": "TklvgoM2XVwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained language detection model\n",
        "model = fasttext.load_model('lid.176.bin')\n",
        "\n",
        "text = \"Краматорськ зазнав ракетного удару агресора\"\n",
        "#text = \"šis ir tests\"\n",
        "\n",
        "predictions = model.predict(text, k=1)  # Return the TOP 1 prediction\n",
        "\n",
        "print(\"Language:\", predictions[0][0])\n",
        "print(\"Confidence:\", predictions[1][0])"
      ],
      "metadata": {
        "id": "29qsJsnfYQKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and using a NB classifier"
      ],
      "metadata": {
        "id": "ESQRdSMPa8El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hands-on dataset: *20 Newsgroup* assembled by Ken Lang @ CMU.\n",
        "\n",
        "https://www.kaggle.com/datasets/au1206/20-newsgroup-original\n",
        "\n",
        "We will use a format-converted, single-file version available from the course GitHub repo."
      ],
      "metadata": {
        "id": "nsj6NXaJc4uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/news20/20_newsgroup.tsv\n",
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/news20/20_newsgroup-freq.tsv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/news20/stoplist.txt"
      ],
      "metadata": {
        "id": "32Ed8tE4e3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install scipy\n",
        "!pip install sklearn\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "bvRF7jVfnb20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import nltk\n",
        "import scipy\n",
        "import numpy\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "JLi2qLKgjpHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialise(stop_txt, freq_tsv):\n",
        "\tglobal stoplist\n",
        "\tstoplist = set()\n",
        "\n",
        "\twith open(stop_txt) as txt:\n",
        "\t\tfor word in txt:\n",
        "\t\t\tstoplist.add(normalize_text(word.strip()))\n",
        "\n",
        "\tprint(\"[I] Word stoplist is read (\" + str(len(stoplist)) + \").\")\n",
        "\n",
        "\tglobal whitelist\n",
        "\twhitelist = set()\n",
        "\n",
        "\twith open(freq_tsv) as tsv:\n",
        "\t\tfor entry in tsv:\n",
        "\t\t\tfreq, word = entry.strip().split(\"\\t\")\n",
        "\n",
        "\t\t\tif int(freq) < 5:\n",
        "        # TODO: experiment with the threshold (e.g., 3 / 5 / 10)\n",
        "\t\t\t\t# Ignore the long tail: 2/3 of words occure less than N times\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\twhitelist.add(normalize_text(word))\n",
        "\n",
        "\tprint(\"[I] Word whitelist is read (\" + str(len(whitelist)) + \").\")"
      ],
      "metadata": {
        "id": "oy2iikUGj49E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "\ttext = text.lower()\n",
        "\ttext = re.sub(r\"\\d+\", \"100\", text)\n",
        "\n",
        "\treturn text.strip()\n",
        "\n",
        "def normalize_vector(vector):\n",
        "\twords = list(vector.keys())\n",
        "\n",
        "\tfor w in words:\n",
        "\t\tif w in stoplist or len(w) == 1 or w not in whitelist:\n",
        "\t\t\tvector.pop(w)\n",
        "\n",
        "\treturn vector\n",
        "\n",
        "def vectorize_text(text):\n",
        "\treturn normalize_vector({word: True for word in nltk.word_tokenize(normalize_text(text))})"
      ],
      "metadata": {
        "id": "ahWkVvFbkDp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file):\n",
        "\tdata_set = {}  # topic => annotated examples\n",
        "\n",
        "\twith open(file) as data:\n",
        "\t\tfor entry in data:\n",
        "\t\t\ttopic, text = entry.strip().split(\"\\t\")\n",
        "\n",
        "\t\t\tsub_set = []\n",
        "\t\t\tif topic in data_set:\n",
        "\t\t\t\tsub_set = data_set[topic]\n",
        "\n",
        "\t\t\tsub_set.append((vectorize_text(text), topic))\n",
        "\t\t\tdata_set[topic] = sub_set\n",
        "\n",
        "\treturn data_set\n",
        "\n",
        "def join_data(data_set):\n",
        "\tunion = []\n",
        "\n",
        "\tfor cat in data_set:\n",
        "\t\tunion += data_set[cat]\n",
        "\n",
        "\treturn union"
      ],
      "metadata": {
        "id": "cP1KYQxYmD-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_accuracy(data_set, k):\n",
        "\tkfold = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "\tdata_split = {}\n",
        "\n",
        "\tfor cat in data_set:\n",
        "\t\t# K-Fold split for each class to ensure balanced training and test data sets\n",
        "\t\tfolds = []\n",
        "\n",
        "\t\tfor train, test in kfold.split(data_set[cat]):    # k loops\n",
        "\t\t\ttrain_data = numpy.array(data_set[cat])[train]  # vs. data[train]\n",
        "\t\t\ttest_data = numpy.array(data_set[cat])[test]    # vs. data[test]\n",
        "\t\t\tfolds.append({\"train\": train_data, \"test\": test_data})\n",
        "\n",
        "\t\tdata_split[cat] = folds\n",
        "\n",
        "\tvalidations = []\n",
        "\n",
        "\tgold_result = []\n",
        "\tsilver_result = []\n",
        "\n",
        "\tfor i in range(k):\n",
        "\t\t# Join the training and test data into two respective sets\n",
        "\t\ttrain_data = numpy.array([])\n",
        "\t\ttest_data = numpy.array([])\n",
        "\n",
        "\t\tfor cat in data_split:\n",
        "\t\t\tif len(train_data) > 0:\n",
        "\t\t\t\ttrain_data = numpy.append(train_data, data_split[cat][i][\"train\"], axis=0)\n",
        "\t\t\telse:\n",
        "\t\t\t\ttrain_data = data_split[cat][i][\"train\"]\n",
        "\n",
        "\t\t\tif len(test_data) > 0:\n",
        "\t\t\t\ttest_data = numpy.append(test_data, data_split[cat][i][\"test\"], axis=0)\n",
        "\t\t\telse:\n",
        "\t\t\t\ttest_data = data_split[cat][i][\"test\"]\n",
        "\n",
        "\t\t# Naive Bayes classifier: training and evaluation\n",
        "\t\tnb = nltk.NaiveBayesClassifier.train(train_data)\n",
        "\t\tvalidations.append(nltk.classify.accuracy(nb, test_data))\n",
        "\n",
        "\t\tfor t in test_data:\n",
        "\t\t\tgold_result.append(t[1])\n",
        "\t\t\tsilver_result.append(nb.classify(t[0]))\n",
        "\n",
        "\treturn (validations, gold_result, silver_result)"
      ],
      "metadata": {
        "id": "RDuowS50bQuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(data_path, k, n):\n",
        "\tprint(\"\\n\\t\" + str(k) + \"-fold cross-validation:\\n\")\n",
        "\n",
        "\titerations = []\n",
        "\tgold_total = []\n",
        "\tsilver_total = []\n",
        "\n",
        "\tstart_time = datetime.datetime.now().replace(microsecond=0)\n",
        "\n",
        "\tfor i in range(n):\n",
        "\t\tvalidations, gold, silver = validate_accuracy(read_data(data_path), k)\n",
        "\t\titerations.append(numpy.mean(validations))\n",
        "\n",
        "\t\tgold_total += gold\n",
        "\t\tsilver_total += silver\n",
        "\n",
        "\t\tprint(\"\\t{0}.\\t\".format(i+1), end='')\n",
        "\t\tfor v in validations:\n",
        "\t\t\tprint(\"{0:.2f}  \".format(v), end='')\n",
        "\t\tprint(\"\\t{0:.0%}\".format(numpy.mean(validations)))\n",
        "\n",
        "\tend_time = datetime.datetime.now().replace(microsecond=0)\n",
        "\tprint(\"\\n\\tTotal validation time: \" + str(end_time - start_time))\n",
        "\n",
        "\tprint(\"\\n\\tAverage accuracy in {0} iterations: {1:.0%}\\n\".format(n, numpy.mean(iterations)))\n",
        "\n",
        "\tprint(classification_report(gold_total, silver_total))\n",
        "\n",
        "\tprint(\"Confusion matrix:\")\n",
        "\tprint(nltk.ConfusionMatrix(gold_total, silver_total))\n",
        "\t#print(confusion_matrix(gold_total, silver_total))"
      ],
      "metadata": {
        "id": "RmYktSqhl0pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(data_path, verbose):\n",
        "\tprint(\"[I] Training an NB classifier...\")\n",
        "\n",
        "\tstart_time = datetime.datetime.now().replace(microsecond=0)\n",
        "\n",
        "\t# The final (production) model is trained by using all available data (train+test)\n",
        "\tnb = nltk.NaiveBayesClassifier.train(join_data(read_data(data_path)))\n",
        "\n",
        "\tend_time = datetime.datetime.now().replace(microsecond=0)\n",
        "\tprint(\"[I] Training time: \" + str(end_time - start_time))\n",
        "\n",
        "\tif verbose:\n",
        "\t\tnb.show_most_informative_features(n=100)\n",
        "\n",
        "\tdmp = open(\"nb_classifier.pickle\", \"wb\")\n",
        "\tpickle.dump(nb, dmp)\n",
        "\tdmp.close()\n",
        "\n",
        "\tprint(\"[I] NB classifier trained and serialised in a file.\")"
      ],
      "metadata": {
        "id": "mb76hkhclwFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "\tdmp = open(\"nb_classifier.pickle\", \"rb\")\n",
        "\tnb = pickle.load(dmp)\n",
        "\tdmp.close()\n",
        "\n",
        "\tprint(\"[I] NB classifier loaded from a file.\")\n",
        "\n",
        "\twhile True:\n",
        "\t\tmessage = input(\"\\nEnter a text:\\n\")\n",
        "\n",
        "\t\tif len(message) == 0:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tfeatures = vectorize_text(message)\n",
        "\t\ttopic = nb.prob_classify(features)\n",
        "\n",
        "\t\tprint(\"\\n{0}\\n\".format(list(features.keys())))\n",
        "\n",
        "\t\tfor t in topic.samples():\n",
        "\t\t\tprint(\"{0}: {1:.3f}\".format(t, topic.prob(t)))\n",
        "\n",
        "\t\tprint(\"\\nGuess: \" + nb.classify(features))"
      ],
      "metadata": {
        "id": "Kc3BPltRlQqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the lexicons\n",
        "initialise('stoplist.txt', '20_newsgroup-freq.tsv')"
      ],
      "metadata": {
        "id": "4SCQbnBGk5_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with 'hyperparameters'\n",
        "run_validation(\"20_newsgroup.tsv\", 5, 2) # 5=folds, 2=iterations"
      ],
      "metadata": {
        "id": "OLdwTyUHt2aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save the final model\n",
        "run_training(\"20_newsgroup.tsv\", True) # True=verbose"
      ],
      "metadata": {
        "id": "9JIDVtoRowU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pre-trained model\n",
        "run()\n",
        "\n",
        "# Test [alt.atheism]: This was a conflict between the atheists and the religious.\n",
        "# Test [soc.religion.christian]: Whoever humbles himself like this child is the greatest in the kingdom of heaven.\n",
        "# Test [sci.med]: My wife had hives during the first two months of her pregnancy.\n",
        "# Test [sci.space]: Correct, we have no parallax measurements on the bursts."
      ],
      "metadata": {
        "id": "31sq0_Gsk9Bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}